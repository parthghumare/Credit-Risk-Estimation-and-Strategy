{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60302b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as nm\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import xgboost as xgb\n",
    "import datetime as dt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4829fd89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_ID</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00000fd6641609c6ece5454664794f0340ad84dddce9a2...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00001b22f846c82c51f6e3958ccd81970162bae8b007e8...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000041bdba6ecadd89a52d11886e8eaaec9325906c9723...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458908</th>\n",
       "      <td>ffff41c8a52833b56430603969b9ca48d208e7c192c6a4...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458909</th>\n",
       "      <td>ffff518bb2075e4816ee3fe9f3b152c57fc0e6f01bf7fd...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458910</th>\n",
       "      <td>ffff9984b999fccb2b6127635ed0736dda94e544e67e02...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458911</th>\n",
       "      <td>ffffa5c46bc8de74f5a4554e74e239c8dee6b9baf38814...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458912</th>\n",
       "      <td>fffff1d38b785cef84adeace64f8f83db3a0c31e8d92ea...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>458913 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              customer_ID  target\n",
       "0       0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...       0\n",
       "1       00000fd6641609c6ece5454664794f0340ad84dddce9a2...       0\n",
       "2       00001b22f846c82c51f6e3958ccd81970162bae8b007e8...       0\n",
       "3       000041bdba6ecadd89a52d11886e8eaaec9325906c9723...       0\n",
       "4       00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8a...       0\n",
       "...                                                   ...     ...\n",
       "458908  ffff41c8a52833b56430603969b9ca48d208e7c192c6a4...       0\n",
       "458909  ffff518bb2075e4816ee3fe9f3b152c57fc0e6f01bf7fd...       0\n",
       "458910  ffff9984b999fccb2b6127635ed0736dda94e544e67e02...       0\n",
       "458911  ffffa5c46bc8de74f5a4554e74e239c8dee6b9baf38814...       1\n",
       "458912  fffff1d38b785cef84adeace64f8f83db3a0c31e8d92ea...       0\n",
       "\n",
       "[458913 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels= pd.read_csv(\"../train_labels.csv\")\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49638d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "final_data=pd.DataFrame()\n",
    "data= pd.read_csv('../train_data.csv',nrows=1000000)\n",
    "data[\"S_2\"]=pd.to_datetime(data[\"S_2\"])\n",
    "data[\"Year-Month\"] = data[\"S_2\"].dt.to_period('M')\n",
    "data1=data.groupby('customer_ID').apply(lambda x: x.sample(n=1)).reset_index(drop = True)\n",
    "final_data=pd.concat([final_data,pd.merge(labels,data1, how= 'inner', on='customer_ID')])\n",
    "\n",
    "\n",
    "\n",
    "final_data.to_csv(\"Final_data-Step3.csv\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a93b0ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_ID</th>\n",
       "      <th>S_2</th>\n",
       "      <th>P_2</th>\n",
       "      <th>D_39</th>\n",
       "      <th>B_1</th>\n",
       "      <th>B_2</th>\n",
       "      <th>R_1</th>\n",
       "      <th>S_3</th>\n",
       "      <th>D_41</th>\n",
       "      <th>B_3</th>\n",
       "      <th>...</th>\n",
       "      <th>D_137</th>\n",
       "      <th>D_138</th>\n",
       "      <th>D_139</th>\n",
       "      <th>D_140</th>\n",
       "      <th>D_141</th>\n",
       "      <th>D_142</th>\n",
       "      <th>D_143</th>\n",
       "      <th>D_144</th>\n",
       "      <th>D_145</th>\n",
       "      <th>Year-Month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...</td>\n",
       "      <td>2017-03-09</td>\n",
       "      <td>0.938469</td>\n",
       "      <td>0.001733</td>\n",
       "      <td>0.008724</td>\n",
       "      <td>1.006838</td>\n",
       "      <td>0.009228</td>\n",
       "      <td>0.124035</td>\n",
       "      <td>0.008771</td>\n",
       "      <td>0.004709</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002427</td>\n",
       "      <td>0.003706</td>\n",
       "      <td>0.003818</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000569</td>\n",
       "      <td>0.000610</td>\n",
       "      <td>0.002674</td>\n",
       "      <td>2017-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...</td>\n",
       "      <td>2017-04-07</td>\n",
       "      <td>0.936665</td>\n",
       "      <td>0.005775</td>\n",
       "      <td>0.004923</td>\n",
       "      <td>1.000653</td>\n",
       "      <td>0.006151</td>\n",
       "      <td>0.126750</td>\n",
       "      <td>0.000798</td>\n",
       "      <td>0.002714</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003954</td>\n",
       "      <td>0.003167</td>\n",
       "      <td>0.005032</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.009576</td>\n",
       "      <td>0.005492</td>\n",
       "      <td>0.009217</td>\n",
       "      <td>2017-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...</td>\n",
       "      <td>2017-05-28</td>\n",
       "      <td>0.954180</td>\n",
       "      <td>0.091505</td>\n",
       "      <td>0.021655</td>\n",
       "      <td>1.009672</td>\n",
       "      <td>0.006815</td>\n",
       "      <td>0.123977</td>\n",
       "      <td>0.007598</td>\n",
       "      <td>0.009423</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003269</td>\n",
       "      <td>0.007329</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003429</td>\n",
       "      <td>0.006986</td>\n",
       "      <td>0.002603</td>\n",
       "      <td>2017-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...</td>\n",
       "      <td>2017-06-13</td>\n",
       "      <td>0.960384</td>\n",
       "      <td>0.002455</td>\n",
       "      <td>0.013683</td>\n",
       "      <td>1.002700</td>\n",
       "      <td>0.001373</td>\n",
       "      <td>0.117169</td>\n",
       "      <td>0.000685</td>\n",
       "      <td>0.005531</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.006117</td>\n",
       "      <td>0.004516</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008419</td>\n",
       "      <td>0.006527</td>\n",
       "      <td>0.009600</td>\n",
       "      <td>2017-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...</td>\n",
       "      <td>2017-07-16</td>\n",
       "      <td>0.947248</td>\n",
       "      <td>0.002483</td>\n",
       "      <td>0.015193</td>\n",
       "      <td>1.000727</td>\n",
       "      <td>0.007605</td>\n",
       "      <td>0.117325</td>\n",
       "      <td>0.004653</td>\n",
       "      <td>0.009312</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003671</td>\n",
       "      <td>0.004946</td>\n",
       "      <td>0.008889</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001670</td>\n",
       "      <td>0.008126</td>\n",
       "      <td>0.009827</td>\n",
       "      <td>2017-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999995</th>\n",
       "      <td>2e3fabd8551cfbb7819ffc99cf22d4335953e4d911b7ea...</td>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>0.444918</td>\n",
       "      <td>0.214272</td>\n",
       "      <td>0.509806</td>\n",
       "      <td>0.024078</td>\n",
       "      <td>0.001514</td>\n",
       "      <td>0.386644</td>\n",
       "      <td>0.001295</td>\n",
       "      <td>0.920215</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.004486</td>\n",
       "      <td>0.006241</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001294</td>\n",
       "      <td>0.006138</td>\n",
       "      <td>0.004346</td>\n",
       "      <td>2017-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999996</th>\n",
       "      <td>2e3fabd8551cfbb7819ffc99cf22d4335953e4d911b7ea...</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>0.445380</td>\n",
       "      <td>0.182068</td>\n",
       "      <td>0.511427</td>\n",
       "      <td>0.025369</td>\n",
       "      <td>0.008337</td>\n",
       "      <td>0.372405</td>\n",
       "      <td>0.002628</td>\n",
       "      <td>0.933117</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007064</td>\n",
       "      <td>0.005506</td>\n",
       "      <td>0.005020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>0.004177</td>\n",
       "      <td>0.004452</td>\n",
       "      <td>2018-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999997</th>\n",
       "      <td>2e3fabd8551cfbb7819ffc99cf22d4335953e4d911b7ea...</td>\n",
       "      <td>2018-02-28</td>\n",
       "      <td>0.439738</td>\n",
       "      <td>0.155119</td>\n",
       "      <td>0.497809</td>\n",
       "      <td>0.023311</td>\n",
       "      <td>0.000641</td>\n",
       "      <td>0.389353</td>\n",
       "      <td>0.009791</td>\n",
       "      <td>0.917929</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002184</td>\n",
       "      <td>0.004863</td>\n",
       "      <td>0.007474</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004654</td>\n",
       "      <td>0.005166</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>2018-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999998</th>\n",
       "      <td>2e3fabd8551cfbb7819ffc99cf22d4335953e4d911b7ea...</td>\n",
       "      <td>2018-03-31</td>\n",
       "      <td>0.449821</td>\n",
       "      <td>0.155417</td>\n",
       "      <td>0.501278</td>\n",
       "      <td>0.025809</td>\n",
       "      <td>0.006869</td>\n",
       "      <td>0.367082</td>\n",
       "      <td>0.001196</td>\n",
       "      <td>0.926053</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000752</td>\n",
       "      <td>0.004426</td>\n",
       "      <td>0.005118</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002986</td>\n",
       "      <td>0.000797</td>\n",
       "      <td>0.006316</td>\n",
       "      <td>2018-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999999</th>\n",
       "      <td>2e3fc76a68eeae6697b0a6b10f9288b7acaa678bbf99cd...</td>\n",
       "      <td>2017-03-29</td>\n",
       "      <td>0.481917</td>\n",
       "      <td>0.009878</td>\n",
       "      <td>0.182513</td>\n",
       "      <td>0.192125</td>\n",
       "      <td>0.005269</td>\n",
       "      <td>0.393390</td>\n",
       "      <td>0.004422</td>\n",
       "      <td>0.239441</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004562</td>\n",
       "      <td>0.003862</td>\n",
       "      <td>0.006811</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002136</td>\n",
       "      <td>0.002043</td>\n",
       "      <td>0.008634</td>\n",
       "      <td>2017-03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000000 rows × 191 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              customer_ID        S_2  \\\n",
       "0       0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f... 2017-03-09   \n",
       "1       0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f... 2017-04-07   \n",
       "2       0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f... 2017-05-28   \n",
       "3       0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f... 2017-06-13   \n",
       "4       0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f... 2017-07-16   \n",
       "...                                                   ...        ...   \n",
       "999995  2e3fabd8551cfbb7819ffc99cf22d4335953e4d911b7ea... 2017-12-01   \n",
       "999996  2e3fabd8551cfbb7819ffc99cf22d4335953e4d911b7ea... 2018-01-01   \n",
       "999997  2e3fabd8551cfbb7819ffc99cf22d4335953e4d911b7ea... 2018-02-28   \n",
       "999998  2e3fabd8551cfbb7819ffc99cf22d4335953e4d911b7ea... 2018-03-31   \n",
       "999999  2e3fc76a68eeae6697b0a6b10f9288b7acaa678bbf99cd... 2017-03-29   \n",
       "\n",
       "             P_2      D_39       B_1       B_2       R_1       S_3      D_41  \\\n",
       "0       0.938469  0.001733  0.008724  1.006838  0.009228  0.124035  0.008771   \n",
       "1       0.936665  0.005775  0.004923  1.000653  0.006151  0.126750  0.000798   \n",
       "2       0.954180  0.091505  0.021655  1.009672  0.006815  0.123977  0.007598   \n",
       "3       0.960384  0.002455  0.013683  1.002700  0.001373  0.117169  0.000685   \n",
       "4       0.947248  0.002483  0.015193  1.000727  0.007605  0.117325  0.004653   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "999995  0.444918  0.214272  0.509806  0.024078  0.001514  0.386644  0.001295   \n",
       "999996  0.445380  0.182068  0.511427  0.025369  0.008337  0.372405  0.002628   \n",
       "999997  0.439738  0.155119  0.497809  0.023311  0.000641  0.389353  0.009791   \n",
       "999998  0.449821  0.155417  0.501278  0.025809  0.006869  0.367082  0.001196   \n",
       "999999  0.481917  0.009878  0.182513  0.192125  0.005269  0.393390  0.004422   \n",
       "\n",
       "             B_3  ...  D_137  D_138     D_139     D_140     D_141  D_142  \\\n",
       "0       0.004709  ...    NaN    NaN  0.002427  0.003706  0.003818    NaN   \n",
       "1       0.002714  ...    NaN    NaN  0.003954  0.003167  0.005032    NaN   \n",
       "2       0.009423  ...    NaN    NaN  0.003269  0.007329  0.000427    NaN   \n",
       "3       0.005531  ...    NaN    NaN  0.006117  0.004516  0.003200    NaN   \n",
       "4       0.009312  ...    NaN    NaN  0.003671  0.004946  0.008889    NaN   \n",
       "...          ...  ...    ...    ...       ...       ...       ...    ...   \n",
       "999995  0.920215  ...    NaN    NaN  0.000700  0.004486  0.006241    NaN   \n",
       "999996  0.933117  ...    NaN    NaN  0.007064  0.005506  0.005020    NaN   \n",
       "999997  0.917929  ...    NaN    NaN  0.002184  0.004863  0.007474    NaN   \n",
       "999998  0.926053  ...    NaN    NaN  0.000752  0.004426  0.005118    NaN   \n",
       "999999  0.239441  ...    NaN    NaN  0.004562  0.003862  0.006811    NaN   \n",
       "\n",
       "           D_143     D_144     D_145  Year-Month  \n",
       "0       0.000569  0.000610  0.002674     2017-03  \n",
       "1       0.009576  0.005492  0.009217     2017-04  \n",
       "2       0.003429  0.006986  0.002603     2017-05  \n",
       "3       0.008419  0.006527  0.009600     2017-06  \n",
       "4       0.001670  0.008126  0.009827     2017-07  \n",
       "...          ...       ...       ...         ...  \n",
       "999995  0.001294  0.006138  0.004346     2017-12  \n",
       "999996  0.006667  0.004177  0.004452     2018-01  \n",
       "999997  0.004654  0.005166  0.000017     2018-02  \n",
       "999998  0.002986  0.000797  0.006316     2018-03  \n",
       "999999  0.002136  0.002043  0.008634     2017-03  \n",
       "\n",
       "[1000000 rows x 191 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "627a2898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8539\n",
      "0.2996838037240895\n"
     ]
    }
   ],
   "source": [
    "new_Data= final_data[final_data[\"Year-Month\"]==\"2018-03\"]\n",
    "print(len(new_Data))\n",
    "Default_rate = nm.mean(new_Data['target'])\n",
    "print(Default_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23488b4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82975, 192)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f43a7a8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.260825549864417"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nm.mean(final_data['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e95f5417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['customer_ID', 'D_63', 'D_64']\n"
     ]
    }
   ],
   "source": [
    "dtypes=final_data.dtypes\n",
    "\n",
    "dtypelist = list()\n",
    "\n",
    "for col in dtypes.index:\n",
    "    if dtypes[col] == 'object':\n",
    "        dtypelist.append(col)\n",
    "        \n",
    "print(dtypelist)\n",
    "\n",
    "# final_data['D_63'].value_counts()\n",
    "# final_data['D_64'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd2ff219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15931200"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = final_data.size\n",
    "size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b5e3d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#One hot encoding\n",
    "\n",
    "one_hot = pd.get_dummies(final_data[['D_63', 'D_64']])\n",
    "\n",
    "# Combine the one hot encoded data with the original dataframe\n",
    "final_data = pd.concat([final_data, one_hot], axis=1)\n",
    "\n",
    "# Drop the original categorical columns\n",
    "final_data.drop(['D_63', 'D_64', 'customer_ID'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07fcf8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "for col in['B_30','B_38','D_114','D_116','D_117','D_120','D_126','D_66','D_68']:\n",
    "    col_dummies=pd.get_dummies(final_data[col],prefix=col)\n",
    "    final_data=pd.concat([final_data,col_dummies],axis=1)\n",
    "    final_data.drop(col,axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff38169a-bc30-4eb3-a17f-01548ffab5d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82975, 225)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65410665-47f4-4304-9d3e-7b81c1c8c7db",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1275c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_columns',None)\n",
    "# final_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "444cadf6-7450-4d2c-9850-8bb6da95b917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>S_2</th>\n",
       "      <th>P_2</th>\n",
       "      <th>D_39</th>\n",
       "      <th>B_1</th>\n",
       "      <th>B_2</th>\n",
       "      <th>R_1</th>\n",
       "      <th>S_3</th>\n",
       "      <th>D_41</th>\n",
       "      <th>B_3</th>\n",
       "      <th>...</th>\n",
       "      <th>D_126_1.0</th>\n",
       "      <th>D_66_0.0</th>\n",
       "      <th>D_66_1.0</th>\n",
       "      <th>D_68_0.0</th>\n",
       "      <th>D_68_1.0</th>\n",
       "      <th>D_68_2.0</th>\n",
       "      <th>D_68_3.0</th>\n",
       "      <th>D_68_4.0</th>\n",
       "      <th>D_68_5.0</th>\n",
       "      <th>D_68_6.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2017-10-08</td>\n",
       "      <td>0.914767</td>\n",
       "      <td>0.003029</td>\n",
       "      <td>0.014324</td>\n",
       "      <td>1.000242</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.108115</td>\n",
       "      <td>0.009527</td>\n",
       "      <td>0.007836</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2017-12-29</td>\n",
       "      <td>0.861109</td>\n",
       "      <td>0.302357</td>\n",
       "      <td>0.006711</td>\n",
       "      <td>0.819772</td>\n",
       "      <td>0.007966</td>\n",
       "      <td>0.139138</td>\n",
       "      <td>0.000728</td>\n",
       "      <td>0.005235</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2017-06-10</td>\n",
       "      <td>0.852514</td>\n",
       "      <td>0.006877</td>\n",
       "      <td>0.007627</td>\n",
       "      <td>0.819987</td>\n",
       "      <td>0.009290</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003959</td>\n",
       "      <td>0.007532</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2017-05-29</td>\n",
       "      <td>0.581232</td>\n",
       "      <td>0.206867</td>\n",
       "      <td>0.279991</td>\n",
       "      <td>1.004374</td>\n",
       "      <td>0.001605</td>\n",
       "      <td>0.149216</td>\n",
       "      <td>0.008668</td>\n",
       "      <td>0.008219</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2017-12-30</td>\n",
       "      <td>0.877417</td>\n",
       "      <td>0.001762</td>\n",
       "      <td>0.007947</td>\n",
       "      <td>0.810670</td>\n",
       "      <td>0.003196</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.009694</td>\n",
       "      <td>0.004701</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82970</th>\n",
       "      <td>1</td>\n",
       "      <td>2017-05-25</td>\n",
       "      <td>0.559848</td>\n",
       "      <td>0.003366</td>\n",
       "      <td>1.319337</td>\n",
       "      <td>0.039250</td>\n",
       "      <td>0.001657</td>\n",
       "      <td>0.240873</td>\n",
       "      <td>0.293558</td>\n",
       "      <td>0.494647</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82971</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-02-28</td>\n",
       "      <td>0.214494</td>\n",
       "      <td>0.007116</td>\n",
       "      <td>0.552728</td>\n",
       "      <td>0.014322</td>\n",
       "      <td>0.009547</td>\n",
       "      <td>0.552067</td>\n",
       "      <td>0.456125</td>\n",
       "      <td>1.024074</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82972</th>\n",
       "      <td>0</td>\n",
       "      <td>2017-11-11</td>\n",
       "      <td>0.257381</td>\n",
       "      <td>0.009242</td>\n",
       "      <td>0.193778</td>\n",
       "      <td>0.165030</td>\n",
       "      <td>0.006112</td>\n",
       "      <td>1.226597</td>\n",
       "      <td>0.009048</td>\n",
       "      <td>0.184833</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82973</th>\n",
       "      <td>1</td>\n",
       "      <td>2017-09-01</td>\n",
       "      <td>0.492951</td>\n",
       "      <td>0.215591</td>\n",
       "      <td>0.516628</td>\n",
       "      <td>0.023872</td>\n",
       "      <td>0.009294</td>\n",
       "      <td>0.490583</td>\n",
       "      <td>0.008054</td>\n",
       "      <td>0.885366</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82974</th>\n",
       "      <td>0</td>\n",
       "      <td>2017-03-29</td>\n",
       "      <td>0.481917</td>\n",
       "      <td>0.009878</td>\n",
       "      <td>0.182513</td>\n",
       "      <td>0.192125</td>\n",
       "      <td>0.005269</td>\n",
       "      <td>0.393390</td>\n",
       "      <td>0.004422</td>\n",
       "      <td>0.239441</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>82975 rows × 225 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       target        S_2       P_2      D_39       B_1       B_2       R_1  \\\n",
       "0           0 2017-10-08  0.914767  0.003029  0.014324  1.000242  0.000263   \n",
       "1           0 2017-12-29  0.861109  0.302357  0.006711  0.819772  0.007966   \n",
       "2           0 2017-06-10  0.852514  0.006877  0.007627  0.819987  0.009290   \n",
       "3           0 2017-05-29  0.581232  0.206867  0.279991  1.004374  0.001605   \n",
       "4           0 2017-12-30  0.877417  0.001762  0.007947  0.810670  0.003196   \n",
       "...       ...        ...       ...       ...       ...       ...       ...   \n",
       "82970       1 2017-05-25  0.559848  0.003366  1.319337  0.039250  0.001657   \n",
       "82971       1 2018-02-28  0.214494  0.007116  0.552728  0.014322  0.009547   \n",
       "82972       0 2017-11-11  0.257381  0.009242  0.193778  0.165030  0.006112   \n",
       "82973       1 2017-09-01  0.492951  0.215591  0.516628  0.023872  0.009294   \n",
       "82974       0 2017-03-29  0.481917  0.009878  0.182513  0.192125  0.005269   \n",
       "\n",
       "            S_3      D_41       B_3  ...  D_126_1.0  D_66_0.0  D_66_1.0  \\\n",
       "0      0.108115  0.009527  0.007836  ...          1         0         0   \n",
       "1      0.139138  0.000728  0.005235  ...          1         0         0   \n",
       "2           NaN  0.003959  0.007532  ...          1         0         0   \n",
       "3      0.149216  0.008668  0.008219  ...          1         0         0   \n",
       "4           NaN  0.009694  0.004701  ...          1         0         1   \n",
       "...         ...       ...       ...  ...        ...       ...       ...   \n",
       "82970  0.240873  0.293558  0.494647  ...          1         0         0   \n",
       "82971  0.552067  0.456125  1.024074  ...          0         0         0   \n",
       "82972  1.226597  0.009048  0.184833  ...          0         0         0   \n",
       "82973  0.490583  0.008054  0.885366  ...          1         0         0   \n",
       "82974  0.393390  0.004422  0.239441  ...          1         0         0   \n",
       "\n",
       "       D_68_0.0  D_68_1.0  D_68_2.0  D_68_3.0  D_68_4.0  D_68_5.0  D_68_6.0  \n",
       "0             0         0         0         0         0         0         1  \n",
       "1             0         0         0         0         0         0         1  \n",
       "2             0         0         0         0         0         0         1  \n",
       "3             0         0         0         0         0         0         1  \n",
       "4             0         0         0         0         0         0         1  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "82970         0         0         0         0         1         0         0  \n",
       "82971         0         0         1         0         0         0         0  \n",
       "82972         0         0         0         0         0         0         1  \n",
       "82973         0         0         0         0         0         1         0  \n",
       "82974         0         0         0         0         0         0         1  \n",
       "\n",
       "[82975 rows x 225 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71cf341f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtypes=final_data.dtypes\n",
    "\n",
    "# dtypelist1 = list()\n",
    "\n",
    "# for col in dtypes.index:\n",
    "#     if dtypes[col] == 'object':\n",
    "#         dtypelist1.append(col)\n",
    "        \n",
    "# print(dtypelist1)\n",
    "\n",
    "# final_data['D_64'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d329ea32",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.dtypes\n",
    "\n",
    "final_data.replace('', nm.nan, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ff59e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.to_csv(\"XGBoost_Input_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b474956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82975, 225)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83abd515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    }
   ],
   "source": [
    "value = final_data['D_42'][1]\n",
    "\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b8fe213e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        2017-10\n",
       "1        2017-12\n",
       "2        2017-06\n",
       "3        2017-05\n",
       "4        2017-12\n",
       "          ...   \n",
       "82970    2017-05\n",
       "82971    2018-02\n",
       "82972    2017-11\n",
       "82973    2017-09\n",
       "82974    2017-03\n",
       "Name: Year-Month, Length: 82975, dtype: period[M]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data['Year-Month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b4b1eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test-train-test split\n",
    "\n",
    "train_start_date = '2017-05'\n",
    "train_end_date = '2018-01'\n",
    "\n",
    "# Define the start and end dates for the test 1 set\n",
    "test_start_date = '2017-03'\n",
    "test_end_date = '2017-04'\n",
    "\n",
    "# Define the start and end dates for the test 2 setYear-Month\n",
    "test1_start_date = '2018-02'\n",
    "test1_end_date = '2018-03'\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_final_data = final_data[(final_data['Year-Month'] >= train_start_date) & (final_data['Year-Month'] <= train_end_date)]\n",
    "test_final_data = final_data[(final_data['Year-Month'] >= test_start_date) & (final_data['Year-Month'] <= test_end_date)]\n",
    "test2_final_data = final_data[(final_data['Year-Month'] >= test1_start_date) & (final_data['Year-Month'] <= test1_end_date) ]         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "00752e61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55826, 225)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_final_data['Year-Month'].max()\n",
    "\n",
    "train_final_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "abce343c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12       2017-03\n",
       "39       2017-03\n",
       "52       2017-03\n",
       "54       2017-04\n",
       "61       2017-04\n",
       "          ...   \n",
       "82957    2017-04\n",
       "82961    2017-04\n",
       "82965    2017-03\n",
       "82966    2017-03\n",
       "82974    2017-03\n",
       "Name: Year-Month, Length: 11111, dtype: period[M]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_final_data['Year-Month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2631e060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9        2018-02\n",
       "10       2018-03\n",
       "17       2018-03\n",
       "34       2018-02\n",
       "35       2018-02\n",
       "          ...   \n",
       "82942    2018-03\n",
       "82943    2018-03\n",
       "82944    2018-02\n",
       "82947    2018-02\n",
       "82971    2018-02\n",
       "Name: Year-Month, Length: 16038, dtype: period[M]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2_final_data['Year-Month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2865cc39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25767563500877727\n"
     ]
    }
   ],
   "source": [
    "print(sum(train_final_data[\"target\"])/(len(train_final_data[\"target\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "223230d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_final_data.shape\n",
    "\n",
    "train_final_data['target'].dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "51150390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14385\n",
      "Length of train: 55826\n",
      "Default Rate of train: 0.25767563500877727\n",
      "shape:  (55826, 225)\n"
     ]
    }
   ],
   "source": [
    "lentrain=len(train_final_data)\n",
    "DR1 = sum(train_final_data['target'])/lentrain\n",
    "print(sum(train_final_data['target']))\n",
    "print(\"Length of train:\",lentrain)\n",
    "print(\"Default Rate of train:\",DR1)\n",
    "print(\"shape: \",train_final_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "00380808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train: 11111\n",
      "Default Rate of train: 0.23697236972369723\n",
      "2633\n",
      "shape:  (11111, 225)\n"
     ]
    }
   ],
   "source": [
    "lentest=len(test_final_data)\n",
    "DR2 = sum(test_final_data['target'])/lentest\n",
    "\n",
    "print(\"Length of train:\",lentest)\n",
    "print(\"Default Rate of train:\",DR2)\n",
    "print(sum(test_final_data['target']))\n",
    "print(\"shape: \",test_final_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9797b88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train: 16038\n",
      "Default Rate of train: 0.2883152512782142\n",
      "4624\n",
      "Shape:  (16038, 225)\n"
     ]
    }
   ],
   "source": [
    "lentest1=len(test2_final_data)\n",
    "DR3 = sum(test2_final_data['target'])/lentest1\n",
    "\n",
    "print(\"Length of train:\",lentest1)\n",
    "print(\"Default Rate of train:\",DR3)\n",
    "print(sum(test2_final_data['target']))\n",
    "print(\"Shape: \",test2_final_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "652328cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>S_2</th>\n",
       "      <th>P_2</th>\n",
       "      <th>D_39</th>\n",
       "      <th>B_1</th>\n",
       "      <th>B_2</th>\n",
       "      <th>R_1</th>\n",
       "      <th>S_3</th>\n",
       "      <th>D_41</th>\n",
       "      <th>B_3</th>\n",
       "      <th>...</th>\n",
       "      <th>D_126_1.0</th>\n",
       "      <th>D_66_0.0</th>\n",
       "      <th>D_66_1.0</th>\n",
       "      <th>D_68_0.0</th>\n",
       "      <th>D_68_1.0</th>\n",
       "      <th>D_68_2.0</th>\n",
       "      <th>D_68_3.0</th>\n",
       "      <th>D_68_4.0</th>\n",
       "      <th>D_68_5.0</th>\n",
       "      <th>D_68_6.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2017-10-08</td>\n",
       "      <td>0.914767</td>\n",
       "      <td>0.003029</td>\n",
       "      <td>0.014324</td>\n",
       "      <td>1.000242</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.108115</td>\n",
       "      <td>0.009527</td>\n",
       "      <td>0.007836</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2017-12-29</td>\n",
       "      <td>0.861109</td>\n",
       "      <td>0.302357</td>\n",
       "      <td>0.006711</td>\n",
       "      <td>0.819772</td>\n",
       "      <td>0.007966</td>\n",
       "      <td>0.139138</td>\n",
       "      <td>0.000728</td>\n",
       "      <td>0.005235</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2017-06-10</td>\n",
       "      <td>0.852514</td>\n",
       "      <td>0.006877</td>\n",
       "      <td>0.007627</td>\n",
       "      <td>0.819987</td>\n",
       "      <td>0.009290</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003959</td>\n",
       "      <td>0.007532</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2017-05-29</td>\n",
       "      <td>0.581232</td>\n",
       "      <td>0.206867</td>\n",
       "      <td>0.279991</td>\n",
       "      <td>1.004374</td>\n",
       "      <td>0.001605</td>\n",
       "      <td>0.149216</td>\n",
       "      <td>0.008668</td>\n",
       "      <td>0.008219</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2017-12-30</td>\n",
       "      <td>0.877417</td>\n",
       "      <td>0.001762</td>\n",
       "      <td>0.007947</td>\n",
       "      <td>0.810670</td>\n",
       "      <td>0.003196</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.009694</td>\n",
       "      <td>0.004701</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82968</th>\n",
       "      <td>0</td>\n",
       "      <td>2017-12-14</td>\n",
       "      <td>0.920545</td>\n",
       "      <td>0.005075</td>\n",
       "      <td>0.003105</td>\n",
       "      <td>1.004365</td>\n",
       "      <td>0.003671</td>\n",
       "      <td>0.134844</td>\n",
       "      <td>0.007686</td>\n",
       "      <td>0.008194</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82969</th>\n",
       "      <td>0</td>\n",
       "      <td>2017-09-28</td>\n",
       "      <td>1.007783</td>\n",
       "      <td>0.002555</td>\n",
       "      <td>0.007038</td>\n",
       "      <td>0.815799</td>\n",
       "      <td>0.002271</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004929</td>\n",
       "      <td>0.006698</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82970</th>\n",
       "      <td>1</td>\n",
       "      <td>2017-05-25</td>\n",
       "      <td>0.559848</td>\n",
       "      <td>0.003366</td>\n",
       "      <td>1.319337</td>\n",
       "      <td>0.039250</td>\n",
       "      <td>0.001657</td>\n",
       "      <td>0.240873</td>\n",
       "      <td>0.293558</td>\n",
       "      <td>0.494647</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82972</th>\n",
       "      <td>0</td>\n",
       "      <td>2017-11-11</td>\n",
       "      <td>0.257381</td>\n",
       "      <td>0.009242</td>\n",
       "      <td>0.193778</td>\n",
       "      <td>0.165030</td>\n",
       "      <td>0.006112</td>\n",
       "      <td>1.226597</td>\n",
       "      <td>0.009048</td>\n",
       "      <td>0.184833</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82973</th>\n",
       "      <td>1</td>\n",
       "      <td>2017-09-01</td>\n",
       "      <td>0.492951</td>\n",
       "      <td>0.215591</td>\n",
       "      <td>0.516628</td>\n",
       "      <td>0.023872</td>\n",
       "      <td>0.009294</td>\n",
       "      <td>0.490583</td>\n",
       "      <td>0.008054</td>\n",
       "      <td>0.885366</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>55826 rows × 225 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       target        S_2       P_2      D_39       B_1       B_2       R_1  \\\n",
       "0           0 2017-10-08  0.914767  0.003029  0.014324  1.000242  0.000263   \n",
       "1           0 2017-12-29  0.861109  0.302357  0.006711  0.819772  0.007966   \n",
       "2           0 2017-06-10  0.852514  0.006877  0.007627  0.819987  0.009290   \n",
       "3           0 2017-05-29  0.581232  0.206867  0.279991  1.004374  0.001605   \n",
       "4           0 2017-12-30  0.877417  0.001762  0.007947  0.810670  0.003196   \n",
       "...       ...        ...       ...       ...       ...       ...       ...   \n",
       "82968       0 2017-12-14  0.920545  0.005075  0.003105  1.004365  0.003671   \n",
       "82969       0 2017-09-28  1.007783  0.002555  0.007038  0.815799  0.002271   \n",
       "82970       1 2017-05-25  0.559848  0.003366  1.319337  0.039250  0.001657   \n",
       "82972       0 2017-11-11  0.257381  0.009242  0.193778  0.165030  0.006112   \n",
       "82973       1 2017-09-01  0.492951  0.215591  0.516628  0.023872  0.009294   \n",
       "\n",
       "            S_3      D_41       B_3  ...  D_126_1.0  D_66_0.0  D_66_1.0  \\\n",
       "0      0.108115  0.009527  0.007836  ...          1         0         0   \n",
       "1      0.139138  0.000728  0.005235  ...          1         0         0   \n",
       "2           NaN  0.003959  0.007532  ...          1         0         0   \n",
       "3      0.149216  0.008668  0.008219  ...          1         0         0   \n",
       "4           NaN  0.009694  0.004701  ...          1         0         1   \n",
       "...         ...       ...       ...  ...        ...       ...       ...   \n",
       "82968  0.134844  0.007686  0.008194  ...          1         0         0   \n",
       "82969       NaN  0.004929  0.006698  ...          1         0         1   \n",
       "82970  0.240873  0.293558  0.494647  ...          1         0         0   \n",
       "82972  1.226597  0.009048  0.184833  ...          0         0         0   \n",
       "82973  0.490583  0.008054  0.885366  ...          1         0         0   \n",
       "\n",
       "       D_68_0.0  D_68_1.0  D_68_2.0  D_68_3.0  D_68_4.0  D_68_5.0  D_68_6.0  \n",
       "0             0         0         0         0         0         0         1  \n",
       "1             0         0         0         0         0         0         1  \n",
       "2             0         0         0         0         0         0         1  \n",
       "3             0         0         0         0         0         0         1  \n",
       "4             0         0         0         0         0         0         1  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "82968         0         0         0         0         0         0         1  \n",
       "82969         0         0         0         0         1         0         0  \n",
       "82970         0         0         0         0         1         0         0  \n",
       "82972         0         0         0         0         0         0         1  \n",
       "82973         0         0         0         0         0         1         0  \n",
       "\n",
       "[55826 rows x 225 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train_final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6cc9b0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data: 0.8563585635856359\n",
      "Accuracy on test data 1: 0.8643845866068088\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Extract the input features and target variable from the training data\n",
    "X_train = train_final_data.drop(columns=['target','S_2', 'Year-Month'])\n",
    "y_train = train_final_data[['target']]\n",
    "\n",
    "# Extract the input features and target variable from the test data\n",
    "X_test = test_final_data.drop(columns=['target','S_2', 'Year-Month'])\n",
    "y_test = test_final_data[['target']]\n",
    "\n",
    "# Extract the input features and target variable from the test data 1\n",
    "X_test1 = test2_final_data.drop(columns=['target','S_2', 'Year-Month'])\n",
    "y_test1 = test2_final_data[['target']]\n",
    "\n",
    "# Define the XGBoost model\n",
    "model = xgb.XGBClassifier(random_state=52)\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of the model on the test data\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy on test data:\", accuracy)\n",
    "\n",
    "# Make predictions on the test data 1\n",
    "y_pred1 = model.predict(X_test1)\n",
    "\n",
    "# Evaluate the accuracy of the model on the test data 1\n",
    "accuracy1 = accuracy_score(y_test1, y_pred1)\n",
    "print(\"Accuracy on test data 1:\", accuracy1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "012d5743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['P_2', 'D_39', 'B_1', 'B_2', 'R_1', 'S_3', 'D_41', 'B_3', 'D_42',\n",
       "       'D_43',\n",
       "       ...\n",
       "       'D_126_1.0', 'D_66_0.0', 'D_66_1.0', 'D_68_0.0', 'D_68_1.0', 'D_68_2.0',\n",
       "       'D_68_3.0', 'D_68_4.0', 'D_68_5.0', 'D_68_6.0'],\n",
       "      dtype='object', length=222)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "01860e9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9        0.075384\n",
       "10       0.011603\n",
       "17       0.054218\n",
       "34       0.057162\n",
       "35       0.004674\n",
       "           ...   \n",
       "82942    0.164878\n",
       "82943    0.009772\n",
       "82944    0.065992\n",
       "82947    0.087645\n",
       "82971    0.095395\n",
       "Name: D_45, Length: 16038, dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test1['D_45']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9348f804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12       0.038232\n",
       "39       0.322905\n",
       "52       0.038526\n",
       "54       0.123623\n",
       "61       0.059196\n",
       "           ...   \n",
       "82957    0.080529\n",
       "82961    0.004656\n",
       "82965    0.359686\n",
       "82966    0.308071\n",
       "82974    0.561051\n",
       "Name: D_45, Length: 11111, dtype: float64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test['D_45']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "999efbf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9        1\n",
       "10       1\n",
       "17       1\n",
       "34       1\n",
       "35       0\n",
       "        ..\n",
       "82942    0\n",
       "82943    0\n",
       "82944    0\n",
       "82947    1\n",
       "82971    1\n",
       "Name: target, Length: 16038, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test1['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0f41d4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.2548433e-01 3.6718410e-03 2.4540812e-02 8.8707311e-03 7.2096852e-03\n",
      " 9.7426455e-03 5.4234467e-03 1.0674164e-02 1.5868738e-02 8.4688459e-03\n",
      " 8.0692349e-03 8.1852712e-03 1.0295361e-02 5.3501404e-03 3.4312266e-03\n",
      " 6.4488314e-03 3.7469645e-03 1.1578597e-02 8.1611322e-03 5.4550935e-03\n",
      " 5.9847659e-03 6.2210439e-03 8.8145994e-03 6.5826736e-03 3.5692215e-02\n",
      " 5.3221653e-03 5.1233820e-03 5.7283114e-03 4.0430920e-03 3.1300751e-03\n",
      " 4.7588889e-03 1.4806628e-02 2.7066909e-03 2.5244725e-03 2.3904284e-03\n",
      " 3.8751916e-03 2.9254926e-03 3.9675958e-03 2.9190884e-03 5.2493610e-03\n",
      " 2.9006389e-03 2.3943682e-03 2.7991829e-03 3.3585725e-03 3.1229679e-03\n",
      " 3.4458607e-03 2.5161903e-03 2.7531248e-03 2.8967254e-03 3.0567695e-03\n",
      " 4.6823402e-03 2.3257448e-03 4.6633929e-03 3.3472946e-03 3.4933046e-03\n",
      " 3.1201565e-03 3.1437294e-03 2.6946475e-03 2.8842045e-03 2.5817985e-03\n",
      " 3.0392089e-03 2.9048393e-03 2.2319516e-03 2.8171360e-03 2.6215550e-03\n",
      " 2.8795137e-03 2.9407630e-03 3.6775414e-03 2.1024733e-03 2.5148869e-03\n",
      " 2.3446474e-03 6.2269606e-03 2.6125354e-03 3.7184523e-03 3.1112337e-03\n",
      " 2.6414441e-03 2.6012037e-03 2.7589072e-03 2.6720932e-03 5.0219130e-03\n",
      " 2.9918572e-03 3.2058696e-03 2.9010884e-03 2.5427770e-03 2.4506066e-03\n",
      " 2.9236572e-03 2.8752673e-03 2.0803155e-03 2.6006585e-03 2.0421830e-03\n",
      " 2.9906048e-03 2.8890395e-03 2.8948376e-03 2.8749155e-03 2.5502681e-03\n",
      " 2.6572556e-03 2.9570656e-03 2.6537110e-03 3.2375508e-03 3.0336962e-03\n",
      " 2.0064341e-03 0.0000000e+00 3.1494116e-03 2.2950317e-03 0.0000000e+00\n",
      " 0.0000000e+00 2.6445989e-03 2.5065381e-03 2.6705165e-03 2.8277112e-03\n",
      " 2.3972334e-03 2.2232095e-03 2.0177870e-03 2.7045347e-03 3.1307947e-03\n",
      " 2.4182701e-03 2.7109645e-03 2.8273603e-03 2.8493162e-03 2.2349393e-03\n",
      " 2.5590686e-03 2.4914439e-03 2.6255436e-03 3.2996943e-03 4.0507969e-03\n",
      " 3.7663628e-03 4.6289926e-03 2.7892473e-03 2.4727064e-03 2.4990563e-03\n",
      " 2.3656441e-03 2.9246183e-03 2.5048407e-03 2.8092088e-03 3.5325608e-03\n",
      " 4.2829844e-03 7.6732771e-03 7.7284356e-03 1.3467026e-03 2.7868734e-03\n",
      " 7.9133073e-03 6.8427674e-03 1.4055835e-03 5.3639696e-03 3.6708058e-03\n",
      " 3.0292494e-03 3.1766521e-03 2.5886171e-03 2.7185478e-03 3.1646956e-03\n",
      " 3.5149204e-03 2.7687470e-03 2.0545498e-03 2.9185452e-03 2.5092147e-03\n",
      " 2.9110142e-03 2.6616016e-03 3.5879083e-03 2.6727177e-03 1.5834268e-03\n",
      " 3.1339582e-03 2.5155200e-03 6.7197187e-03 2.9225543e-03 2.7634522e-03\n",
      " 5.5832975e-03 4.4421032e-03 3.0763971e-03 3.4066434e-03 4.3359715e-03\n",
      " 2.8521747e-03 2.9488236e-03 2.6606095e-03 2.5109004e-03 2.8591072e-03\n",
      " 2.4298525e-03 2.8934502e-03 3.5626020e-03 3.0837597e-03 0.0000000e+00\n",
      " 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 3.3972578e-03 1.0513528e-03 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 5.7610957e-04 5.7729856e-05 0.0000000e+00 1.7544259e-03\n",
      " 0.0000000e+00 1.8006916e-03 2.6262752e-03 6.5260874e-03 0.0000000e+00\n",
      " 0.0000000e+00 2.9324070e-03 0.0000000e+00 3.1128733e-03 2.2006293e-03\n",
      " 2.7383261e-03 2.3152318e-03 0.0000000e+00 2.8470566e-03 0.0000000e+00\n",
      " 0.0000000e+00 2.7534263e-03 1.5314633e-03 0.0000000e+00 7.3151626e-03\n",
      " 0.0000000e+00 3.6022223e-03 0.0000000e+00 0.0000000e+00 2.4813360e-03\n",
      " 1.4527616e-03 0.0000000e+00]\n",
      "      feature  importance\n",
      "0         P_2    0.225484\n",
      "24        B_9    0.035692\n",
      "2         B_1    0.024541\n",
      "8        D_42    0.015869\n",
      "31       B_11    0.014807\n",
      "..        ...         ...\n",
      "184    D_64_O    0.000000\n",
      "183   D_64_-1    0.000000\n",
      "182   D_63_XZ    0.000000\n",
      "181   D_63_XM    0.000000\n",
      "221  D_68_6.0    0.000000\n",
      "\n",
      "[222 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#Feature_Importance\n",
    "importance = model.feature_importances_\n",
    "\n",
    "print(importance)\n",
    "\n",
    "feature_importance_df = pd.DataFrame(list(zip(X_train.columns, importance)), columns=['feature', 'importance'])\n",
    "\n",
    "# sort the dataframe in descending order of importance\n",
    "feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)\n",
    "\n",
    "# print the dataframe\n",
    "print(feature_importance_df)\n",
    "\n",
    "feature_importance_df.to_csv(\"Feature_Importance_1.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "11fc4280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55826, 222)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "44d38dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.04411191 0.00332216 0.0077614  0.010398   0.00529694 0.01137279\n",
      " 0.00498838 0.01153226 0.0063805  0.0047298  0.01489071 0.00541424\n",
      " 0.00448661 0.00383925 0.00362351 0.00475134 0.00379966 0.01292047\n",
      " 0.00406436 0.00329627 0.11240999 0.01597023 0.0068591  0.00423617\n",
      " 0.00683122 0.00535881 0.01945839 0.005311   0.00384929 0.00332538\n",
      " 0.00392554 0.00426926 0.00374939 0.00315016 0.0037331  0.00357301\n",
      " 0.00351068 0.00473543 0.00357824 0.00447622 0.00347179 0.00367096\n",
      " 0.00438051 0.00401331 0.00408521 0.00376424 0.00386774 0.00772514\n",
      " 0.00409561 0.00375203 0.00375591 0.00312253 0.00408822 0.00324681\n",
      " 0.00471247 0.00353061 0.0043679  0.00333656 0.00388468 0.00343522\n",
      " 0.00360443 0.00307815 0.00238614 0.00334694 0.00360556 0.00371442\n",
      " 0.00399136 0.00386093 0.0031723  0.00372044 0.00352677 0.00404039\n",
      " 0.00397872 0.00388713 0.00401504 0.0040277  0.00325267 0.00353751\n",
      " 0.0030153  0.00388439 0.00362046 0.00401429 0.00363444 0.00385017\n",
      " 0.00279872 0.00329634 0.00427672 0.00325273 0.00406502 0.00361409\n",
      " 0.00378225 0.00325293 0.00341297 0.00327681 0.00332707 0.00363848\n",
      " 0.00324866 0.003515   0.00422214 0.00388189 0.00408279 0.\n",
      " 0.00398152 0.0030069  0.         0.         0.00333414 0.00353176\n",
      " 0.00355297 0.00360974 0.00277239 0.00334648 0.00356632 0.00347036\n",
      " 0.00368063 0.0036094  0.00324834 0.0039947  0.0036534  0.00327394\n",
      " 0.00357323 0.00325121 0.00383189 0.00378239 0.00388515 0.0040872\n",
      " 0.00456189 0.00360493 0.00382311 0.00399423 0.00390781 0.00282527\n",
      " 0.00324411 0.00331218 0.0031797  0.00365562 0.00419598 0.00777056\n",
      " 0.         0.0034629  0.0038224  0.00292708 0.         0.00437558\n",
      " 0.00413897 0.00326201 0.00331715 0.0038009  0.00392668 0.003985\n",
      " 0.00390564 0.00379339 0.00317233 0.00350876 0.00330155 0.00397292\n",
      " 0.00376538 0.00359298 0.00322932 0.00395366 0.00383456 0.00294597\n",
      " 0.00642605 0.00357194 0.00307128 0.00293075 0.00391703 0.0044076\n",
      " 0.00419387 0.00364042 0.00350056 0.00373042 0.00404957 0.0037106\n",
      " 0.00324323 0.0036435  0.00351208 0.00814761 0.0035871  0.\n",
      " 0.         0.         0.00385121 0.         0.00317916 0.00476677\n",
      " 0.00335619 0.         0.         0.         0.         0.00463458\n",
      " 0.         0.00262579 0.00328501 0.00295332 0.         0.00102698\n",
      " 0.01813864 0.0040969  0.         0.         0.         0.\n",
      " 0.00263852 0.00306111 0.00334392 0.00370377 0.         0.00140636\n",
      " 0.         0.0043955  0.00039773 0.00297862 0.01855123 0.\n",
      " 0.00274866 0.         0.         0.00305734 0.00252301 0.00257267]\n"
     ]
    }
   ],
   "source": [
    "# set the hyperparameters\n",
    "params = {\n",
    "    'n_estimators': 300,\n",
    "    'learning_rate': 0.5,\n",
    "    'max_depth': 4,\n",
    "    'subsample': 0.5,\n",
    "    'colsample_bytree': 0.5,\n",
    "    'scale_pos_weight': 5\n",
    "}\n",
    "\n",
    "# create the XGBClassifier object with the hyperparameters\n",
    "model1 = xgb.XGBClassifier(**params)\n",
    "\n",
    "# train the model on the training set\n",
    "model1.fit(X_train, y_train)\n",
    "\n",
    "# # make predictions on the test set\n",
    "# y_pred = model.predict(X_test)\n",
    "\n",
    "# print(y_pred)\n",
    "\n",
    "importance = model1.feature_importances_\n",
    "\n",
    "print(importance)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6b745556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      feature  importance\n",
      "0         P_2    0.225484\n",
      "24        B_9    0.035692\n",
      "2         B_1    0.024541\n",
      "8        D_42    0.015869\n",
      "31       B_11    0.014807\n",
      "..        ...         ...\n",
      "213  D_66_0.0    0.000000\n",
      "104      D_88    0.000000\n",
      "179   D_63_CR    0.000000\n",
      "101      D_87    0.000000\n",
      "221  D_68_6.0    0.000000\n",
      "\n",
      "[222 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "feature_importance = pd.DataFrame(list(zip(X_train.columns, importance)), columns=['feature', 'importance'])\n",
    "\n",
    "# sort the dataframe in descending order of importance\n",
    "feature_importance = feature_importance_df.sort_values(by='importance', ascending=False)\n",
    "\n",
    "# print the dataframe\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9485fe99",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance.to_csv(\"Feature_Importance2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aba1b8b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B_11',\n",
       " 'R_26',\n",
       " 'D_49',\n",
       " 'B_9',\n",
       " 'P_2',\n",
       " 'D_110',\n",
       " 'D_48',\n",
       " 'D_43',\n",
       " 'D_75',\n",
       " 'D_56',\n",
       " 'R_1',\n",
       " 'D_112',\n",
       " 'D_66_1.0',\n",
       " 'D_79',\n",
       " 'B_4',\n",
       " 'D_45',\n",
       " 'D_50',\n",
       " 'D_46',\n",
       " 'B_6',\n",
       " 'D_42',\n",
       " 'B_2',\n",
       " 'R_3',\n",
       " 'R_27',\n",
       " 'B_7',\n",
       " 'B_3',\n",
       " 'D_111',\n",
       " 'B_5',\n",
       " 'D_132',\n",
       " 'P_3',\n",
       " 'D_51',\n",
       " 'B_1',\n",
       " 'B_8',\n",
       " 'D_41',\n",
       " 'D_134',\n",
       " 'D_114_1.0',\n",
       " 'S_3',\n",
       " 'D_52',\n",
       " 'D_44']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_selected_features = set(feature_importance[feature_importance['importance'] > 0.005]['feature']) | set(feature_importance_df[feature_importance_df['importance']> 0.005]['feature'])\n",
    "df_selected_features= list(df_selected_features)\n",
    "df_selected_features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "379d6062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     callbacks=None, colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=None,\n",
       "                                     early_stopping_rounds=None,\n",
       "                                     enable_categorical=False,\n",
       "                                     eval_metric=&#x27;auc&#x27;, feature_types=None,\n",
       "                                     gamma=None, gpu_id=None, grow_policy=None,\n",
       "                                     importance_type=None,\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=None...\n",
       "                                     max_delta_step=None, max_depth=None,\n",
       "                                     max_leaves=None, min_child_weight=None,\n",
       "                                     missing=nan, monotone_constraints=None,\n",
       "                                     n_estimators=100, n_jobs=None,\n",
       "                                     num_parallel_tree=None, predictor=None,\n",
       "                                     random_state=52, ...),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;colsample_bytree&#x27;: [0.5, 1.0],\n",
       "                         &#x27;learning_rate&#x27;: [0.01, 0.1],\n",
       "                         &#x27;n_estimators&#x27;: [50, 100, 300],\n",
       "                         &#x27;scale_pos_weight&#x27;: [1, 5, 10],\n",
       "                         &#x27;subsample&#x27;: [0.5, 0.8]},\n",
       "             scoring=&#x27;roc_auc&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     callbacks=None, colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=None,\n",
       "                                     early_stopping_rounds=None,\n",
       "                                     enable_categorical=False,\n",
       "                                     eval_metric=&#x27;auc&#x27;, feature_types=None,\n",
       "                                     gamma=None, gpu_id=None, grow_policy=None,\n",
       "                                     importance_type=None,\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=None...\n",
       "                                     max_delta_step=None, max_depth=None,\n",
       "                                     max_leaves=None, min_child_weight=None,\n",
       "                                     missing=nan, monotone_constraints=None,\n",
       "                                     n_estimators=100, n_jobs=None,\n",
       "                                     num_parallel_tree=None, predictor=None,\n",
       "                                     random_state=52, ...),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;colsample_bytree&#x27;: [0.5, 1.0],\n",
       "                         &#x27;learning_rate&#x27;: [0.01, 0.1],\n",
       "                         &#x27;n_estimators&#x27;: [50, 100, 300],\n",
       "                         &#x27;scale_pos_weight&#x27;: [1, 5, 10],\n",
       "                         &#x27;subsample&#x27;: [0.5, 0.8]},\n",
       "             scoring=&#x27;roc_auc&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;auc&#x27;, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=52, ...)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;auc&#x27;, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=52, ...)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     callbacks=None, colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=None,\n",
       "                                     early_stopping_rounds=None,\n",
       "                                     enable_categorical=False,\n",
       "                                     eval_metric='auc', feature_types=None,\n",
       "                                     gamma=None, gpu_id=None, grow_policy=None,\n",
       "                                     importance_type=None,\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=None...\n",
       "                                     max_delta_step=None, max_depth=None,\n",
       "                                     max_leaves=None, min_child_weight=None,\n",
       "                                     missing=nan, monotone_constraints=None,\n",
       "                                     n_estimators=100, n_jobs=None,\n",
       "                                     num_parallel_tree=None, predictor=None,\n",
       "                                     random_state=52, ...),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'colsample_bytree': [0.5, 1.0],\n",
       "                         'learning_rate': [0.01, 0.1],\n",
       "                         'n_estimators': [50, 100, 300],\n",
       "                         'scale_pos_weight': [1, 5, 10],\n",
       "                         'subsample': [0.5, 0.8]},\n",
       "             scoring='roc_auc')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Subset X_train to include only selected features\n",
    "X_train_selected = X_train[df_selected_features]\n",
    "X_test_selected= X_test[df_selected_features]\n",
    "X_test1_selected= X_test1[df_selected_features]\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 300],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'subsample': [0.5, 0.8],\n",
    "    'colsample_bytree': [0.5, 1.0],\n",
    "    'scale_pos_weight': [1, 5, 10]\n",
    "}\n",
    "\n",
    "# Create an XGBoost classifier object\n",
    "xgb_model = xgb.XGBClassifier(random_state=52,objective='binary:logistic', eval_metric='auc')\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=5, n_jobs=-1,scoring='roc_auc')\n",
    "\n",
    "# Fit the GridSearchCV object to the training data\n",
    "grid_search.fit(X_train_selected, y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "de1b4c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "train= []\n",
    "test1=[]\n",
    "test2=[]\n",
    "t=[]\n",
    "l=[]\n",
    "s=[]\n",
    "c=[]\n",
    "w=[]\n",
    "scores =pd.DataFrame()\n",
    "\n",
    "# Create an XGBoost classifier object\n",
    "for trees in [50,100,300]:\n",
    "    for LR in [0.01, 0.1]:\n",
    "        for subsample in [0.5,0.8]:\n",
    "            for colsample in [0.5,1]:\n",
    "                for weight in [1,5,10]:\n",
    "                    model_xgb_test= xgb.XGBClassifier(n_estimators=trees, learning_rate=LR, subsample=subsample,colsample_bytree=colsample, min_child_weight=weight, random_state=42)\n",
    "                    model_xgb_test.fit(X_train_selected, y_train)\n",
    "                    t.append(trees)\n",
    "                    l.append(LR)\n",
    "                    s.append(subsample)\n",
    "                    c.append(colsample)\n",
    "                    w.append(weight)\n",
    "                    train.append(roc_auc_score(y_train, model_xgb_test.predict(X_train_selected)))\n",
    "                    test1.append(roc_auc_score(y_test, model_xgb_test.predict(X_test_selected)))\n",
    "                    test2.append(roc_auc_score(y_test1, model_xgb_test.predict(X_test1_selected)))\n",
    "scores['trees'] = t\n",
    "scores['Learning rate'] = l\n",
    "scores['subsample'] = s\n",
    "scores['percentage features'] = c\n",
    "scores['Weight of default'] = w\n",
    "scores['AUC train 1'] = train\n",
    "scores['AUC test 1'] = test1\n",
    "scores['AUC test 2'] = test2\n",
    "scores.to_csv('XGBoost_scores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9834f7ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trees</th>\n",
       "      <th>Learning rate</th>\n",
       "      <th>subsample</th>\n",
       "      <th>percentage features</th>\n",
       "      <th>Weight of default</th>\n",
       "      <th>AUC train 1</th>\n",
       "      <th>AUC test 1</th>\n",
       "      <th>AUC test 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.823111</td>\n",
       "      <td>0.773798</td>\n",
       "      <td>0.844898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.822323</td>\n",
       "      <td>0.773477</td>\n",
       "      <td>0.844937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.822327</td>\n",
       "      <td>0.773071</td>\n",
       "      <td>0.844457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.830130</td>\n",
       "      <td>0.787632</td>\n",
       "      <td>0.848319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.828112</td>\n",
       "      <td>0.786185</td>\n",
       "      <td>0.847714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>300</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.890637</td>\n",
       "      <td>0.786194</td>\n",
       "      <td>0.853321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>300</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.883332</td>\n",
       "      <td>0.788008</td>\n",
       "      <td>0.853812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>300</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.910355</td>\n",
       "      <td>0.788689</td>\n",
       "      <td>0.850317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>300</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.896491</td>\n",
       "      <td>0.792632</td>\n",
       "      <td>0.851561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>300</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.888273</td>\n",
       "      <td>0.787793</td>\n",
       "      <td>0.851895</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    trees  Learning rate  subsample  percentage features  Weight of default  \\\n",
       "0      50           0.01        0.5                  0.5                  1   \n",
       "1      50           0.01        0.5                  0.5                  5   \n",
       "2      50           0.01        0.5                  0.5                 10   \n",
       "3      50           0.01        0.5                  1.0                  1   \n",
       "4      50           0.01        0.5                  1.0                  5   \n",
       "..    ...            ...        ...                  ...                ...   \n",
       "67    300           0.10        0.8                  0.5                  5   \n",
       "68    300           0.10        0.8                  0.5                 10   \n",
       "69    300           0.10        0.8                  1.0                  1   \n",
       "70    300           0.10        0.8                  1.0                  5   \n",
       "71    300           0.10        0.8                  1.0                 10   \n",
       "\n",
       "    AUC train 1  AUC test 1  AUC test 2  \n",
       "0      0.823111    0.773798    0.844898  \n",
       "1      0.822323    0.773477    0.844937  \n",
       "2      0.822327    0.773071    0.844457  \n",
       "3      0.830130    0.787632    0.848319  \n",
       "4      0.828112    0.786185    0.847714  \n",
       "..          ...         ...         ...  \n",
       "67     0.890637    0.786194    0.853321  \n",
       "68     0.883332    0.788008    0.853812  \n",
       "69     0.910355    0.788689    0.850317  \n",
       "70     0.896491    0.792632    0.851561  \n",
       "71     0.888273    0.787793    0.851895  \n",
       "\n",
       "[72 rows x 8 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8da84046",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred_proba_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43my_pred_proba_1\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_pred_proba_1' is not defined"
     ]
    }
   ],
   "source": [
    "y_pred_proba_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "56edc9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['mean_fit_time', 'std_fit_time', 'mean_score_time', 'std_score_time', 'param_colsample_bytree', 'param_learning_rate', 'param_n_estimators', 'param_scale_pos_weight', 'param_subsample', 'params', 'split0_test_score', 'split1_test_score', 'split2_test_score', 'split3_test_score', 'split4_test_score', 'mean_test_score', 'std_test_score', 'rank_test_score'])\n"
     ]
    }
   ],
   "source": [
    "print(grid_search.cv_results_.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9149cb60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9290277787837091\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# print(grid_search.best_params_)\n",
    "# print(grid_search.best_estimator_)\n",
    "print(grid_search.best_score_)\n",
    "# print(grid_search.scorer_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "db059dff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([ 26.14893498,  37.4731061 ,  26.70093341,  35.49747458,\n",
       "         29.15080318,  35.11253924,  52.41870823,  68.72602739,\n",
       "         53.93784237,  74.57991419,  50.75349755,  73.37745943,\n",
       "        161.44493847, 209.52809038, 171.89410796, 210.26672292,\n",
       "        162.11331773, 205.02920303,  30.97005944,  36.96614499,\n",
       "         26.01612206,  34.268435  ,  27.30284796,  36.60396957,\n",
       "         52.04495935,  71.81695776,  51.67431784,  72.92435937,\n",
       "         50.7056416 ,  74.55191197, 152.89734468, 199.78806534,\n",
       "        164.39946737, 206.01854458, 158.56818833, 206.4004756 ,\n",
       "         56.45306268,  67.92297406,  50.88397021,  68.67936339,\n",
       "         55.31714749,  66.87038217,  98.37086382, 138.53707414,\n",
       "         98.1099133 , 130.45227203, 104.09533033, 138.23499064,\n",
       "        301.19929199, 384.13249912, 310.66007662, 414.7283782 ,\n",
       "        296.40775247, 426.12398701,  50.55771871,  57.347435  ,\n",
       "         49.32164578,  63.36700296,  56.51637206,  68.94574823,\n",
       "        127.14349799, 157.84481168, 100.37860594, 129.82306709,\n",
       "         98.80825377, 130.83660364, 279.76492214, 377.82673693,\n",
       "        291.76372881, 370.14781175, 281.09607821, 321.42289848]),\n",
       " 'std_fit_time': array([ 2.89039831,  4.85830044,  4.00399855,  6.17688725,  4.60821172,\n",
       "         5.13577901,  7.88513608,  8.81240742,  8.15611169, 10.04483397,\n",
       "         6.2204128 ,  8.84193841, 13.13247676, 14.99726662, 16.29192762,\n",
       "         6.9067305 , 15.09418041, 11.62604535,  0.44681896,  4.19406631,\n",
       "         3.36790535,  4.35450014,  2.4490806 ,  4.3532015 ,  5.98224789,\n",
       "         8.96985929,  6.52280756,  8.27222533,  5.62805177,  7.86014309,\n",
       "        11.18115969, 20.15338652, 15.28441783, 10.3759181 , 13.95304778,\n",
       "        13.5418357 ,  6.4327335 ,  6.31448589,  6.02866449,  9.5913652 ,\n",
       "         7.17700803,  8.02529758,  8.30597371, 13.95841494,  7.74522782,\n",
       "        10.72427148, 14.49673665, 17.79488301, 38.13755152, 42.73819804,\n",
       "        40.68823001, 49.34101203, 15.21645134, 21.8859658 ,  6.98751152,\n",
       "         0.23698227,  6.47210762,  6.03776487,  5.95273375,  8.69139096,\n",
       "         9.43551291, 11.91902725,  9.7133266 ,  4.60638702,  5.87967497,\n",
       "        11.31916907, 14.47499253, 28.14489076, 10.52172259, 15.06912653,\n",
       "        26.43454146,  3.56535226]),\n",
       " 'mean_score_time': array([0.12640934, 0.12271223, 0.09840908, 0.10040708, 0.10340858,\n",
       "        0.10400877, 0.16261249, 0.16481304, 0.1590127 , 0.1658124 ,\n",
       "        0.14821162, 0.15501237, 0.40525303, 0.38903008, 0.41163239,\n",
       "        0.46643453, 0.39122934, 0.35702724, 0.12081013, 0.11420875,\n",
       "        0.12741084, 0.10240774, 0.12061429, 0.10180936, 0.14041138,\n",
       "        0.16461372, 0.16241302, 0.1518115 , 0.15361295, 0.16881371,\n",
       "        0.36882987, 0.34825811, 0.3824297 , 0.33802538, 0.40322938,\n",
       "        0.35042734, 0.10160818, 0.10160894, 0.11100922, 0.09920797,\n",
       "        0.10820937, 0.09120746, 0.15281239, 0.15881209, 0.19021497,\n",
       "        0.16001325, 0.14821157, 0.16641226, 0.43943353, 0.41623263,\n",
       "        0.38962984, 0.39834251, 0.41723709, 0.42043271, 0.09860897,\n",
       "        0.09266696, 0.11060905, 0.10560884, 0.11640911, 0.11420908,\n",
       "        0.17681513, 0.16961451, 0.1502121 , 0.15361333, 0.16681337,\n",
       "        0.14641194, 0.34042692, 0.32282491, 0.35742745, 0.3461318 ,\n",
       "        0.34982796, 0.28803444]),\n",
       " 'std_score_time': array([0.02528852, 0.02077587, 0.01559003, 0.01324509, 0.01685987,\n",
       "        0.02017022, 0.03411751, 0.03734596, 0.02443823, 0.02412972,\n",
       "        0.01536808, 0.02267264, 0.08789853, 0.04554408, 0.07531446,\n",
       "        0.06525023, 0.06519197, 0.03833896, 0.01322704, 0.01875701,\n",
       "        0.04230842, 0.01600864, 0.03590885, 0.01417875, 0.00922306,\n",
       "        0.02418854, 0.03506501, 0.02119895, 0.03089955, 0.02880064,\n",
       "        0.07268838, 0.04452593, 0.03630515, 0.0445311 , 0.07144623,\n",
       "        0.05786851, 0.00922308, 0.01350047, 0.01878564, 0.01513226,\n",
       "        0.02103931, 0.00370964, 0.02218603, 0.03887422, 0.01538095,\n",
       "        0.02624657, 0.01809458, 0.02267773, 0.07505309, 0.0523691 ,\n",
       "        0.05109812, 0.06627852, 0.10723212, 0.06585172, 0.01472032,\n",
       "        0.00142047, 0.02533505, 0.01537085, 0.02065651, 0.02168456,\n",
       "        0.02299064, 0.02680168, 0.01925114, 0.02059834, 0.02725956,\n",
       "        0.01718893, 0.03590499, 0.01961097, 0.03397039, 0.03747646,\n",
       "        0.04572955, 0.03305517]),\n",
       " 'param_colsample_bytree': masked_array(data=[0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
       "                    1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
       "                    1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
       "                    1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_learning_rate': masked_array(data=[0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_n_estimators': masked_array(data=[50, 50, 50, 50, 50, 50, 100, 100, 100, 100, 100, 100,\n",
       "                    300, 300, 300, 300, 300, 300, 50, 50, 50, 50, 50, 50,\n",
       "                    100, 100, 100, 100, 100, 100, 300, 300, 300, 300, 300,\n",
       "                    300, 50, 50, 50, 50, 50, 50, 100, 100, 100, 100, 100,\n",
       "                    100, 300, 300, 300, 300, 300, 300, 50, 50, 50, 50, 50,\n",
       "                    50, 100, 100, 100, 100, 100, 100, 300, 300, 300, 300,\n",
       "                    300, 300],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_scale_pos_weight': masked_array(data=[1, 1, 5, 5, 10, 10, 1, 1, 5, 5, 10, 10, 1, 1, 5, 5, 10,\n",
       "                    10, 1, 1, 5, 5, 10, 10, 1, 1, 5, 5, 10, 10, 1, 1, 5, 5,\n",
       "                    10, 10, 1, 1, 5, 5, 10, 10, 1, 1, 5, 5, 10, 10, 1, 1,\n",
       "                    5, 5, 10, 10, 1, 1, 5, 5, 10, 10, 1, 1, 5, 5, 10, 10,\n",
       "                    1, 1, 5, 5, 10, 10],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_subsample': masked_array(data=[0.5, 0.8, 0.5, 0.8, 0.5, 0.8, 0.5, 0.8, 0.5, 0.8, 0.5,\n",
       "                    0.8, 0.5, 0.8, 0.5, 0.8, 0.5, 0.8, 0.5, 0.8, 0.5, 0.8,\n",
       "                    0.5, 0.8, 0.5, 0.8, 0.5, 0.8, 0.5, 0.8, 0.5, 0.8, 0.5,\n",
       "                    0.8, 0.5, 0.8, 0.5, 0.8, 0.5, 0.8, 0.5, 0.8, 0.5, 0.8,\n",
       "                    0.5, 0.8, 0.5, 0.8, 0.5, 0.8, 0.5, 0.8, 0.5, 0.8, 0.5,\n",
       "                    0.8, 0.5, 0.8, 0.5, 0.8, 0.5, 0.8, 0.5, 0.8, 0.5, 0.8,\n",
       "                    0.5, 0.8, 0.5, 0.8, 0.5, 0.8],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'n_estimators': 50,\n",
       "   'scale_pos_weight': 1,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'n_estimators': 50,\n",
       "   'scale_pos_weight': 1,\n",
       "   'subsample': 0.8},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'n_estimators': 50,\n",
       "   'scale_pos_weight': 5,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'n_estimators': 50,\n",
       "   'scale_pos_weight': 5,\n",
       "   'subsample': 0.8},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'n_estimators': 50,\n",
       "   'scale_pos_weight': 10,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'n_estimators': 50,\n",
       "   'scale_pos_weight': 10,\n",
       "   'subsample': 0.8},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'n_estimators': 100,\n",
       "   'scale_pos_weight': 1,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'n_estimators': 100,\n",
       "   'scale_pos_weight': 1,\n",
       "   'subsample': 0.8},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'n_estimators': 100,\n",
       "   'scale_pos_weight': 5,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'n_estimators': 100,\n",
       "   'scale_pos_weight': 5,\n",
       "   'subsample': 0.8},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'n_estimators': 100,\n",
       "   'scale_pos_weight': 10,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'n_estimators': 100,\n",
       "   'scale_pos_weight': 10,\n",
       "   'subsample': 0.8},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'n_estimators': 300,\n",
       "   'scale_pos_weight': 1,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'n_estimators': 300,\n",
       "   'scale_pos_weight': 1,\n",
       "   'subsample': 0.8},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'n_estimators': 300,\n",
       "   'scale_pos_weight': 5,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'n_estimators': 300,\n",
       "   'scale_pos_weight': 5,\n",
       "   'subsample': 0.8},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'n_estimators': 300,\n",
       "   'scale_pos_weight': 10,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'n_estimators': 300,\n",
       "   'scale_pos_weight': 10,\n",
       "   'subsample': 0.8},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_estimators': 50,\n",
       "   'scale_pos_weight': 1,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_estimators': 50,\n",
       "   'scale_pos_weight': 1,\n",
       "   'subsample': 0.8},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_estimators': 50,\n",
       "   'scale_pos_weight': 5,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_estimators': 50,\n",
       "   'scale_pos_weight': 5,\n",
       "   'subsample': 0.8},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_estimators': 50,\n",
       "   'scale_pos_weight': 10,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_estimators': 50,\n",
       "   'scale_pos_weight': 10,\n",
       "   'subsample': 0.8},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_estimators': 100,\n",
       "   'scale_pos_weight': 1,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_estimators': 100,\n",
       "   'scale_pos_weight': 1,\n",
       "   'subsample': 0.8},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_estimators': 100,\n",
       "   'scale_pos_weight': 5,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_estimators': 100,\n",
       "   'scale_pos_weight': 5,\n",
       "   'subsample': 0.8},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_estimators': 100,\n",
       "   'scale_pos_weight': 10,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_estimators': 100,\n",
       "   'scale_pos_weight': 10,\n",
       "   'subsample': 0.8},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_estimators': 300,\n",
       "   'scale_pos_weight': 1,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_estimators': 300,\n",
       "   'scale_pos_weight': 1,\n",
       "   'subsample': 0.8},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_estimators': 300,\n",
       "   'scale_pos_weight': 5,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_estimators': 300,\n",
       "   'scale_pos_weight': 5,\n",
       "   'subsample': 0.8},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_estimators': 300,\n",
       "   'scale_pos_weight': 10,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_estimators': 300,\n",
       "   'scale_pos_weight': 10,\n",
       "   'subsample': 0.8},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.01,\n",
       "   'n_estimators': 50,\n",
       "   'scale_pos_weight': 1,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.01,\n",
       "   'n_estimators': 50,\n",
       "   'scale_pos_weight': 1,\n",
       "   'subsample': 0.8},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.01,\n",
       "   'n_estimators': 50,\n",
       "   'scale_pos_weight': 5,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.01,\n",
       "   'n_estimators': 50,\n",
       "   'scale_pos_weight': 5,\n",
       "   'subsample': 0.8},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.01,\n",
       "   'n_estimators': 50,\n",
       "   'scale_pos_weight': 10,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.01,\n",
       "   'n_estimators': 50,\n",
       "   'scale_pos_weight': 10,\n",
       "   'subsample': 0.8},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.01,\n",
       "   'n_estimators': 100,\n",
       "   'scale_pos_weight': 1,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.01,\n",
       "   'n_estimators': 100,\n",
       "   'scale_pos_weight': 1,\n",
       "   'subsample': 0.8},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.01,\n",
       "   'n_estimators': 100,\n",
       "   'scale_pos_weight': 5,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.01,\n",
       "   'n_estimators': 100,\n",
       "   'scale_pos_weight': 5,\n",
       "   'subsample': 0.8},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.01,\n",
       "   'n_estimators': 100,\n",
       "   'scale_pos_weight': 10,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.01,\n",
       "   'n_estimators': 100,\n",
       "   'scale_pos_weight': 10,\n",
       "   'subsample': 0.8},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.01,\n",
       "   'n_estimators': 300,\n",
       "   'scale_pos_weight': 1,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.01,\n",
       "   'n_estimators': 300,\n",
       "   'scale_pos_weight': 1,\n",
       "   'subsample': 0.8},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.01,\n",
       "   'n_estimators': 300,\n",
       "   'scale_pos_weight': 5,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.01,\n",
       "   'n_estimators': 300,\n",
       "   'scale_pos_weight': 5,\n",
       "   'subsample': 0.8},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.01,\n",
       "   'n_estimators': 300,\n",
       "   'scale_pos_weight': 10,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.01,\n",
       "   'n_estimators': 300,\n",
       "   'scale_pos_weight': 10,\n",
       "   'subsample': 0.8},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_estimators': 50,\n",
       "   'scale_pos_weight': 1,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_estimators': 50,\n",
       "   'scale_pos_weight': 1,\n",
       "   'subsample': 0.8},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_estimators': 50,\n",
       "   'scale_pos_weight': 5,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_estimators': 50,\n",
       "   'scale_pos_weight': 5,\n",
       "   'subsample': 0.8},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_estimators': 50,\n",
       "   'scale_pos_weight': 10,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_estimators': 50,\n",
       "   'scale_pos_weight': 10,\n",
       "   'subsample': 0.8},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_estimators': 100,\n",
       "   'scale_pos_weight': 1,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_estimators': 100,\n",
       "   'scale_pos_weight': 1,\n",
       "   'subsample': 0.8},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_estimators': 100,\n",
       "   'scale_pos_weight': 5,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_estimators': 100,\n",
       "   'scale_pos_weight': 5,\n",
       "   'subsample': 0.8},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_estimators': 100,\n",
       "   'scale_pos_weight': 10,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_estimators': 100,\n",
       "   'scale_pos_weight': 10,\n",
       "   'subsample': 0.8},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_estimators': 300,\n",
       "   'scale_pos_weight': 1,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_estimators': 300,\n",
       "   'scale_pos_weight': 1,\n",
       "   'subsample': 0.8},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_estimators': 300,\n",
       "   'scale_pos_weight': 5,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_estimators': 300,\n",
       "   'scale_pos_weight': 5,\n",
       "   'subsample': 0.8},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_estimators': 300,\n",
       "   'scale_pos_weight': 10,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_estimators': 300,\n",
       "   'scale_pos_weight': 10,\n",
       "   'subsample': 0.8}],\n",
       " 'split0_test_score': array([0.92247484, 0.92209409, 0.91945113, 0.91935088, 0.91620233,\n",
       "        0.91681422, 0.92377823, 0.92361828, 0.92116429, 0.92117194,\n",
       "        0.91828161, 0.91893941, 0.92799162, 0.92795851, 0.92657644,\n",
       "        0.92692004, 0.9246338 , 0.92502168, 0.92876078, 0.92879179,\n",
       "        0.92721759, 0.92817688, 0.92615543, 0.92751928, 0.92898697,\n",
       "        0.9296536 , 0.92846039, 0.92907769, 0.92686454, 0.92876051,\n",
       "        0.92646024, 0.92793084, 0.92597092, 0.92723345, 0.9241663 ,\n",
       "        0.92626001, 0.9196115 , 0.91769115, 0.91667544, 0.91636616,\n",
       "        0.91352385, 0.91444965, 0.92189487, 0.92067427, 0.91911696,\n",
       "        0.91859225, 0.91625735, 0.91657031, 0.92737855, 0.92669188,\n",
       "        0.92600669, 0.92564793, 0.92442461, 0.92359006, 0.92842946,\n",
       "        0.92805697, 0.92641397, 0.92781271, 0.92579067, 0.92625789,\n",
       "        0.92870341, 0.92913654, 0.92695612, 0.92840309, 0.92675054,\n",
       "        0.92734681, 0.92530791, 0.92709536, 0.92456335, 0.92577799,\n",
       "        0.92273185, 0.92385833]),\n",
       " 'split1_test_score': array([0.91954059, 0.9196296 , 0.91792064, 0.91766928, 0.91556331,\n",
       "        0.91595133, 0.92129311, 0.92121772, 0.91946034, 0.9192448 ,\n",
       "        0.91749899, 0.91744332, 0.9249967 , 0.92495958, 0.92388929,\n",
       "        0.92392004, 0.92231569, 0.92248526, 0.9265165 , 0.92527431,\n",
       "        0.92384876, 0.92496725, 0.92252553, 0.9242954 , 0.92733463,\n",
       "        0.92651062, 0.92459784, 0.92595522, 0.92410536, 0.92499753,\n",
       "        0.92409599, 0.92542851, 0.92228151, 0.92378992, 0.92187722,\n",
       "        0.92361993, 0.91616475, 0.91463711, 0.91416637, 0.91300183,\n",
       "        0.91271933, 0.91158755, 0.91911133, 0.9180652 , 0.91680039,\n",
       "        0.9158001 , 0.91498224, 0.91384271, 0.92449398, 0.92394899,\n",
       "        0.92297928, 0.9225777 , 0.921213  , 0.92112292, 0.92492035,\n",
       "        0.92449339, 0.9250592 , 0.92468061, 0.92155604, 0.923364  ,\n",
       "        0.92518902, 0.92547857, 0.92502509, 0.92515141, 0.9232049 ,\n",
       "        0.92431769, 0.92232089, 0.92466362, 0.92148099, 0.92340487,\n",
       "        0.92005828, 0.92226796]),\n",
       " 'split2_test_score': array([0.92520343, 0.92542809, 0.92392654, 0.92399209, 0.92179823,\n",
       "        0.92221273, 0.92699545, 0.92690243, 0.92532191, 0.9252932 ,\n",
       "        0.92338042, 0.92396028, 0.93029863, 0.93004088, 0.92963903,\n",
       "        0.92934915, 0.92847914, 0.92835635, 0.929435  , 0.92978869,\n",
       "        0.92946757, 0.92953767, 0.92822649, 0.92921292, 0.93049136,\n",
       "        0.93008007, 0.93024363, 0.93062967, 0.92845752, 0.93014254,\n",
       "        0.92819692, 0.92814827, 0.92556662, 0.9279532 , 0.92537831,\n",
       "        0.92731265, 0.92248711, 0.92109149, 0.92147288, 0.92108381,\n",
       "        0.91898904, 0.91888491, 0.92521597, 0.92414598, 0.92355876,\n",
       "        0.92290846, 0.92129   , 0.92084005, 0.92982387, 0.92865369,\n",
       "        0.92873557, 0.9282025 , 0.92724886, 0.92675055, 0.92928866,\n",
       "        0.92905093, 0.92945763, 0.92995889, 0.92825895, 0.92860248,\n",
       "        0.92938813, 0.92982689, 0.92980081, 0.93060143, 0.92859412,\n",
       "        0.9297734 , 0.92730057, 0.92774132, 0.92676156, 0.92786351,\n",
       "        0.92392329, 0.92716731]),\n",
       " 'split3_test_score': array([0.92466102, 0.92428358, 0.92251135, 0.92299473, 0.92076468,\n",
       "        0.9208286 , 0.92623805, 0.92586742, 0.92462114, 0.92456601,\n",
       "        0.92260527, 0.92276988, 0.93007831, 0.92969722, 0.92902398,\n",
       "        0.92881121, 0.92753551, 0.92768747, 0.93019522, 0.92967575,\n",
       "        0.92859833, 0.92879215, 0.92760523, 0.92863058, 0.93036347,\n",
       "        0.93019268, 0.92895361, 0.92925576, 0.9286332 , 0.92926219,\n",
       "        0.9272319 , 0.92895313, 0.9256216 , 0.92726824, 0.9256598 ,\n",
       "        0.92664183, 0.92218373, 0.92033769, 0.9201769 , 0.91946883,\n",
       "        0.91738056, 0.91717349, 0.92439228, 0.92316913, 0.92271882,\n",
       "        0.92194086, 0.91987262, 0.91953046, 0.92959646, 0.92865979,\n",
       "        0.92813586, 0.92786812, 0.9264499 , 0.92639309, 0.92954622,\n",
       "        0.92918324, 0.9289485 , 0.92952135, 0.92747346, 0.92828348,\n",
       "        0.93011543, 0.92991199, 0.92943322, 0.92959598, 0.92802562,\n",
       "        0.92925477, 0.92657705, 0.92738653, 0.92643254, 0.92719963,\n",
       "        0.92426351, 0.92721378]),\n",
       " 'split4_test_score': array([0.92129751, 0.9215427 , 0.9186121 , 0.91908283, 0.91627901,\n",
       "        0.91716756, 0.92293465, 0.92285501, 0.92034516, 0.92069217,\n",
       "        0.9177597 , 0.91870422, 0.92679207, 0.9266292 , 0.92519349,\n",
       "        0.92546055, 0.92345569, 0.92389475, 0.92664873, 0.92775512,\n",
       "        0.92573772, 0.92723337, 0.92403497, 0.92589086, 0.92782895,\n",
       "        0.92870192, 0.92642461, 0.92816054, 0.92452973, 0.92636132,\n",
       "        0.92558735, 0.92690405, 0.92352351, 0.92577631, 0.92188353,\n",
       "        0.92381865, 0.91827695, 0.91698456, 0.91625678, 0.91492308,\n",
       "        0.91302546, 0.91258899, 0.92086789, 0.91995383, 0.91832964,\n",
       "        0.91745557, 0.91573608, 0.91500969, 0.92632058, 0.92591999,\n",
       "        0.92453345, 0.92437016, 0.92286986, 0.92274522, 0.92599363,\n",
       "        0.92682474, 0.92512761, 0.92592902, 0.92430205, 0.92523616,\n",
       "        0.92704957, 0.92774717, 0.92598564, 0.92697337, 0.92544088,\n",
       "        0.9261181 , 0.92361735, 0.92590684, 0.92283111, 0.92449602,\n",
       "        0.92290377, 0.92381748]),\n",
       " 'mean_test_score': array([0.92263548, 0.92259561, 0.92048435, 0.92061796, 0.91812151,\n",
       "        0.91859489, 0.9242479 , 0.92409217, 0.92218257, 0.92219362,\n",
       "        0.9199052 , 0.92036342, 0.92803147, 0.92785708, 0.92686445,\n",
       "        0.9268922 , 0.92528397, 0.9254891 , 0.92831124, 0.92825713,\n",
       "        0.926974  , 0.92774146, 0.92570953, 0.92710981, 0.92900108,\n",
       "        0.92902778, 0.92773602, 0.92861577, 0.92651807, 0.92790482,\n",
       "        0.92631448, 0.92747296, 0.92459283, 0.92640422, 0.92379304,\n",
       "        0.92553061, 0.91974481, 0.9181484 , 0.91774967, 0.91696874,\n",
       "        0.91512765, 0.91493692, 0.92229647, 0.92120168, 0.92010491,\n",
       "        0.91933945, 0.91762766, 0.91715864, 0.92752269, 0.92677487,\n",
       "        0.92607817, 0.92573328, 0.92444125, 0.92412037, 0.92763566,\n",
       "        0.92752186, 0.92700138, 0.92758052, 0.92547623, 0.9263488 ,\n",
       "        0.92808911, 0.92842023, 0.92744017, 0.92814505, 0.92640321,\n",
       "        0.92736216, 0.92502476, 0.92655873, 0.92441391, 0.9257484 ,\n",
       "        0.92277614, 0.92486497]),\n",
       " 'std_test_score': array([0.00210197, 0.00205083, 0.00232821, 0.00243682, 0.00261255,\n",
       "        0.00246068, 0.00210645, 0.00205264, 0.00235056, 0.00233364,\n",
       "        0.00254545, 0.00253125, 0.00200385, 0.00190098, 0.00219493,\n",
       "        0.00202994, 0.00236028, 0.00222879, 0.0014832 , 0.00166092,\n",
       "        0.00201053, 0.00157951, 0.00214801, 0.00180611, 0.00128282,\n",
       "        0.00136369, 0.00199315, 0.0015469 , 0.0019042 , 0.00191921,\n",
       "        0.00140406, 0.00121326, 0.00144162, 0.00148743, 0.00164035,\n",
       "        0.00151816, 0.0023857 , 0.0023387 , 0.0026824 , 0.00294851,\n",
       "        0.00256036, 0.00274071, 0.0022478 , 0.00220057, 0.00260035,\n",
       "        0.00268848, 0.00248623, 0.00265096, 0.00201171, 0.00177799,\n",
       "        0.00215825, 0.00212027, 0.00222749, 0.00215593, 0.00184835,\n",
       "        0.00173448, 0.00186829, 0.00203048, 0.00239073, 0.0019482 ,\n",
       "        0.00176985, 0.00166274, 0.00188295, 0.00192563, 0.00193433,\n",
       "        0.00201061, 0.00184179, 0.00113032, 0.00203548, 0.00165174,\n",
       "        0.00147873, 0.00198357]),\n",
       " 'rank_test_score': array([51, 52, 58, 57, 66, 64, 46, 48, 55, 54, 61, 59,  9, 11, 25, 24, 40,\n",
       "        38,  5,  6, 23, 12, 36, 21,  2,  1, 13,  3, 28, 10, 32, 18, 43, 29,\n",
       "        49, 37, 62, 65, 67, 70, 71, 72, 53, 56, 60, 63, 68, 69, 16, 26, 33,\n",
       "        35, 44, 47, 14, 17, 22, 15, 39, 31,  8,  4, 19,  7, 30, 20, 41, 27,\n",
       "        45, 34, 50, 42])}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d5145c94-bff4-4867-a6e6-e365a8d89d0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>StandardScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Normalization of data for Neural Networks\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc= StandardScaler()\n",
    "\n",
    "sc.fit(X_train_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "010061af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters {'colsample_bytree': 0.5, 'learning_rate': 0.01, 'n_estimators': 50, 'scale_pos_weight': 1, 'subsample': 0.5}\n",
      "AUC Score 0.9226354793632909\n",
      "Parameters {'colsample_bytree': 0.5, 'learning_rate': 0.01, 'n_estimators': 50, 'scale_pos_weight': 1, 'subsample': 0.8}\n",
      "AUC Score 0.9225956136548605\n",
      "Parameters {'colsample_bytree': 0.5, 'learning_rate': 0.01, 'n_estimators': 50, 'scale_pos_weight': 5, 'subsample': 0.5}\n",
      "AUC Score 0.9204843503118869\n",
      "Parameters {'colsample_bytree': 0.5, 'learning_rate': 0.01, 'n_estimators': 50, 'scale_pos_weight': 5, 'subsample': 0.8}\n",
      "AUC Score 0.9206179638296469\n",
      "Parameters {'colsample_bytree': 0.5, 'learning_rate': 0.01, 'n_estimators': 50, 'scale_pos_weight': 10, 'subsample': 0.5}\n",
      "AUC Score 0.9181215139736347\n",
      "Parameters {'colsample_bytree': 0.5, 'learning_rate': 0.01, 'n_estimators': 50, 'scale_pos_weight': 10, 'subsample': 0.8}\n",
      "AUC Score 0.9185948856921726\n",
      "Parameters {'colsample_bytree': 0.5, 'learning_rate': 0.01, 'n_estimators': 100, 'scale_pos_weight': 1, 'subsample': 0.5}\n",
      "AUC Score 0.9242478984740423\n",
      "Parameters {'colsample_bytree': 0.5, 'learning_rate': 0.01, 'n_estimators': 100, 'scale_pos_weight': 1, 'subsample': 0.8}\n",
      "AUC Score 0.924092172997424\n",
      "Parameters {'colsample_bytree': 0.5, 'learning_rate': 0.01, 'n_estimators': 100, 'scale_pos_weight': 5, 'subsample': 0.5}\n",
      "AUC Score 0.9221825652119044\n",
      "Parameters {'colsample_bytree': 0.5, 'learning_rate': 0.01, 'n_estimators': 100, 'scale_pos_weight': 5, 'subsample': 0.8}\n",
      "AUC Score 0.9221936241460863\n",
      "Parameters {'colsample_bytree': 0.5, 'learning_rate': 0.01, 'n_estimators': 100, 'scale_pos_weight': 10, 'subsample': 0.5}\n",
      "AUC Score 0.9199051985978889\n",
      "Parameters {'colsample_bytree': 0.5, 'learning_rate': 0.01, 'n_estimators': 100, 'scale_pos_weight': 10, 'subsample': 0.8}\n",
      "AUC Score 0.920363421109305\n",
      "Parameters {'colsample_bytree': 0.5, 'learning_rate': 0.01, 'n_estimators': 300, 'scale_pos_weight': 1, 'subsample': 0.5}\n",
      "AUC Score 0.9280314665114142\n",
      "Parameters {'colsample_bytree': 0.5, 'learning_rate': 0.01, 'n_estimators': 300, 'scale_pos_weight': 1, 'subsample': 0.8}\n",
      "AUC Score 0.92785707964244\n",
      "Parameters {'colsample_bytree': 0.5, 'learning_rate': 0.01, 'n_estimators': 300, 'scale_pos_weight': 5, 'subsample': 0.5}\n",
      "AUC Score 0.9268644469886539\n",
      "Parameters {'colsample_bytree': 0.5, 'learning_rate': 0.01, 'n_estimators': 300, 'scale_pos_weight': 5, 'subsample': 0.8}\n",
      "AUC Score 0.926892197630653\n",
      "Parameters {'colsample_bytree': 0.5, 'learning_rate': 0.01, 'n_estimators': 300, 'scale_pos_weight': 10, 'subsample': 0.5}\n",
      "AUC Score 0.9252839667064576\n",
      "Parameters {'colsample_bytree': 0.5, 'learning_rate': 0.01, 'n_estimators': 300, 'scale_pos_weight': 10, 'subsample': 0.8}\n",
      "AUC Score 0.9254891025332652\n",
      "Parameters {'colsample_bytree': 0.5, 'learning_rate': 0.1, 'n_estimators': 50, 'scale_pos_weight': 1, 'subsample': 0.5}\n",
      "AUC Score 0.9283112432382264\n",
      "Parameters {'colsample_bytree': 0.5, 'learning_rate': 0.1, 'n_estimators': 50, 'scale_pos_weight': 1, 'subsample': 0.8}\n",
      "AUC Score 0.9282571295545559\n",
      "Parameters {'colsample_bytree': 0.5, 'learning_rate': 0.1, 'n_estimators': 50, 'scale_pos_weight': 5, 'subsample': 0.5}\n",
      "AUC Score 0.9269739952178753\n",
      "Parameters {'colsample_bytree': 0.5, 'learning_rate': 0.1, 'n_estimators': 50, 'scale_pos_weight': 5, 'subsample': 0.8}\n",
      "AUC Score 0.9277414631748439\n",
      "Parameters {'colsample_bytree': 0.5, 'learning_rate': 0.1, 'n_estimators': 50, 'scale_pos_weight': 10, 'subsample': 0.5}\n",
      "AUC Score 0.9257095278258541\n",
      "Parameters {'colsample_bytree': 0.5, 'learning_rate': 0.1, 'n_estimators': 50, 'scale_pos_weight': 10, 'subsample': 0.8}\n",
      "AUC Score 0.9271098091827991\n",
      "Parameters {'colsample_bytree': 0.5, 'learning_rate': 0.1, 'n_estimators': 100, 'scale_pos_weight': 1, 'subsample': 0.5}\n",
      "AUC Score 0.9290010760057126\n",
      "Parameters {'colsample_bytree': 0.5, 'learning_rate': 0.1, 'n_estimators': 100, 'scale_pos_weight': 1, 'subsample': 0.8}\n",
      "AUC Score 0.9290277787837091\n",
      "Parameters {'colsample_bytree': 0.5, 'learning_rate': 0.1, 'n_estimators': 100, 'scale_pos_weight': 5, 'subsample': 0.5}\n",
      "AUC Score 0.9277360169412072\n",
      "Parameters {'colsample_bytree': 0.5, 'learning_rate': 0.1, 'n_estimators': 100, 'scale_pos_weight': 5, 'subsample': 0.8}\n",
      "AUC Score 0.9286157744511068\n",
      "Parameters {'colsample_bytree': 0.5, 'learning_rate': 0.1, 'n_estimators': 100, 'scale_pos_weight': 10, 'subsample': 0.5}\n",
      "AUC Score 0.9265180719567004\n",
      "Parameters {'colsample_bytree': 0.5, 'learning_rate': 0.1, 'n_estimators': 100, 'scale_pos_weight': 10, 'subsample': 0.8}\n",
      "AUC Score 0.9279048195784739\n",
      "Parameters {'colsample_bytree': 0.5, 'learning_rate': 0.1, 'n_estimators': 300, 'scale_pos_weight': 1, 'subsample': 0.5}\n",
      "AUC Score 0.926314479853977\n",
      "Parameters {'colsample_bytree': 0.5, 'learning_rate': 0.1, 'n_estimators': 300, 'scale_pos_weight': 1, 'subsample': 0.8}\n",
      "AUC Score 0.9274729594100115\n",
      "Parameters {'colsample_bytree': 0.5, 'learning_rate': 0.1, 'n_estimators': 300, 'scale_pos_weight': 5, 'subsample': 0.5}\n",
      "AUC Score 0.9245928296788538\n",
      "Parameters {'colsample_bytree': 0.5, 'learning_rate': 0.1, 'n_estimators': 300, 'scale_pos_weight': 5, 'subsample': 0.8}\n",
      "AUC Score 0.9264042216509004\n",
      "Parameters {'colsample_bytree': 0.5, 'learning_rate': 0.1, 'n_estimators': 300, 'scale_pos_weight': 10, 'subsample': 0.5}\n",
      "AUC Score 0.9237930351417718\n",
      "Parameters {'colsample_bytree': 0.5, 'learning_rate': 0.1, 'n_estimators': 300, 'scale_pos_weight': 10, 'subsample': 0.8}\n",
      "AUC Score 0.9255306124961733\n",
      "Parameters {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'n_estimators': 50, 'scale_pos_weight': 1, 'subsample': 0.5}\n",
      "AUC Score 0.9197448071856463\n",
      "Parameters {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'n_estimators': 50, 'scale_pos_weight': 1, 'subsample': 0.8}\n",
      "AUC Score 0.9181483982149203\n",
      "Parameters {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'n_estimators': 50, 'scale_pos_weight': 5, 'subsample': 0.5}\n",
      "AUC Score 0.9177496737602355\n",
      "Parameters {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'n_estimators': 50, 'scale_pos_weight': 5, 'subsample': 0.8}\n",
      "AUC Score 0.916968744712\n",
      "Parameters {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'n_estimators': 50, 'scale_pos_weight': 10, 'subsample': 0.5}\n",
      "AUC Score 0.915127648249169\n",
      "Parameters {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'n_estimators': 50, 'scale_pos_weight': 10, 'subsample': 0.8}\n",
      "AUC Score 0.9149369158701994\n",
      "Parameters {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'n_estimators': 100, 'scale_pos_weight': 1, 'subsample': 0.5}\n",
      "AUC Score 0.9222964686683384\n",
      "Parameters {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'n_estimators': 100, 'scale_pos_weight': 1, 'subsample': 0.8}\n",
      "AUC Score 0.921201683100685\n",
      "Parameters {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'n_estimators': 100, 'scale_pos_weight': 5, 'subsample': 0.5}\n",
      "AUC Score 0.9201049135913063\n",
      "Parameters {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'n_estimators': 100, 'scale_pos_weight': 5, 'subsample': 0.8}\n",
      "AUC Score 0.9193394481839199\n",
      "Parameters {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'n_estimators': 100, 'scale_pos_weight': 10, 'subsample': 0.5}\n",
      "AUC Score 0.9176276560976795\n",
      "Parameters {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'n_estimators': 100, 'scale_pos_weight': 10, 'subsample': 0.8}\n",
      "AUC Score 0.9171586446091954\n",
      "Parameters {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'n_estimators': 300, 'scale_pos_weight': 1, 'subsample': 0.5}\n",
      "AUC Score 0.9275226905431045\n",
      "Parameters {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'n_estimators': 300, 'scale_pos_weight': 1, 'subsample': 0.8}\n",
      "AUC Score 0.9267748682758523\n",
      "Parameters {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'n_estimators': 300, 'scale_pos_weight': 5, 'subsample': 0.5}\n",
      "AUC Score 0.926078168710192\n",
      "Parameters {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'n_estimators': 300, 'scale_pos_weight': 5, 'subsample': 0.8}\n",
      "AUC Score 0.9257332813207164\n",
      "Parameters {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'n_estimators': 300, 'scale_pos_weight': 10, 'subsample': 0.5}\n",
      "AUC Score 0.9244412476060777\n",
      "Parameters {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'n_estimators': 300, 'scale_pos_weight': 10, 'subsample': 0.8}\n",
      "AUC Score 0.9241203687698455\n",
      "Parameters {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'n_estimators': 50, 'scale_pos_weight': 1, 'subsample': 0.5}\n",
      "AUC Score 0.9276356636279285\n",
      "Parameters {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'n_estimators': 50, 'scale_pos_weight': 1, 'subsample': 0.8}\n",
      "AUC Score 0.9275218563760085\n",
      "Parameters {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'n_estimators': 50, 'scale_pos_weight': 5, 'subsample': 0.5}\n",
      "AUC Score 0.9270013793243491\n",
      "Parameters {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'n_estimators': 50, 'scale_pos_weight': 5, 'subsample': 0.8}\n",
      "AUC Score 0.9275805171191358\n",
      "Parameters {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'n_estimators': 50, 'scale_pos_weight': 10, 'subsample': 0.5}\n",
      "AUC Score 0.9254762340940033\n",
      "Parameters {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'n_estimators': 50, 'scale_pos_weight': 10, 'subsample': 0.8}\n",
      "AUC Score 0.926348802814109\n",
      "Parameters {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'n_estimators': 100, 'scale_pos_weight': 1, 'subsample': 0.5}\n",
      "AUC Score 0.9280891144407842\n",
      "Parameters {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'n_estimators': 100, 'scale_pos_weight': 1, 'subsample': 0.8}\n",
      "AUC Score 0.9284202317052783\n",
      "Parameters {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'n_estimators': 100, 'scale_pos_weight': 5, 'subsample': 0.5}\n",
      "AUC Score 0.9274401746312732\n",
      "Parameters {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'n_estimators': 100, 'scale_pos_weight': 5, 'subsample': 0.8}\n",
      "AUC Score 0.9281450547438258\n",
      "Parameters {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'n_estimators': 100, 'scale_pos_weight': 10, 'subsample': 0.5}\n",
      "AUC Score 0.9264032142042687\n",
      "Parameters {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'n_estimators': 100, 'scale_pos_weight': 10, 'subsample': 0.8}\n",
      "AUC Score 0.9273621558481354\n",
      "Parameters {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'n_estimators': 300, 'scale_pos_weight': 1, 'subsample': 0.5}\n",
      "AUC Score 0.9250247552242504\n",
      "Parameters {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'n_estimators': 300, 'scale_pos_weight': 1, 'subsample': 0.8}\n",
      "AUC Score 0.9265587339162767\n",
      "Parameters {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'n_estimators': 300, 'scale_pos_weight': 5, 'subsample': 0.5}\n",
      "AUC Score 0.9244139089010954\n",
      "Parameters {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'n_estimators': 300, 'scale_pos_weight': 5, 'subsample': 0.8}\n",
      "AUC Score 0.9257484011184383\n",
      "Parameters {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'n_estimators': 300, 'scale_pos_weight': 10, 'subsample': 0.5}\n",
      "AUC Score 0.9227761382752911\n",
      "Parameters {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'n_estimators': 300, 'scale_pos_weight': 10, 'subsample': 0.8}\n",
      "AUC Score 0.9248649718879218\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results = grid_search.cv_results_\n",
    "for i in range(len(results['params'])):\n",
    "    print(\"Parameters\",results['params'][i])\n",
    "    print(\"AUC Score\",results['mean_test_score'][i])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "286985dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>auc_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'colsample_bytree': 0.5, 'learning_rate': 0.0...</td>\n",
       "      <td>0.922635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'colsample_bytree': 0.5, 'learning_rate': 0.0...</td>\n",
       "      <td>0.922596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'colsample_bytree': 0.5, 'learning_rate': 0.0...</td>\n",
       "      <td>0.920484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'colsample_bytree': 0.5, 'learning_rate': 0.0...</td>\n",
       "      <td>0.920618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'colsample_bytree': 0.5, 'learning_rate': 0.0...</td>\n",
       "      <td>0.918122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>{'colsample_bytree': 1.0, 'learning_rate': 0.1...</td>\n",
       "      <td>0.926559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>{'colsample_bytree': 1.0, 'learning_rate': 0.1...</td>\n",
       "      <td>0.924414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>{'colsample_bytree': 1.0, 'learning_rate': 0.1...</td>\n",
       "      <td>0.925748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>{'colsample_bytree': 1.0, 'learning_rate': 0.1...</td>\n",
       "      <td>0.922776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>{'colsample_bytree': 1.0, 'learning_rate': 0.1...</td>\n",
       "      <td>0.924865</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               params  auc_score\n",
       "0   {'colsample_bytree': 0.5, 'learning_rate': 0.0...   0.922635\n",
       "1   {'colsample_bytree': 0.5, 'learning_rate': 0.0...   0.922596\n",
       "2   {'colsample_bytree': 0.5, 'learning_rate': 0.0...   0.920484\n",
       "3   {'colsample_bytree': 0.5, 'learning_rate': 0.0...   0.920618\n",
       "4   {'colsample_bytree': 0.5, 'learning_rate': 0.0...   0.918122\n",
       "..                                                ...        ...\n",
       "67  {'colsample_bytree': 1.0, 'learning_rate': 0.1...   0.926559\n",
       "68  {'colsample_bytree': 1.0, 'learning_rate': 0.1...   0.924414\n",
       "69  {'colsample_bytree': 1.0, 'learning_rate': 0.1...   0.925748\n",
       "70  {'colsample_bytree': 1.0, 'learning_rate': 0.1...   0.922776\n",
       "71  {'colsample_bytree': 1.0, 'learning_rate': 0.1...   0.924865\n",
       "\n",
       "[72 rows x 2 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'params': results['params'],\n",
    "    'auc_score': results['mean_test_score']\n",
    "})\n",
    "\n",
    "# Write the dataframe to a CSV file\n",
    "results_df.to_csv('grid_search_results_xgb.csv', index=False)\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3fc98a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best AUC: 0.9290277787837091\n",
      "Best parameters: {'colsample_bytree': 0.5, 'learning_rate': 0.1, 'n_estimators': 100, 'scale_pos_weight': 1, 'subsample': 0.8}\n"
     ]
    }
   ],
   "source": [
    "best_params = grid_search.best_params_\n",
    "best_auc = grid_search.best_score_\n",
    "print(\"Best AUC:\", best_auc)\n",
    "print(\"Best parameters:\", best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3dade29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC score on test data: 0.9178918613955542\n",
      "AUC score on test 2 data: 0.9411621654763803\n"
     ]
    }
   ],
   "source": [
    "#running xgb on hyper parameters\n",
    "params = {\n",
    "    'learning_rate': 0.1,\n",
    "    'n_estimators': 100,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.5,\n",
    "    'scale_pos_weight': 1\n",
    "}\n",
    "\n",
    "# Train the model\n",
    "xgb_best_model = xgb.XGBClassifier(**params)\n",
    "xgb_best_model.fit(X_train_selected, y_train)\n",
    "# Evaluate the model\n",
    "auc_score = roc_auc_score(y_test, xgb_best_model.predict_proba(X_test_selected)[:, 1])\n",
    "print(\"AUC score on test data:\", auc_score)\n",
    "\n",
    "auc_score1 = roc_auc_score(y_test1, xgb_best_model.predict_proba(X_test1_selected)[:, 1])\n",
    "print(\"AUC score on test 2 data:\", auc_score1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4749457c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sum</th>\n",
       "      <th>count</th>\n",
       "      <th>Bad Rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Score Bins</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(0.0, 0.00336]</th>\n",
       "      <td>1</td>\n",
       "      <td>5583</td>\n",
       "      <td>0.000179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0.00336, 0.00576]</th>\n",
       "      <td>5</td>\n",
       "      <td>5583</td>\n",
       "      <td>0.000896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0.00576, 0.0108]</th>\n",
       "      <td>11</td>\n",
       "      <td>5582</td>\n",
       "      <td>0.001971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0.0108, 0.0235]</th>\n",
       "      <td>36</td>\n",
       "      <td>5583</td>\n",
       "      <td>0.006448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0.0235, 0.0592]</th>\n",
       "      <td>122</td>\n",
       "      <td>5582</td>\n",
       "      <td>0.021856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0.0592, 0.171]</th>\n",
       "      <td>430</td>\n",
       "      <td>5583</td>\n",
       "      <td>0.077020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0.171, 0.398]</th>\n",
       "      <td>1337</td>\n",
       "      <td>5582</td>\n",
       "      <td>0.239520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0.398, 0.632]</th>\n",
       "      <td>2880</td>\n",
       "      <td>5583</td>\n",
       "      <td>0.515852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0.632, 0.809]</th>\n",
       "      <td>4261</td>\n",
       "      <td>5582</td>\n",
       "      <td>0.763346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0.809, 1.0]</th>\n",
       "      <td>5302</td>\n",
       "      <td>5583</td>\n",
       "      <td>0.949669</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     sum  count  Bad Rate\n",
       "Score Bins                               \n",
       "(0.0, 0.00336]         1   5583  0.000179\n",
       "(0.00336, 0.00576]     5   5583  0.000896\n",
       "(0.00576, 0.0108]     11   5582  0.001971\n",
       "(0.0108, 0.0235]      36   5583  0.006448\n",
       "(0.0235, 0.0592]     122   5582  0.021856\n",
       "(0.0592, 0.171]      430   5583  0.077020\n",
       "(0.171, 0.398]      1337   5582  0.239520\n",
       "(0.398, 0.632]      2880   5583  0.515852\n",
       "(0.632, 0.809]      4261   5582  0.763346\n",
       "(0.809, 1.0]        5302   5583  0.949669"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Rank ordering\n",
    "\n",
    "perf_train_data = pd.DataFrame({\"Actual\": y_train['target'], \"Prediction\": xgb_best_model.predict_proba(X_train_selected)[:,1]})\n",
    "quantiles = list(set(perf_train_data.Prediction.quantile([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])))\n",
    "quantiles.sort()\n",
    "quantiles.insert(0,0)\n",
    "quantiles.insert(len(quantiles),1)\n",
    "perf_train_data[\"Score Bins\"] = pd.cut(perf_train_data[\"Prediction\"], quantiles)\n",
    "stat = perf_train_data.groupby(\"Score Bins\")[\"Actual\"].agg([\"sum\", \"count\"])\n",
    "stat[\"Bad Rate\"] = stat[\"sum\"] / stat[\"count\"]\n",
    "stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f5f502fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAI+CAYAAABAJytmAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy88F64QAAAACXBIWXMAAA9hAAAPYQGoP6dpAAB0iElEQVR4nO3deVxUZfs/8M8MCIMgi8iqgOCKkqm4IbngAm64VI9bibvikrs8qblkmpnlkqWlluWjpqZpapqalWa4ouS+K6ACpiK4sl6/P/w53ya2YZHDnD7v14tXzbnPMJ8bx+HynPtcRyMiAiIiIiKV0CodgIiIiKg4sbghIiIiVWFxQ0RERKrC4oaIiIhUhcUNERERqQqLGyIiIlIVFjdERESkKixuiIiISFVY3BAREZGqsLghKmZff/01NBqNwZeTkxNatmyJ7du3F/vrVa5cGf369ct3v39msrW1RdOmTfHtt98W+rV37NiBGTNmFPr5uZkxYwY0Gg3u3LlT7N/770QE69atQ7NmzeDs7AydTodKlSohJCQEK1aseKGv/SK0bNnS4M+4TJkyqFy5MgYOHIiYmBiDfZ+/T69fv65MWKIXiMUN0QuycuVKHDx4EJGRkVi2bBnMzMwQGhqKbdu2KZbp9ddf12f6/PPPkZKSgt69e2Pt2rWF+n47duzAu+++W8wpS86kSZPQq1cv+Pr6YsWKFdi5cydmzZoFFxcX/PDDD0rHKxQfHx8cPHgQBw8exN69exEREYHt27ejWbNmePz4sX6/jh074uDBg3Bzc1MwLdGLYa50ACK18vPzQ4MGDfSP27VrBwcHB3z77bcIDQ1VJJOLiwuaNGkCAAgICEBgYCAqV66ML774Ar1791Ykk1KePHmChQsXIiwsDMuWLTMY69evH7Kysko8j5WVVZG/j5WVlf7PGACaN28OnU6HgQMH4sCBAwgODgYAODk5wcnJqcivR1Qa8cgNUQnR6XSwsLBAmTJlDLa/++67aNy4McqXLw9bW1vUr18fX375Jf55T9v09HRERETA1dUVZcuWxSuvvIIjR44UKZOXlxecnJyQmJhosH39+vUIDg6Gm5sbrKys4Ovri7fffhuPHj3S79OvXz989tlnAAxPeT0/zSEiWLJkCerWrQsrKys4ODjg9ddfx9WrV43OFxcXh1dffRW2traws7PDm2++ib/++ks/PnDgQJQvX97giMRzrVq1Qu3atXP93o8ePUJqamquRy60WsOPx9TUVMycORO+vr7Q6XRwdHREUFAQIiMj9fs8ffoUkyZNgre3NywsLFCxYkWMGDEC9+/fN/helStXRqdOnfD999+jXr160Ol0+iNgCQkJGDp0KCpVqgQLCwt4e3vj3XffRUZGRr4/r9zY2dkBgMF7L6fTUi1btoSfnx+OHj2KZs2aoWzZsvDx8cEHH3xgUOxlZWVh1qxZqFGjBqysrGBvb486depg0aJFhc5IVJx45IboBcnMzERGRgZEBImJiZg3bx4ePXqU7QjJ9evXMXToUHh6egIADh06hLfeegs3b97EtGnT9PsNHjwYq1atwoQJE9C2bVucPn0ar776Kh48eFDojMnJybh3757Bv/QB4NKlS+jQoQPGjBkDa2trnD9/HnPnzsWRI0fwyy+/AACmTp2KR48eYePGjTh48KD+uc+LhaFDh+Lrr7/GqFGjMHfuXNy7dw8zZ85E06ZN8eeff8LFxSXffN26dUP37t0RHh6OM2fOYOrUqTh79iwOHz6MMmXKYPTo0fjqq6+wdu1aDBo0SP+8s2fP4tdff9UXXzmpUKECqlatiiVLlsDZ2RkdOnRAjRo1oNFosu2bkZGB9u3b4/fff8eYMWPQqlUrZGRk4NChQ4iNjUXTpk0hIujatSv27t2LSZMmoVmzZjh58iSmT5+uP01kaWmp/57Hjx/HuXPn8M4778Db2xvW1tZISEhAo0aNoNVqMW3aNFSpUgUHDx7ErFmzcP36daxcuTLfn9nzvACQlpaG06dPY+bMmfDx8UHTpk3zfW5CQgLeeOMNjB8/HtOnT8fmzZsxadIkuLu7IywsDADw4YcfYsaMGXjnnXfQvHlzpKen4/z589mKOCLFCBEVq5UrVwqAbF+WlpayZMmSPJ+bmZkp6enpMnPmTHF0dJSsrCwRETl37pwAkLFjxxrsv2bNGgEgffv2zTcXABk+fLikp6dLWlqaXLx4UTp37izlypWTY8eO5fq8rKwsSU9Pl3379gkA+fPPP/VjI0aMkJw+Rg4ePCgA5OOPPzbYHhcXJ1ZWVhIREZFn1unTp+c539WrV+u3tWjRQurWrWuw37Bhw8TW1lYePHiQ5+scOXJEPD099X9G5cqVk06dOsmqVav0P3sRkVWrVgkAWb58ea7f66effhIA8uGHHxpsX79+vQCQZcuW6bd5eXmJmZmZXLhwwWDfoUOHio2NjcTExBhs/+ijjwSAnDlzJs/5tGjRIsf3XvXq1eXcuXMG+z5/n167di3b8w8fPmywb61atSQkJET/uFOnTtl+5kSlCYsbomL2/JfGqlWr5OjRo3L06FHZuXOnDBkyRDQajSxevNhg/71790rr1q3F1tY22y+lhIQEERFZsmSJAMhWhKSnp4u5ubnRxc0/v8qUKSPbt2/Ptu+VK1ekV69e4uLiIhqNxuA569at0++XW3EzZcoU0Wg0kpiYKOnp6QZfTZo0kUaNGuWZ9Xlxk9t8Bw4cqN/2/fffCwA5cOCAiIgkJyeLjY2NvPXWW/n+TERE0tLS5KeffpLJkydLcHCwWFlZCQDp1KmTvsDp1auX6HQ6yczMzPX7RERECAC5ffu2wfasrCyxtraWHj166Ld5eXlJvXr1sn2PihUrSmhoaLaf2ZkzZwRAvsVxixYtpEqVKvr33cGDB2Xt2rXy8ssvi7u7u1y8eFG/b27Fjaura7bv27NnT6lZs6b+8cyZM0Wj0ciwYcPkp59+kuTk5DxzEZU0npYiekF8fX2zLSiOiYlBREQE3nzzTdjb2+PIkSMIDg5Gy5YtsXz5cv06iy1btmD27Nl48uQJAODu3bsAAFdXV4PXMDc3h6Ojo9GZunfvjokTJyI9PR2nTp3CpEmT0LNnTxw/fhzVqlUDADx8+BDNmjWDTqfDrFmzUL16dZQtW1a//uV5prwkJiZCRHI99eTj42NU3tzm+/znAQBdunRB5cqV8dlnnyEwMBBff/01Hj16hBEjRhj1GmXKlEFISAhCQkIAPPtZv/7669i+fTt27tyJDh064K+//oK7u3u2dTh/d/fuXZibm2dbpKvRaODq6mqQGUCOa30SExOxbdu2bOuynjPm0nidTmfwvmvSpAlatmyJihUrYtq0afle+p/T+8nS0tLgz33SpEmwtrbG6tWr8fnnn8PMzAzNmzfH3LlzDV6bSCksbohKUJ06dbBr1y5cvHgRjRo1wrp161CmTBls374dOp1Ov9+WLVsMnvf8F05CQgIqVqyo356RkZHtl2ZenJyc9L98AgIC4OvrixYtWmDs2LH6Hjy//PILbt26hd9++w0tWrTQP7cg6ykqVKgAjUaD33//3WCdyXM5bctJbvP9+y9grVaLESNGYPLkyfj444+xZMkStG7dGjVq1DA67985OjpizJgx+O2333D69Gl06NABTk5OOHDgALKysnItcBwdHZGRkYG//vrLoMARESQkJKBhw4YG++e0tqdChQqoU6cOZs+eneNruLu7F2pObm5uqFChAv78889CPf+fzM3NMW7cOIwbNw7379/Hzz//jMmTJyMkJARxcXEoW7ZssbwOUWHxaimiEhQdHQ0A+l9+Go0G5ubmMDMz0+/z5MkT/O9//zN4XsuWLQEAa9asMdi+YcOGIl1F06xZM4SFheHHH3/ULwp+/kv3nwXIF198ke35z/f559GcTp06QURw8+ZNNGjQINvXSy+9ZFS+3Ob7/Ofx3KBBg2BhYYE33ngDFy5cwMiRI/P93unp6bkWhufOnQPwf8VE+/bt8fTpU3z99de5fr/WrVsDAFavXm2wfdOmTXj06JF+PC+dOnXC6dOnUaVKlRx/boUtbm7cuIE7d+7A2dm5UM/Pi729PV5//XWMGDEC9+7dY1NAKhV45IboBTl9+rS+8Lh79y6+//577NmzB926dYO3tzeAZ43U5s+fj969e2PIkCG4e/cuPvroo2yFha+vL958800sXLgQZcqUQZs2bXD69Gl89NFHsLW1LVLO9957D+vXr8fUqVPx888/o2nTpnBwcEB4eDimT5+OMmXKYM2aNTn+q/95kTJ37ly0b98eZmZmqFOnDgIDAzFkyBD0798fx44dQ/PmzWFtbY34+HgcOHAAL730EoYNG5Zvtu+//x7m5uZo27at/mqpl19+Gd27dzfYz97eHmFhYVi6dCm8vLyM6iOUnJyMypUr4z//+Q/atGkDDw8PPHz4EL/99hsWLVoEX19fvPrqqwCAXr16YeXKlQgPD8eFCxcQFBSErKwsHD58GL6+vujZsyfatm2LkJAQ/Pe//0VKSgoCAwP1V0vVq1cPffr0yTfTzJkzsWfPHjRt2hSjRo1CjRo18PTpU1y/fh07duzA559/jkqVKuX5PZ48eYJDhw4BeHbF3rVr1/Dhhx8CAMaMGZNvBmOEhobq+zg5OTkhJiYGCxcuhJeXl/70JpGiFF7zQ6Q6OV0tZWdnJ3Xr1pX58+fL06dPDfb/6quvpEaNGmJpaSk+Pj4yZ84c+fLLL7Mt9kxNTZXx48eLs7Oz6HQ6adKkiRw8eFC8vLyMXlA8YsSIHMcmTpwoAGTfvn0iIhIZGSkBAQFStmxZcXJykkGDBsnx48cFgKxcudIg06BBg8TJyUm/8Pjvmb/66itp3LixWFtbi5WVlVSpUkXCwsLyvDpL5P8WFEdFRUloaKjY2NhIuXLlpFevXpKYmJjjc3777TcBIB988EG+P4vn2T/66CNp3769eHp6iqWlpeh0OvH19ZWIiAi5e/euwf5PnjyRadOmSbVq1cTCwkIcHR2lVatWEhkZabDPf//7X/Hy8pIyZcqIm5ubDBs2TJKSkgy+l5eXl3Ts2DHHXH/99ZeMGjVKvL29pUyZMlK+fHnx9/eXKVOmyMOHD/Oc0z+vltJqteLu7i7t27eX3377zWDf3BYU165dO9v37du3r3h5eekff/zxx9K0aVOpUKGCWFhYiKenpwwcOFCuX7+eZz6ikqIR+UenMCIiEzR+/HgsXboUcXFxBVpkTUTqw9NSRGTSDh06hIsXL2LJkiUYOnQoCxsiAo/cEJFJ02g0KFu2LDp06ICVK1fCxsZG6UhEpDAeuSEik8Z/nxHRP/FScCIiIlIVFjdERESkKixuiIiISFX+dWtusrKycOvWLZQrVy7H9udERERU+ogIHjx4kO993oB/YXFz69YteHh4KB2DiIiICiEuLi7fTt3/uuKmXLlyAJ79cIratp6IiIhKRkpKCjw8PPS/x/Pyrytunp+KsrW1ZXFDRERkYoxZUsIFxURERKQqLG6IiIhIVVjcEBERkaqwuCEiIiJVYXFDREREqsLihoiIiFSFxQ0RERGpCosbIiIiUhUWN0RERKQqLG6IiIhIVVjcEBERkaqwuCEiIiJVYXFDREREqsLihoiIiFSFxQ0RERGpirnSAYiIiKj0CQ198a+xbduL+b48ckNERESqwuKGiIiIVIXFDREREakKixsiIiJSFRY3REREpCosboiIiEhVWNwQERGRqrC4ISIiIlVhcUNERESqwuKGiIiIVIXFDREREakKixsiIiJSFRY3REREpCosboiIiEhVWNwQERGRqrC4ISIiIlVhcUNERESqwuKGiIiIVIXFDREREakKixsiIiJSFRY3REREpCosboiIiEhVWNwQERGRqrC4ISIiIlVhcUNERESqwuKGiIiIVIXFDREREakKixsiIiJSFRY3REREpCosboiIiEhVWNwQERGRqrC4ISIiIlVhcUNERESqwuKGiIiIVIXFDREREakKixsiIiJSFRY3REREpCosboiIiEhVWNwQERGRqrC4ISIiIlVhcUNERESqwuKGiIiIVIXFDREREakKixsiIiJSFRY3REREpCosboiIiEhVWNwQERGRqrC4ISIiIlVhcUNERESqwuKGiIiIVEXx4mbJkiXw9vaGTqeDv78/fv/99zz3X7NmDV5++WWULVsWbm5u6N+/P+7evVtCaYmIiKi0U7S4Wb9+PcaMGYMpU6bgxIkTaNasGdq3b4/Y2Ngc9z9w4ADCwsIwcOBAnDlzBt999x2OHj2KQYMGlXByIiIiKq0ULW7mz5+PgQMHYtCgQfD19cXChQvh4eGBpUuX5rj/oUOHULlyZYwaNQre3t545ZVXMHToUBw7dqyEkxMREVFppVhxk5aWhqioKAQHBxtsDw4ORmRkZI7Padq0KW7cuIEdO3ZARJCYmIiNGzeiY8eOub5OamoqUlJSDL6IiIhIvRQrbu7cuYPMzEy4uLgYbHdxcUFCQkKOz2natCnWrFmDHj16wMLCAq6urrC3t8fixYtzfZ05c+bAzs5O/+Xh4VGs8yAiIqLSRfEFxRqNxuCxiGTb9tzZs2cxatQoTJs2DVFRUfjpp59w7do1hIeH5/r9J02ahOTkZP1XXFxcseYnIiKi0sVcqReuUKECzMzMsh2luX37drajOc/NmTMHgYGBmDhxIgCgTp06sLa2RrNmzTBr1iy4ublle46lpSUsLS2LfwJERERUKil25MbCwgL+/v7Ys2ePwfY9e/agadOmOT7n8ePH0GoNI5uZmQF4dsSHiIiISNHTUuPGjcOKFSvw1Vdf4dy5cxg7dixiY2P1p5kmTZqEsLAw/f6hoaH4/vvvsXTpUly9ehV//PEHRo0ahUaNGsHd3V2paRAREVEpothpKQDo0aMH7t69i5kzZyI+Ph5+fn7YsWMHvLy8AADx8fEGPW/69euHBw8e4NNPP8X48eNhb2+PVq1aYe7cuUpNgYiIiEoZjfzLzuekpKTAzs4OycnJsLW1VToOERFRqRQa+uJfY9s24/ctyO9vxa+WIiIiIipOLG6IiIhIVVjcEBERkaqwuCEiIiJVYXFDREREqsLihoiIiFSFxQ0RERGpiqJN/IiIiNToRfeIKUh/mH8jHrkhIiIiVWFxQ0RERKrC4oaIiIhUhcUNERERqQqLGyIiIlIVFjdERESkKixuiIiISFVY3BAREZGqsLghIiIiVWFxQ0RERKrC4oaIiIhUhcUNERERqQqLGyIiIlIVFjdERESkKixuiIiISFVY3BAREZGqsLghIiIiVWFxQ0RERKrC4oaIiIhUhcUNERERqQqLGyIiIlIVFjdERESkKixuiIiISFVY3BAREZGqsLghIiIiVWFxQ0RERKrC4oaIiIhUhcUNERERqQqLGyIiIlIVFjdERESkKixuiIiISFVY3BAREZGqsLghIiIiVWFxQ0RERKrC4oaIiIhUhcUNERERqQqLGyIiIlIVFjdERESkKixuiIiISFVY3BAREZGqsLghIiIiVWFxQ0RERKrC4oaIiIhUhcUNERERqQqLGyIiIlIVFjdERESkKixuiIiISFVY3BAREZGqsLghIiIiVWFxQ0RERKrC4oaIiIhUhcUNERERqQqLGyIiIlIVFjdERESkKixuiIiISFUUL26WLFkCb29v6HQ6+Pv74/fff89z/9TUVEyZMgVeXl6wtLRElSpV8NVXX5VQWiIiIirtzJV88fXr12PMmDFYsmQJAgMD8cUXX6B9+/Y4e/YsPD09c3xO9+7dkZiYiC+//BJVq1bF7du3kZGRUcLJiYiIqLRStLiZP38+Bg4ciEGDBgEAFi5ciF27dmHp0qWYM2dOtv1/+ukn7Nu3D1evXkX58uUBAJUrVy7JyERERFTKKXZaKi0tDVFRUQgODjbYHhwcjMjIyByfs3XrVjRo0AAffvghKlasiOrVq2PChAl48uRJrq+TmpqKlJQUgy8iIiJSL8WO3Ny5cweZmZlwcXEx2O7i4oKEhIQcn3P16lUcOHAAOp0Omzdvxp07dzB8+HDcu3cv13U3c+bMwbvvvlvs+YmIiKh0UnxBsUajMXgsItm2PZeVlQWNRoM1a9agUaNG6NChA+bPn4+vv/4616M3kyZNQnJysv4rLi6u2OdAREREpYdiR24qVKgAMzOzbEdpbt++ne1oznNubm6oWLEi7Ozs9Nt8fX0hIrhx4waqVauW7TmWlpawtLQs3vBERERUail25MbCwgL+/v7Ys2ePwfY9e/agadOmOT4nMDAQt27dwsOHD/XbLl68CK1Wi0qVKr3QvERERGQaFD0tNW7cOKxYsQJfffUVzp07h7FjxyI2Nhbh4eEAnp1SCgsL0+/fu3dvODo6on///jh79iz279+PiRMnYsCAAbCyslJqGkRERFSKKHopeI8ePXD37l3MnDkT8fHx8PPzw44dO+Dl5QUAiI+PR2xsrH5/Gxsb7NmzB2+99RYaNGgAR0dHdO/eHbNmzVJqCkRERFTKaERElA5RklJSUmBnZ4fk5GTY2toqHYeIiFQoNPTFfv9t217s9wde/ByAgs2jIL+/Fb9aioiIiKg4Faq4+d///ofAwEC4u7sjJiYGwLPuwj/88EOxhiMiIiIqqAIXN0uXLsW4cePQoUMH3L9/H5mZmQAAe3t7LFy4sLjzERERERVIgYubxYsXY/ny5ZgyZQrMzMz02xs0aIBTp04VazgiIiKigipwcXPt2jXUq1cv23ZLS0s8evSoWEIRERERFVaBixtvb29ER0dn275z507UqlWrODIRERERFVqB+9xMnDgRI0aMwNOnTyEiOHLkCL799lvMmTMHK1aseBEZiYiIiIxW4OKmf//+yMjIQEREBB4/fozevXujYsWKWLRoEXr27PkiMhIREREZrVAdigcPHozBgwfjzp07yMrKgrOzc3HnIiIiIiqUAq+5adWqFe7fvw/g2Z29nxc2KSkpaNWqVbGGIyIiIiqoAhc3v/32G9LS0rJtf/r0KX7//fdiCUVERERUWEafljp58qT+/8+ePYuEhAT948zMTPz000+oWLFi8aYjIiIiKiCji5u6detCo9FAo9HkePrJysoKixcvLtZwRERERAVldHFz7do1iAh8fHxw5MgRODk56ccsLCzg7Oxs0LGYiIiISAlGFzdeXl4AgKysrBcWhoiIiKioCnUpOPBs3U1sbGy2xcWdO3cucigiIiKiwipwcXP16lV069YNp06dgkajgYgAADQaDQDo7xJOREREpIQCXwo+evRoeHt7IzExEWXLlsWZM2ewf/9+NGjQAL/99tsLiEhERERkvAIfuTl48CB++eUXODk5QavVQqvV4pVXXsGcOXMwatQonDhx4kXkJCIiIjJKgY/cZGZmwsbGBsCzDsW3bt0C8GzB8YULF4o3HREREVEBFfjIjZ+fH06ePAkfHx80btwYH374ISwsLLBs2TL4+Pi8iIxERERERitwcfPOO+/g0aNHAIBZs2ahU6dOaNasGRwdHbFu3bpiD0hERERUEAUubkJCQvT/7+Pjg7Nnz+LevXtwcHDQXzFFREREpJQCr7nJSfny5ZGQkICRI0cWx7cjIiIiKrQCHbk5e/Ysfv31V5QpUwbdu3eHvb097ty5g9mzZ+Pzzz+Ht7f3i8pJREREZBSji5vt27fjtddeQ3p6OgDgww8/xPLly9G9e3f4+fnhu+++Q6dOnV5YUCIiUr/Q0Bf/Gtu2vfjXIGUZfVpq9uzZCA8PR0pKCj766CNcvXoV4eHh2LRpE3799VcWNkRERFQqGF3cnDt3DiNGjICNjQ1GjRoFrVaLhQsXonnz5i8yHxEREVGBGF3cpKSkwN7eHgBgbm4OKysrVK9e/UXlIiIiIiqUAi8oTkhIAACICC5cuKDvefNcnTp1ii8dERERUQEVqLhp3bq1/i7gAPTrbJ7fHVyj0fCu4ERERKQoo4uba9euvcgcRERERMXC6OLGy8vrReYgIiIiKhbF0qGYiIiIqLRgcUNERESqwuKGiIiIVIXFDREREakKixsiIiJSFaOulqpXrx40Go1R3/D48eNFCkRERERUFEYVN127dtX//9OnT7FkyRLUqlULAQEBAIBDhw7hzJkzGD58+AsJSURERGQso4qb6dOn6/9/0KBBGDVqFN57771s+8TFxRVvOiIiIqICKvCam++++w5hYWHZtr/55pvYtGlTsYQiIiIiKqwCFzdWVlY4cOBAtu0HDhyATqcrllBEREREhVWgG2cCwJgxYzBs2DBERUWhSZMmAJ6tufnqq68wbdq0Yg9IREREVBAFLm7efvtt+Pj4YNGiRVi7di0AwNfXF19//TW6d+9e7AGJiIiICqLAxQ0AdO/enYUMERERlUps4kdERESqUuAjN5mZmViwYAE2bNiA2NhYpKWlGYzfu3ev2MIRERERFVSBj9y8++67mD9/Prp3747k5GSMGzcOr776KrRaLWbMmPECIhIREREZr8DFzZo1a7B8+XJMmDAB5ubm6NWrF1asWIFp06bh0KFDLyIjERERkdEKXNwkJCTgpZdeAgDY2NggOTkZANCpUyf8+OOPxZuOiIiIqIAKXNxUqlQJ8fHxAICqVati9+7dAICjR4/C0tKyeNMRERERFVCBi5tu3bph7969AIDRo0dj6tSpqFatGsLCwjBgwIBiD0hERERUEAW+WuqDDz7Q///rr7+OSpUqITIyElWrVkXnzp2LNRwRERFRQRWqid/fNWnSRH8bBiIiIiKlFbi4uXv3LhwdHQEAcXFxWL58OZ48eYLOnTujWbNmxR6QiIiIqCCMXnNz6tQpVK5cGc7OzqhZsyaio6PRsGFDLFiwAMuWLUNQUBC2bNnyAqMSERER5c/o4iYiIgIvvfQS9u3bh5YtW6JTp07o0KEDkpOTkZSUhKFDhxqsxyEiIiJSgtGnpY4ePYpffvkFderUQd26dbFs2TIMHz4cWu2z+uitt97i2hsiIiJSnNFHbu7duwdXV1cAz5r3WVtbo3z58vpxBwcHPHjwoPgTEhERERVAgfrcaDSaPB8TERERKa1AV0v169dP34X46dOnCA8Ph7W1NQAgNTW1+NMRERERFZDRxU3fvn0NHr/55pvZ9gkLCyt6IiIiIqIiMLq4Wbly5QsJsGTJEsybNw/x8fGoXbs2Fi5caFS/nD/++AMtWrSAn58foqOjX0g2IiIiMj0FvrdUcVq/fj3GjBmDKVOm4MSJE2jWrBnat2+P2NjYPJ+XnJyMsLAwtG7duoSSEhERkalQtLiZP38+Bg4ciEGDBsHX1xcLFy6Eh4cHli5dmufzhg4dit69eyMgIKCEkhIREZGpUKy4SUtLQ1RUFIKDgw22BwcHIzIyMtfnrVy5EleuXMH06dONep3U1FSkpKQYfBEREZF6KVbc3LlzB5mZmXBxcTHY7uLigoSEhByfc+nSJbz99ttYs2YNzM2NWy40Z84c2NnZ6b88PDyKnJ2IiIhKL0VPSwHZe+WISI79czIzM9G7d2+8++67qF69utHff9KkSUhOTtZ/xcXFFTkzERERlV4Fvit4calQoQLMzMyyHaW5fft2tqM5APDgwQMcO3YMJ06cwMiRIwEAWVlZEBGYm5tj9+7daNWqVbbnWVpa6nvzEBERkfopduTGwsIC/v7+2LNnj8H2PXv2oGnTptn2t7W1xalTpxAdHa3/Cg8PR40aNRAdHY3GjRuXVHQiIiIqxRQ7cgMA48aNQ58+fdCgQQMEBARg2bJliI2NRXh4OIBnp5Ru3ryJVatWQavVws/Pz+D5zs7O0Ol02bYTERHRv5eixU2PHj1w9+5dzJw5E/Hx8fDz88OOHTvg5eUFAIiPj8+35w0RERHR32lERJQOUZJSUlJgZ2eH5ORk2NraKh2HiIj+JjT0xb/Gtm0v/jVe9DzUMAegYPMoyO9vxa+WIiIiIipOLG6IiIhIVVjcEBERkaqwuCEiIiJVYXFDREREqsLihoiIiFSFxQ0RERGpCosbIiIiUhUWN0RERKQqLG6IiIhIVVjcEBERkaqwuCEiIiJVYXFDREREqsLihoiIiFSFxQ0RERGpCosbIiIiUhUWN0RERKQqLG6IiIhIVVjcEBERkaqwuCEiIiJVYXFDREREqsLihoiIiFSFxQ0RERGpCosbIiIiUhUWN0RERKQqLG6IiIhIVVjcEBERkaqwuCEiIiJVYXFDREREqsLihoiIiFSFxQ0RERGpCosbIiIiUhUWN0RERKQqLG6IiIhIVVjcEBERkaqwuCEiIiJVYXFDREREqsLihoiIiFSFxQ0RERGpCosbIiIiUhUWN0RERKQqLG6IiIhIVVjcEBERkaqwuCEiIiJVYXFDREREqsLihoiIiFSFxQ0RERGpCosbIiIiUhUWN0RERKQqLG6IiIhIVVjcEBERkaqwuCEiIiJVYXFDREREqmKudAAiIiq60NAX/xrbtr341yAqDjxyQ0RERKrC4oaIiIhUhcUNERERqQqLGyIiIlIVFjdERESkKixuiIiISFVY3BAREZGqsLghIiIiVWFxQ0RERKqieHGzZMkSeHt7Q6fTwd/fH7///nuu+37//fdo27YtnJycYGtri4CAAOzatasE0xIREVFpp2hxs379eowZMwZTpkzBiRMn0KxZM7Rv3x6xsbE57r9//360bdsWO3bsQFRUFIKCghAaGooTJ06UcHIiIiIqrRQtbubPn4+BAwdi0KBB8PX1xcKFC+Hh4YGlS5fmuP/ChQsRERGBhg0bolq1anj//fdRrVo1bOMNT4iIiOj/U6y4SUtLQ1RUFIKDgw22BwcHIzIy0qjvkZWVhQcPHqB8+fIvIiIRERGZIMXuCn7nzh1kZmbCxcXFYLuLiwsSEhKM+h4ff/wxHj16hO7du+e6T2pqKlJTU/WPU1JSCheYiIiITILiC4o1Go3BYxHJti0n3377LWbMmIH169fD2dk51/3mzJkDOzs7/ZeHh0eRMxMREVHppVhxU6FCBZiZmWU7SnP79u1sR3P+af369Rg4cCA2bNiANm3a5LnvpEmTkJycrP+Ki4srcnYiIiIqvRQrbiwsLODv7489e/YYbN+zZw+aNm2a6/O+/fZb9OvXD2vXrkXHjh3zfR1LS0vY2toafBEREZF6KbbmBgDGjRuHPn36oEGDBggICMCyZcsQGxuL8PBwAM+Outy8eROrVq0C8KywCQsLw6JFi9CkSRP9UR8rKyvY2dkpNg8iIiIqPRQtbnr06IG7d+9i5syZiI+Ph5+fH3bs2AEvLy8AQHx8vEHPmy+++AIZGRkYMWIERowYod/et29ffP311yUdn4iIiEohRYsbABg+fDiGDx+e49g/C5bffvvtxQciIiIik6b41VJERERExYnFDREREakKixsiIiJSFRY3REREpCosboiIiEhVWNwQERGRqrC4ISIiIlVhcUNERESqwuKGiIiIVIXFDREREakKixsiIiJSFRY3REREpCosboiIiEhVWNwQERGRqrC4ISIiIlVhcUNERESqwuKGiIiIVIXFDREREakKixsiIiJSFRY3REREpCosboiIiEhVWNwQERGRqrC4ISIiIlVhcUNERESqwuKGiIiIVIXFDREREakKixsiIiJSFRY3REREpCosboiIiEhVWNwQERGRqrC4ISIiIlVhcUNERESqwuKGiIiIVIXFDREREakKixsiIiJSFRY3REREpCosboiIiEhVWNwQERGRqrC4ISIiIlUxVzoAEZHSQkNf/Gts2/biX4OInuGRGyIiIlIVFjdERESkKixuiIiISFVY3BAREZGqsLghIiIiVWFxQ0RERKrC4oaIiIhUhcUNERERqQqLGyIiIlIVFjdERESkKixuiIiISFVY3BAREZGqsLghIiIiVWFxQ0RERKrC4oaIiIhUhcUNERERqQqLGyIiIlIVFjdERESkKixuiIiISFVY3BAREZGqmCsdgIhMV2joi3+Nbdte/GsQkbrwyA0RERGpCosbIiIiUhXFi5slS5bA29sbOp0O/v7++P333/Pcf9++ffD394dOp4OPjw8+//zzEkpKREREpkDR4mb9+vUYM2YMpkyZghMnTqBZs2Zo3749YmNjc9z/2rVr6NChA5o1a4YTJ05g8uTJGDVqFDZt2lTCyYmIiKi0UnRB8fz58zFw4EAMGjQIALBw4ULs2rULS5cuxZw5c7Lt//nnn8PT0xMLFy4EAPj6+uLYsWP46KOP8Nprr5VkdKIie9GLcbkQl4j+rRQ7cpOWloaoqCgEBwcbbA8ODkZkZGSOzzl48GC2/UNCQnDs2DGkp6e/sKxERERkOhQ7cnPnzh1kZmbCxcXFYLuLiwsSEhJyfE5CQkKO+2dkZODOnTtwc3PL9pzU1FSkpqbqHycnJwMAUlJSijoFk9O9+4t/jQ0bXvxrqGUeL7oeL4m3eEn8m4LzMI4a5gBwHsZSwxyAgs3j+e9tEcl3X8X73Gg0GoPHIpJtW37757T9uTlz5uDdd9/Ntt3Dw6OgUckIdnZKJygeapiHGuYAcB6liRrmAKhjHmqYA1C4eTx48AB2+TxRseKmQoUKMDMzy3aU5vbt29mOzjzn6uqa4/7m5uZwdHTM8TmTJk3CuHHj9I+zsrJw7949ODo65llEFUVKSgo8PDwQFxcHW1vbF/IaJUEN81DDHADOozRRwxwAdcxDDXMAOA9jiQgePHgAd3f3fPdVrLixsLCAv78/9uzZg27duum379mzB126dMnxOQEBAdj2j1WSu3fvRoMGDVCmTJkcn2NpaQlLS0uDbfb29kULbyRbW1uTfqM+p4Z5qGEOAOdRmqhhDoA65qGGOQCchzHyO2LznKKXgo8bNw4rVqzAV199hXPnzmHs2LGIjY1FeHg4gGdHXcLCwvT7h4eHIyYmBuPGjcO5c+fw1Vdf4csvv8SECROUmgIRERGVMoquuenRowfu3r2LmTNnIj4+Hn5+ftixYwe8vLwAAPHx8QY9b7y9vbFjxw6MHTsWn332Gdzd3fHJJ5/wMnAiIiLSU3xB8fDhwzF8+PAcx77++uts21q0aIHjx4+/4FRFY2lpienTp2c7HWZq1DAPNcwB4DxKEzXMAVDHPNQwB4DzeBE0Ysw1VUREREQmQvF7SxEREREVJxY3REREpCosboiIiEhVWNwQERGRqih+tZSp2rp1a4Gf07ZtW1hZWb2ANKQGhbnfWWls+KWGeZw8ebLAz6lVqxbMzUvXR6paPqfUMA81zAGAQcd/Y73zzjsoX778C0iTO14tVUhabcEOemk0Gly6dAk+Pj4vKFHh1K9fv0D7azQabN26FRUrVnxBiQqnoH9xNBoNjh8/ru+pVBpotdoC3RJEo9Hg4sWLpe49pYZ5PJ+DsR+PWq221M0BUM/nlBrmoYY5AM/mERAQAAsLC6P2P3DgAC5cuFDi8yhd/8wwMQkJCXB2djZq33Llyr3gNIUTHR2N8ePHw8bGJt99RQQffPCBwV3WS4v79+9j4cKFRrXmFhEMHz4cmZmZJZCsYDZu3GhUoSYi6NChQwkkKhw1zOPw4cNwcnLKdz8RgZ+fXwkkKhw1fE4B6piHGuYAAJs3by7182BxU0h9+/Yt0OHCN998s9Qden9u4sSJRr9RP/744xecpvB69uxp9DzeeuutF5ym4Ly8vNC8efNcbwL7Tz4+PrneU01JaphHixYtULVqVaPvQ9e8efNSd/oAUM/nlBrmoYY5AMDKlSuNvr8TAHzxxRe53gz7ReJpqX+5mJgYeHp6Gn0aIS4uDu7u7jAzM3vByYiIiAqHxc0LkJGRUeoWFhIR/ZOIFGh9FNE/xcTEICEhARqNBi4uLqVmHSMvBS+Cn376CadOnQIAZGVlYdasWahYsSIsLS1RqVIlfPDBB0YvSCxN7t+/j+XLl2Pq1KlYsWIFkpOTlY5ktD///BMrV67EtWvXAABnzpzB8OHDER4ejl27dimcruDS09OxZcsWzJs3D6tXr8ajR4+UjlQgN27cwMOHD7NtT09Px/79+xVIVHzi4uIwYMAApWPkKzU1FePHj0eLFi0wb948AMCsWbNgY2MDGxsb9O7du1BXuCkhPj4eq1evxo4dO5CWlmYw9ujRI8ycOVOhZMb7888/ERYWBh8fH1hZWcHGxgYvvfQSpk6dajJ/DgCwYMECeHh4wMfHBwEBAWjSpAl8fHzg4eGBhQsXKh0PECq0WrVqyR9//CEiIu+//744OjrK/PnzZefOnbJw4UJxcXGRDz74QOGU+Xvttddk06ZNIiJy5swZqVChgjg5OUnjxo3FxcVFXF1d5ezZswqnzN/GjRvFzMxMHB0dpVy5cvLzzz+Lvb29tGnTRkJCQsTMzEzWrFmjdMw8BQQESFJSkoiI3L59W1566SWxsLCQatWqiU6nE09PT7lx44ayIY1w69YtadiwoWi1WjEzM5OwsDB58OCBfjwhIUG0Wq2CCYsuOjraJOYwduxYcXd3l/Hjx4uvr6+MGDFCPD09ZfXq1bJ27VqpWrWqvPXWW0rHzNeRI0fE3t5ebG1txcrKSqpVqyanT5/Wj5vCe+qnn34SKysr6dq1q/Tq1UvKli0rI0eOlP/+979StWpVqVKlisTHxysdM18zZ84UW1tb+eCDD+TEiRNy69YtuXnzppw4cUI++OADsbOzk/fee0/RjCxuikCn00lsbKyIiPj5+cn69esNxrdv3y5Vq1ZVIlqBVKhQQS5evCgiIu3bt5fevXtLamqqiIikpaXJwIEDJTg4WMmIRqlfv77MmjVLRES+/fZbsbe3l5kzZ+rHP/roI6lbt65S8Yyi0WgkMTFRREQGDx4sdevW1X/Y3blzR5o2bSoDBgxQMqJRwsLCpEmTJnL06FHZs2ePNGjQQPz9/eXevXsi8uwXkUajUThl3n744Yc8vxYsWFDqf5mKiHh4eMiePXtEROTKlSui1Wply5Yt+vHdu3eLl5eXQumM16ZNGxkwYIBkZmZKSkqKDB8+XBwdHeX48eMiYhrFTd26dWXp0qX6x7t375aaNWuKyLPP2tatW0u/fv2Uime0SpUqyebNm3Md//7778Xd3b3kAuWAxU0RuLm5ycGDB0VExMXFRf+X7LmLFy+KlZWVEtEKxMrKSi5fviwiz+b0z3lcuHBB7OzsFEhWMNbW1nLt2jUREcnKypIyZcrIyZMn9eNXrlwRGxsbhdIZ5+/FTfXq1WX79u0G47/++qtUrlxZiWgF4u7uLocPH9Y/fvr0qXTp0kXq1q0rd+/eNYlfRBqNRrRarWg0mly/SvscRJ79/Y6JidE/LlOmjMERj2vXrknZsmWViFYgDg4OcuHCBYNtc+fOFQcHBzly5IhJvKd0Op3+M0rk/z6nbt26JSIi+/fvFycnJ4XSGc/KyirPo/mnT59W/Hcf19wUQbdu3TB79mxkZmaiS5cuWLJkicEam08//RR169ZVLqCR6tSpg19++QUA4OrqipiYGIPxmJiYUnmZ6z+VK1cOd+/eBfBs3VBGRob+MQDcvXvXqH4+Snu+wPP+/fvw9vY2GPP29kZ8fLwSsQokOTkZDg4O+seWlpbYuHEjKleujKCgINy+fVvBdMZxc3PDpk2bkJWVlePX8ePHlY5oFE9PTxw8eBAAcPToUWg0Ghw5ckQ/fvjw4VLXlDM3T58+NXgcERGByZMnIzg4GJGRkQqlMl7FihVx4cIF/eMrV64gKytL3zahUqVKOa5RK20aNWqE2bNnIyMjI9tYRkYG3n//fTRq1EiBZP+Hl/QUwfvvv482bdqgZs2aCAgIwHfffYc9e/agevXquHz5Mu7evYvdu3crHTNfU6dORVhYGMqUKYNRo0Zh7NixuHv3Lnx9fXHhwgVMnz4dffr0UTpmvtq0aYMRI0bgrbfewvr16xESEoJJkyZh5cqV0Gg0mDhxIl555RWlY+arX79+sLS0RHp6OmJiYlCrVi39WHx8vNG9V5Tk4+ODkydPolq1avpt5ubm+O677/Cf//wHnTp1UjCdcfz9/XH8+HF07do1x/GCdDBWUnh4OPr164cVK1YgKioKH3/8MSZPnozz589Dq9Vi6dKlGD9+vNIx8+Xn54fIyEjUqVPHYPuECRMgIujVq5dCyYwXFhaGQYMGYcqUKbC0tMT8+fPRuXNnfbff6OjobP+gKY0WL16M4OBgODs7o0WLFnBxcYFGo0FCQgL2798PS0tL7NmzR9mQih43UoG0tDRZunSpdOjQQWrWrCnVq1eXFi1ayOTJkyUuLk7peEbbuHGjVKpUKdtheJ1OJ2PGjJGMjAylI+YrISFB2rRpIzY2NtK+fXtJTk6WkSNH6k8fVKtWTX/6rbTq16+fwdeGDRsMxidMmCAhISEKpTNeREREruu00tPTpXPnzqX+FML+/ftl586duY4/fPhQfvvttxJMVHirV6+WkSNHyrp160Tk2enNZs2aib+/v8yYMUMyMzMVTpi/5cuXy5tvvpnr+Ny5c0v9Kdv09HSJiIgQd3d3cXR0lN69e8tff/2lHz98+LDs27dPwYTGS0lJkSVLlkhYWJgEBwdLcHCwhIWFydKlSyU5OVnpeMI+N6SXmZmJ48eP4+rVq8jKyoKbmxv8/f1LdRtwY1y9ehWPHz9GzZo1Tb7/0KNHj2BmZgadTqd0lDxlZGTg8ePHuXZYzczMxI0bN0pNTwwiUhcWN0RE/xKZmZm4c+cOzMzMUKFCBaXj/Ks9/7PQaDRwdHRk1/dixgXFRbRixQr07dsXK1euBACsX78evr6+8PHxwfTp0xVOZ5xNmzbh8ePHSscoFo8ePcLy5cvRv39/tG/fHh06dED//v2xYsUKk2mAd+7cOaxcuRLnz58HAJw/fx7Dhg3DgAED9Au/TUF8fDymTZuGVq1awdfXF35+fggNDcWXX35ZKm9a+k+hoaH43//+hydPnigdpch+/PFHNG/eHNbW1nB3d4eLiwvs7e3Rp08fxMbGKh3PaGr4vN28eTMCAwNRtmxZuLu7w83NDWXLlkVgYCC2bNmidLxi0aZNG+XvZq7sWTHTtmDBArG2tpZXX31V3NzcZNasWeLo6CizZs2SmTNnip2dnXzxxRdKx8yXRqORcuXKyeDBg+XQoUNKxym0M2fOiLu7u9jb20uXLl1kyJAhMnjwYOnSpYvY29tLxYoV5cyZM0rHzNPOnTvFwsJCypcvLzqdTnbu3ClOTk7Spk0bad26tZibm8vevXuVjpmvo0ePip2dndStW1cCAgJEq9VKnz59pEePHmJvby8BAQGSkpKidMw8aTQaMTc3Fzs7OwkPD5djx44pHalQVq1aJeXKlZMxY8bI22+/LS4uLvL222/L0qVLpUWLFgZ9rkozNXzefv7552JhYSHh4eGyefNmiYyMlD/++EM2b94s4eHhYmlpKcuWLVM6ZpF9+umnMmPGDEUzsLgpgpo1a+o73h4/flzMzc1lxYoV+vGvvvpK/P39lYpnNI1GIzNnzpR69eqJRqOR2rVry4IFC+TOnTtKRyuQli1bSs+ePfUNCP8uNTVVevXqJS1btlQgmfECAgJkypQpIvKsEaGDg4NMnjxZPz558mRp27atUvGMFhgYaPDh9r///U8aN24sIiL37t2TunXryqhRo5SKZxSNRiNnzpyRBQsWyEsvvSRarVbq1Kkjixcv1jcjNAU1a9bULyQWeVZ4VqpUSbKyskREpEePHtKtWzel4hlNDZ+3VapUMcj8T19++aX4+PiUYCL1YnFTBP9sjmVpaWnQHOvSpUtib2+vRLQC+XvjuGPHjsmwYcPE3t5eLC0t5T//+Y/s3r1b4YTGsbKyyvPIzKlTpxRvLJUfW1tbuXTpkoiIZGZmirm5uURFRenHT506JS4uLkrFM5qVlZVcuXJF/zgzM1PKlCkjCQkJIvKsM6vSHUzz8/e/FyLPrmQZMmSI2NnZiZWVlfTq1cskjqJZWVkZNI4TETE3N5ebN2+KyLN5mcLnlBo+b3U6nZw/fz7X8XPnzolOpyvBROrFNTdFULZsWYN1HE5OTtmaxOXU5Kg08/f3x5IlSxAfH4/ly5fjr7/+Qrt27VC5cmWlo+XLwcEBly5dynX88uXLBo3lSjutVgudTmfQ16ZcuXImcSNTZ2dng2aDiYmJyMjI0F89Va1aNdy7d0+peIXSqFEjfPHFF4iPj8eSJUsQFxeHtm3bKh0rX5UrV8axY8f0j48fPw6tVgsXFxcAQPny5ZGenq5UPKOp4fO2du3aWLZsWa7jy5cvR+3atUsw0Ytx5coVtGrVStEMpn1drMJq1qyJkydPwtfXF8CzuwT/3fnz502iKHjeEffvdDod+vTpgz59+uDy5cv6BXyl2eDBg9G3b1+88847aNu2rUFjqT179uD999/HmDFjlI6Zp8qVK+Py5cuoWrUqAODgwYPw9PTUj8fFxcHNzU2peEbr2rUrwsPDMW/ePFhaWuK9995DixYt9J2uL1y4YDJdcf/JysoK/fr1Q79+/fIspkuLESNGYNCgQTh69Ch0Oh1WrFiBPn366K/OOXz4MKpXr65wyvyp4fP2448/RseOHfHTTz8hODg422dUTEwMduzYoXTMInv48CH27dunaAYWN0Uwd+5cWFtb5zoeGxuLoUOHlmCiwpF8ugFUrVoVs2fPLqE0hTdjxgxYWVlh/vz5iIiI0BdtIgJXV1e8/fbbiIiIUDhl3oYNG2ZwJZGfn5/B+M6dOxX/F5ExZs2ahfj4eISGhiIzMxMBAQFYvXq1flyj0WDOnDkKJsxfixYt9J1jc/P3Dsyl1YgRI6DVarF69WqkpqaiX79+mDp1qn68UaNGWLt2rYIJjaOGz9sWLVrg9OnTWLp0KQ4dOoSEhAQAz25706lTJ4SHh5f6Ag0APvnkkzzHb968WUJJcsc+N4SYmBh4enrmeATHVF27ds3gg8MUWpqr0dOnT5GRkWES9/QiIuNotVq4ubnlWvynpaUhISFB0ZYPLG5eAHm2UBtaLZc0EVHppIbPqUuXLiE2NhZeXl76U7mmIDMz06Bp35EjR5CVlYV69erB0tJSwWTG8fb2xty5c9G9e/ccx6Ojo+Hv769ocWO67+pSICMjA++88w5atGihbyA1b9482NjYwMrKCn379kVaWprCKQsuPT0dW7Zswbx587B69WqTaX6Xn7i4OAwYMEDpGPmKj4/H6tWrsWPHjmzvn0ePHmHmzJkKJTPeP3NfuXIFY8aMQceOHTFo0CBERUUplKz4nDt3TvlGZUZQy+fUBx98oG9imZSUhDZt2qBGjRpo27YtatSogfbt2+P+/fvKhszH9evX4e/vD0tLS3Ts2BEpKSlo27YtmjRpgqZNm6JWrVq4ePGi0jHz5e/vn+ff4VJxU1mFrtJShXfeeUdcXFxk3LhxUqtWLQkPDxcPDw9ZvXq1rFq1SipVqiRz585VOma+AgICJCkpSUREbt++LS+99JJYWFhItWrVRKfTiaenp9y4cUPZkMUgOjq61N+s8ciRI2Jvby+2trZiZWUl1apVM7jcNSEhodTPQUREq9XqL6M+ceKElC1bVurWrSuDBw+Whg0bioWFhRw+fFjhlEVjCu8nEfV8Tnl6esqff/4pIiKDBg2SevXqyfHjx+XJkycSHR0tTZo0kYEDByqcMm+vvfaatGjRQrZt2ybdu3eXwMBAadmypdy4cUNu3bolISEh0rVrV6Vj5uvMmTNy9OjRXMfT0tLk+vXrJZgoO56WKoIqVapg0aJF6NSpEy5fvowaNWpg7dq16NGjBwDgu+++w8yZM3Hq1CmFk+ZNq9UiISEBzs7OGDJkCI4ePYqdO3fC1dUVd+/eRefOnVGzZk18+eWXSkfN09atW/Mcv3r1KsaPH1+qW/+3bdsWnp6eWL58OR49eoS3334b69evx549e1CvXj0kJibC3d29VM8BMHxPhYaGQqfTYcOGDfp1XQMGDEB8fDx27typcNLcjRs3Ls/xv/76C2vXri31fxZq+ZzS6XS4cOECvLy84O3tjW+++QbNmzfXj0dFRSE0NBS3bt1SMGXenJ2dsXv3btStWxfJyclwcHDA/v378corrwB4dpl+hw4d9OsFqfB4tVQR3Lp1Cy+//DKAZ1cUWVhY6B8DQIMGDRATE6NUvELZt28f5s+fD1dXVwCAo6MjZs+ejf79+yucLH9du3bN93BoaV80HRUVhc8++wxarRblypXDZ599Bi8vL7Ru3Rq7du0yuCzcVERHR2PdunUGP/vRo0cjJCREwVT5W7RoEerWrZvrnc0fPnxYwokKRy2fU15eXjh9+jS8vLyg0Whgbm7468vMzKzUn0J/+vQp7OzsADzrWWVmZoZy5crpx21tbVVznz+lcc1NEdjZ2Rmc461fv77BGzU1NbXU/zJ97nnO+/fvZ7uyyNvb26AhW2nl5uaGTZs2ISsrK8ev48ePKx3RKE+fPjV4HBERgcmTJyM4OBiRkZEKpSoYjUajf0+ZmZllKxBsbW1LfTPCatWqYezYsfj1119z/Fq+fLnSEY2ils+pwYMHY+LEibh8+TJGjhyJCRMm4MqVKwCeXR05duxYBAcHK5wyb7Vr18ZXX30FAPjmm2/g6OiIdevW6ce//fZbk+g5ZAp45KYIatWqhePHj+Oll14CAPzxxx8G46dOnTKJPhgA0K9fP1haWiI9PR0xMTGoVauWfiw+Pt6gS25p5e/vj+PHj6Nr1645jpeKRW758PPzQ2RkJOrUqWOwfcKECRAR9OrVS6FkBSMiqF69OjQaDR4+fIhTp07p/54Az65yeX50sLR6vmjyzTffzHHcFN5PgHo+pyZMmIDY2FjUqlULVapUwfXr11G9enWYm5sjIyMD9evXx7fffqt0zDzNmDEDXbt2xYcffggzMzPs2rULgwYNwt69e2FmZoajR4+aRM8hU8Dipgg+//xzlClTJtfx9PT0Ut80DgD69u2r//8uXbpkO9y+adMm1K1bt4RTFdzEiRPzPCxdtWpV/PrrryWYqODCwsKwb98+hIeHZxubOHEiRARLly5VIFnB/LOjdZUqVQweHzp0CN26dSvJSAX28ccfIzU1Ndfxl19+GVlZWSWYqHDU8jkFPGseN2zYMGzfvh1Xr15FVlYW3NzcEBgYiDZt2pT6I1AhISE4e/Ysjh8/jgYNGsDLywv79+/HZ599hsePH+P9999HUFCQ0jFVgQuKKV+PHj2CmZkZdDqd0lGIiIjyxeKmGDx8+BBRUVFISEiARqOBi4sL/P392ZWViiQzMxN37tyBRqOBo6OjQdMvU6KWeRCR8VatWoXAwMBsR21LChcUF0F6ejpGjx4NZ2dnBAUFoW/fvujTpw+CgoLg7OyMMWPGmMTddgHgxo0bmDJlCoKCguDr64tatWohKCgIU6ZMyXaDutLs6NGjeOONN+Dt7Q0rKyuULVsW3t7eeOONNwzujFyabd68GYGBgShbtizc3d3h5uaGsmXLIjAwEFu2bFE6ntHUMo/c9O3b1yTu85WfNm3amEQzwvyoYR5qmMNz/fr1Q61atfDWW28p8vpcc1MEEyZMwKZNm7By5UqEhIToF93ev38fu3btwsSJEwEACxcuVC6kEQ4cOID27dvDw8MDwcHBCA4Ohojg9u3b2LJlCxYvXoydO3ciMDBQ6ah52rJlC7p3747WrVtj9OjRcHFx0c9j9+7dCAwMxIYNG9ClSxelo+bqiy++wKhRozBgwABMnDjRYA67du1Cz549sXjxYgwePFjpqHlSyzzyUrFiRZO+dcFz3bp1w507d5SOUWRqmIca5vBcVlYWrl+/jl27dikToOT7BqpHhQoVZO/evbmO//zzz1KhQoUSTFQ4DRo0kDFjxuQ6PmbMGGnQoEEJJiqc2rVry5w5c3Id/+CDD6RWrVolmKjgqlSpIitWrMh1/MsvvxQfH58STFQ4apkHEZkmrrkpAhsbmxwv230uOjoar7zySqlv9mVlZYXo6GjUqFEjx/Hz58+jXr16ePLkSQknKxidToeTJ0/m2ifiwoULePnll7P1kSlN1PJnoZZ5EFHuHj16hKioKMTHx8PMzAze3t6oX79+qbhqzfSPqSooKCgI48aNQ2JiYraxxMREREREmMQ5eTc3tzybwx08eBBubm4lmKhwqlSpkudajh9++KHUn8+uXbs2li1bluv48uXLUbt27RJMVDhqmQfXopUuapiHGuaQlZWFiIgI/XrT3r17o3v37mjYsCG8vb2xbds2pSPyaqmiiIuLQ4cOHXD+/Hn4+fnBxcUFGo0GCQkJOH36NGrVqoUff/wRlSpVUjpqnpYsWYKxY8di8ODBaNu2rcE89uzZgxUrVmDhwoU59l4pTTZt2oSePXvq1w39cx67d+/GunXr8OqrryodNVf79u1Dx44d4eXlleMcYmJisGPHDjRr1kzpqHlSwzz+uRbt7+uG9uzZg7i4OJNbixYSEpJtLdrevXtL/Vo0QB3zUMMcAODtt9/G1q1bMWfOHOh0OsyePRudOnVC586dsXbtWnz44YfYunWrsh2jlTsjpg6ZmZmyY8cOmTZtmgwZMkSGDBki06ZNk507d0pmZqbS8Yy2bt06ady4sZibm4tGoxGNRiPm5ubSuHFjWb9+vdLxjBYZGSk9evQQT09PsbCwEAsLC/H09JQePXpIZGSk0vGMcu3aNYmIiJDmzZtL9erVpXr16tK8eXP573//K9euXVM6ntFMfR5ci1a6qGEeapiDiIi7u7vs379f//jGjRtiY2MjT58+FRGRmTNnSkBAgFLxRIRrbugf0tPT9av1K1SokGdnUyI1U8u6ITWsRQPUMQ81zAF4dm+46Oho/Wn+rKwsWFpaIi4uDq6urjh79iwaNmyo6I1MueamGFy6dAnffPMN5s6diw8//BDffPMNLl26pHSsQilTpgzc3Nzg5ubGwob+1bgWrXRRwzzUMAcAeOmllwzu47VhwwbY2Njo7xf3vNhREvvcFEFycjLCwsKwbds22NnZwdnZGSKCv/76CykpKQgNDcWqVauy3RG5NDp69CgWLlyIyMhIg07LTZs2xdixY9GgQQOlIxbZ5MmTkZCQoL8rrynq27cv4uLi8MsvvygdpUhMYR4TJkxAeHg4oqKi8lyLVtrNnDkTPXv2xL59+/Jci1baqWEeapgD8GweHTt2xNatW6HT6RAZGYl58+bpx3/66SfUq1dPwYTgmpui6NOnj7z00kty6NChbGOHDh2SOnXqSFhYmALJCmbz5s1SpkwZadeunSxYsEDWrl0ra9askQULFkj79u3FwsJCtmzZonTMIgsLC5OgoCClYxTJpEmTpF+/fkrHKDJTmQfXopUuapiHGuYgIvLnn3/K5MmTZfz48bJ7926l42TDNTdFYG9vj127dqFx48Y5jh86dAjt2rXD/fv3SzZYAfn5+eHNN9/E22+/neP43LlzsWrVKpw5c6aEkxGVDlyLRmRaeFqqiPJqVlQaGhkZ4/Lly3leHt21a1dMnz69BBMRlS7P16IR0f+5dOlSjksZqlWrpnQ0FjdFERoaisGDB+PLL7/Mtibl2LFjCA8PR+fOnRVKZ7zni9wiIiJyHDeVRW7As46Za9euzfYXLjAwEL169YK1tbXSEfN148YNLF26NMcPjfDwcHh4eCgd0ShqmQcRGTKF9aY8LVUE9+/fR69evbBr1y7Y29vD2dkZGo0GiYmJSE5ORkhICNauXau/oWZppYbmdwBw9uxZtG3bFo8fP0aLFi0MGmTt27cP1tbW2L17N2rVqqV01FyppXGcWuZBRNmFhYUhOjoay5cvz7Ys4/DhwxgyZAjq1q2Lb775RqGELG6Kxfnz53Hw4EEkJCQAAFxdXREQEICaNWsqnMx4Bw8exKJFi3Kcx+jRoxEQEKBwwvwFBQXB1dUV33zzDSwsLAzG0tLS0K9fP8THx+PXX39VKGH+GjZsiFdeeQULFizIcXzs2LE4cOAAjh49WsLJCkYt8yCi7ExhvSmLG1KNsmXL4tixY7kemTl9+jQaNWqEx48fl3Ay46mlcZxa5kFE2dnb22P37t1o1KhRjuOHDx9GSEiIosUNm/i9QPHx8YiNjVU6xr+Gg4NDns0TL1++DAcHhxJMVHBqaRynlnnkZ//+/UhOTlY6RpGtWrUKV65cUTpGkalhHqYwh+frTXO60WepWW+qzBXo/w41a9YUrVardIwimzRpkvTv31/pGPmaPn262NnZybx58yQ6Olri4+MlISFBoqOjZd68eeLg4CDvvvuu0jHz9Nlnn4mFhYWMGDFCtmzZIgcPHpRDhw7Jli1bZMSIEWJpaSlLly5VOma+1DKP/Gg0Gilfvrx89NFHSkcpEo1GIxYWFjJy5EiloxSJGuZhCnNISkqSdu3aiUajEQcHB6lRo4bUrFlTHBwcRKvVSvv27SUpKUnRjLxa6gVatWpVqT4FYqybN28iLi5O6Rj5mjFjBqysrDB//nxEREToL8UXEbi6uuLtt9/O9Yqw0mL48OFwdHTEggUL8MUXXyAzMxMAYGZmBn9/f6xatQrdu3dXOGX+1DKP/Fy7dg3Xrl3Drl27lI5SJFlZWbh+/TrnUQqYwhzs7e2xc+dOnDt3DocOHSqV60255oZU6dq1awZ/4by9vRVOVHBqaRynlnkQkelgcVNMYmJiDHp5eHl5KR2JiChfGRkZuHXrFjw9PZWOkq/MzEzExsbCy8sLWq0Wqamp+OGHH5CVlYWgoCC4uLgoHfFf488//8Tx48fRsmVLeHt748yZM/jss8+QlZWFbt26ISQkRNF8XFBcRAsWLICHhwd8fHwQEBCAJk2awMfHBx4eHiZxU73nHj16hOXLl6N///5o3749OnTogP79+2PFihWK3raeSEk//vgjBg0ahIiICJw/f95gLCkpCa1atVIoWfE5c+aMSRzZ/PPPP1GpUiVUrVoV9erVw40bN9CgQQMMGDAAgwcPhq+vr0m0FliyZAnatGmD7t27Z7tx7J07d0yiYeqmTZvg7++PiRMn4uWXX8bevXvxyiuv4NKlS7h+/To6duyItWvXKpqRxU0RvPfee5gxYwZGjhyJqKgo3Lx5Ezdu3EBUVBRGjhyJGTNmYNasWUrHzNfZs2dRvXp1REREICkpCZ6enqhUqRKSkpIwceJE1KhRA2fPnlU6JlGJWrt2Lbp06YKEhAQcPHgQ9erVw5o1a/TjaWlp2Ldvn4IJ/10iIiLwyiuv4M8//0RQUBBCQkLg6+uLpKQkJCUloWPHjpg8ebLSMfP0ySefYOLEiahZsyYsLS3RoUMHzJkzRz+emZmJmJgYBRMa5/3338e7776LO3fuYNmyZXj99dcxbtw47NmzBz/99BPmzp1rcJdwRSi6nNnEVapUSTZv3pzr+Pfffy/u7u4lF6iQWrZsKT179pTU1NRsY6mpqdKrVy9p2bKlAsmIlFOvXj355JNP9I+/++47sbGxkRUrVoiISEJCgklcDVmvXr08v0zlqk4HBwc5e/asiIg8fvxYzMzM5PDhw/rx06dPi6Ojo1LxjFKrVi1Zs2aN/nFkZKQ4OzvL1KlTRcR03lPW1tZy7do1ERHJysqSMmXKyMmTJ/XjV65cERsbG4XSPcOrpYrg7t27uTYpA4Dq1asjKSmpBBMVzuHDh3Hs2LFsXX0BwMLCApMnT861WRORWl28eBGdOnXSP3799ddRoUIFdO7cGenp6ejWrZuC6Yx39uxZ9OzZM9dTT/Hx8bh48WIJpyo4EYG5+bNfWf/8L/DsSrysrCxFshnr2rVraNq0qf5xQEAAfvnlF7Ru3Rrp6ekYM2aMcuEKoFy5crh79y4qV66M+/fvIyMjA3fv3tWP3717FzY2NgomBI/cFEWLFi3kjTfekPT09Gxj6enp0rt3b2nRokXJBysgd3d32bJlS67jmzdvNokjUMaIiYmRjIwMpWMUyb59++T+/ftKxyiy0j4PNzc3OXjwYLbtv/32m9jY2MiUKVNM4l/Z/v7+smTJklzHT5w4YRLzaN26tQwcOFBu3Lgh7777rlStWtWg/9bw4cOlWbNmCibMn4eHh+zfvz/b9jNnzoiLi4v06dPHJP4s3nzzTWncuLGsXr1aQkNDpV27dtKkSRM5d+6cnD9/Xlq0aCGvv/66ohlZ3BTByZMnxdXVVRwcHKRr164ydOhQCQ8Pl65du0r58uXFzc1NTp8+rXTMfKmh+Z2xNBqNVK9eXTZt2qR0lEJTU+O40jyPLl26yLRp03Ic+/XXX8Xa2tokfhGNHj1aRo8enev45cuXTeK085EjR6R8+fKi1WrF2dlZzpw5I40bNxZXV1dxd3cXKysr+fnnn5WOmadevXrl+mdx+vRpcXJyMon3VEJCgrRp00ZsbGykffv2kpycLCNHjhSNRiNarVaqVasmly9fVjQjLwUvogcPHmD16tU5NjLq3bu3ord8L4i5c+di0aJF+svZgf9rfjdmzJhS3/zOWPv27cO1a9ewe/duxVfzF1ZMTIy+cdzfFyOamtI+j3379iEyMhKTJk3Kcfy3337DN998g5UrV5Zwsn+vhw8f4sKFC6hRowZsbGzw9OlTrFmzBk+ePEHbtm3zXCZQGpw8eRJRUVHo379/juNnzpzBxo0bMX369BJOVjyuXr2Kx48fo2bNmganDJXA4oYMqKH5HRER/buxuCFVS01NxY0bN1CpUiVYWloqHcdomZmZMDMz0z8+cuQIsrKyUK9ePZOahxo8evQIUVFRiI+Ph5mZGby9vVG/fn39Ec7SLioqCv7+/krHKDZXr17FgQMH9H8ePj4+aNOmjckcJQey//0+fPgwUlNTERAQoIoO3seOHcPjx4/RvHlz5UIoeEqMqFitXLlSvwD0yZMnMnDgQDEzMxOtVivm5uYydOhQefr0qcIp83bt2jWpX7++mJmZSYcOHSQ5OVnatGkjGo1GNBqN+Pj4yIULF5SOma9du3YZLLRfs2aNvPzyy1K2bFmpUqWKLFq0SMF0xsnIyJCJEydK2bJlRavVilar1f85eHl5ydatW5WOaJTn75vZs2fLjRs3lI5TaA8fPpTXX39d/2eg1WrF1dVVzMzMxMbGRj799FOlI+br1q1b0rRpUzEzM5PmzZvLvXv3pGPHjvo5Va9eXW7duqV0zCIrDe0F2MSPVGP27Nn687xTp07F3r178d133+nPY//666+YOnWqwinzNmHCBJQrVw5btmyBjY0NOnTogIyMDMTFxeHmzZuoVq0a/vvf/yodM1/t27fHvXv3ADzrZhoWFobmzZtj+fLl6Nq1KyIiIvDtt98qnDJvU6ZMwfbt27F27Vrs2LEDgYGB+OCDD3D27FmEhYXhP//5D3bv3q10TKO0bt0an3zyCSpXroxOnTphy5Yt+puZmopx48YhPj4eJ06cwLlz59CtWzeEhYUhJSUFixYtQkRERKlfR/f87+7mzZvh5uaGTp06ISUlBXFxcYiJiYGLiwtmz56tcMqi27t3L65evapsCEVLK6JiZGlpKTExMSIiUr16ddm5c6fB+L59+8TT01OJaEZzcnKSEydOiIjI/fv3RaPRyO+//64fj4qKEhcXF4XSGU+j0UhiYqKIiAQGBma76mjevHnSsGFDJaIZzd3d3eCy3Rs3boiNjY3+6N/MmTMlICBAqXhGe/5nkZ6eLhs3bpQOHTqImZmZuLi4SEREhJw/f17piEapUKGCHDt2TP/43r17otPp5NGjRyIi8umnn0rdunWVimeUv7cXuHv3rmg0GoMrvH755Rfx8fFRKp6q8MgNqYarqyuuXLkC4Nk6iQoVKhiMOzk5GTSaKo2ePn0KOzs7AM8aZZmZmaFcuXL6cVtbWzx+/FipeIVy6dIldOnSxWBb586dS33juAcPHqBixYr6x25ubnj69Km+Medrr72GP//8U6l4BWZubo7XXnsNP/74I2JiYjBixAhs3LgRtWrVUnZthJEyMjIM1tXY2NggIyNDf++74ODgbPf/Km2SkpL076ny5cujbNmyBjdZrlKlCuLj45WKV2APHz7Evn37sH79emzYsAH79u3Dw4cPlY4FgPeWeuEGDBiA//3vf0rHKLLY2NhSfxj7jTfewJQpU3D//n306dMHM2fO1P9Fe/z4MWbMmIHAwECFU+atdu3a+OqrrwAA33zzDRwdHbFu3Tr9+Lfffovq1asrFa9Azp49i5MnT8LKyipb59isrKxS/3566aWXDE6dbdiwATY2NnB1dQXwbA6msLg7p4XPFStWxNSpU3HlyhXs3r0bHh4eCiQrmIYNG2LRokX6x4sWLYKTkxOcnJwAPPtFq3hX3Hw4OzsbFC8jR45E+fLl9Y+TkpJgbW2tRLQCycjIwOjRo+Hs7IygoCD07dsXffr0QVBQEJydnTFmzBikp6crG1LpQ0dq16JFC6lcubLUqVNH6ShFYgrN71JTU6Vz587i4OAgbdu2FZ1OJ2XLlpVq1aqJtbW1eHp6lvrFuD/99JPodDqxsLAQKysr2b9/v1SvXl0aNmwoTZo0ETMzM1m/fr3SMfP1fMHn84WSCxcuNBhfu3at1KpVS6F0xvn555/F0tJSGjVqJM2bNxdzc3NZsGCBfnzevHnSqlUr5QIa6e+nCE1ZVFSUlC9fXlxdXcXT01MsLCzk22+/1Y9/+umnEhYWpmDC/HXu3Dnb34W/+/TTT03iPTVq1CipWLGirFu3TpKSkvTbk5KSZN26deLh4ZFn48iSwEvBS8jzxlOmypSa3/3000/Ytm0brl69iqysLLi5uSEwMBC9e/c2iX8VXbt2DcePH0eDBg3g5eWFxMREfPbZZ3j8+DE6duyIoKAgpSPm6593NraxsYGjo6P+8apVqwAAYWFhJZqroE6ePIn169cjNTUVISEhaNu2rdKRCmzfvn0IDAxUvKlacYiPj8f27duRmpqKVq1aoVatWkpHKlZHjx6FlZUV/Pz8lI6SJycnJ6xfvx6tWrXKcXzv3r3o2bMn/vrrrxJO9n9Y3BAREZHRbGxsEBkZiTp16uQ4Hh0djVdeeUXR9TcsbopIRPDzzz8jMjJSf+sCFxcXBAYGonXr1ibT6OufTLX5nVo8fPgQUVFRBu8pf3//Ur+mID+JiYlITU2Fp6en0lEKJSkpCZcvX4abmxsqVaqkdJxikZSUhG3btpX6o2jAs8/b69evw8PDA+bm5khLS8PmzZuRmpqKDh06ZLuIoLRJTU2FVqvVN+q7cuUKvvrqK8TGxsLLywsDBw40ia7woaGhePLkCdasWQMXFxeDscTERPTp0wc6nQ5bt25VKCG45qYobty4IXXr1hUzMzN5+eWXJTg4WNq2bSsvv/yymJmZSf369U2iaZYamt+JiNjY2MiAAQPkjz/+UDpKoaWnp8uoUaPEyspKNBqNWFpaioWFhWg0GrGyspLRo0dLWlqa0jHzlZKSIm+88YZ4enpKWFiYpKamyvDhw/VrcZo3by7JyclKx8zTpEmT9JcZp6WlyeDBg/XriLRarXTr1k2ePHmicMqii46OVrzhmjHOnz8vXl5eotFopGrVqnL16lXx9/cXa2trKVu2rFSoUEEuXryodMw8BQUF6dctHjhwQCwtLaVOnTrSo0cPqVevnpQtW1YiIyMVTpm/2NhY8fPzE3Nzc6lbt66EhIRIu3btpG7dumJubi516tSRuLg4RTOyuCmCzp07S6tWrXLsKHnr1i1p1aqVdOnSpeSDFVDVqlXl6NGjIiIyYcIEqVy5snz//fdy7tw52bJli1SvXl0mTpyocMr8aTQaqV27tmg0GqlZs6Z89NFHJreQ0hQW6hlj5MiRUrNmTfnkk0+kZcuW0qVLF/Hz85MDBw7I/v37xc/PTyZPnqx0zDxptVr9+2f27Nni5OQkmzZtkps3b8q2bdukYsWKMnPmTIVT5i85OTnPr99//90kipsuXbpI586d5eTJkzJmzBipVauWdOnSRdLS0iQ1NVW6dOkib775ptIx82Rvb6+/W3aLFi1k7NixBuPvvPOOBAYGKhGtwDIzM2XHjh0ybdo0GTJkiAwZMkSmTZsmO3fulMzMTKXjsbgpCmtra4mOjs51/Pjx42JtbV2CiQpHDc3vRP7vqpDo6GgZOXKklC9fXiwsLOTVV1+VHTt2SFZWltIR81WhQgXZu3dvruM///yzVKhQoQQTFY6Hh4f88ssvIiJy8+ZN0Wg0Brcr+PHHH6VGjRpKxTPK368yqlu3rnz55ZcG4+vXrxdfX18lohXI8yNNuX09Hy/t/t7g8uHDh9kaXEZGRpb6zylra2s5d+6ciIi4uLhk+/1x+fJlsbGxUSKa6rDPTRFYWVnpW8znJCkpCVZWViWYqHDU0Pzu715++WUsXrwY8fHx+Prrr5GcnIxOnTrB09MT06ZNUzpenp48eZLnugFHR0c8efKkBBMVzu3bt1G1alUAgLu7O6ysrAyuFqxduzbi4uKUime052vm4uLi0KhRI4OxRo0aZbsqrDQqV64c5syZg19++SXHr2XLlikd0SgPHz7U94SxtraGtbU13Nzc9OOVKlVCYmKiUvGM0rhxY2zbtg3As4Z9/2wCGR0dbdD3pjSKjY0t0P43b958QUnyoXR1ZcpGjhwpHh4e8t1338n9+/f12+/fvy/fffedeHp6yqhRoxRMaJzJkydLQECAJCUlydtvvy2hoaHy4MEDERF59OiRdO/eXYKDgxVOmb+/n0b4p2vXrsk777wjHh4eJZyqYDp16iStW7eWhISEbGMJCQnStm1bCQ0NVSBZwbi7u0tUVJT+ca9evQz+bE6fPi0ODg5KRDOaRqOR2bNny6JFi7LdikHk2VqV0j4HEZGWLVvK3Llzcx2Pjo4WjUZTgokKp0qVKgZHapYsWSIpKSn6x1FRUeLq6qpENKNFRkaKnZ2dTJ8+XRYvXiwVKlSQd955R9asWSPTpk0Te3v7PP+sSgNnZ2cZNGiQHD58ONd97t+/L8uWLZPatWvLJ598UoLp/g+LmyJITU2V8PBwsbCwEK1WKzqdTnQ6nWi1WrGwsJBhw4ZJamqq0jHzpYbmdyLGNSsr7aemTGGhnjHatWsnn3/+ea7jK1eulKZNm5ZgooLz8vKSypUr67/+2XxtwYIF0qRJE4XSGW/ZsmV53oU9ISFBZsyYUYKJCmfo0KGyfPnyXMfnzJkjHTp0KMFEhRMZGSlNmjTRN7h8/lWxYsU8G/yVFnfv3pXx48eLg4ODODs7S4cOHWTQoEEycuRIeeONN6RevXpiYWEhTZs2lR07diiWk5eCF4OUlBT9ZbvAs9M8/v7+BvdBMQWm3vzu3XffxcSJE1G2bFmloxRJVlYWdu3ahUOHDhm8pwICAhAcHAyttvSfTb537x60Wi3s7e1zHN+5cyesrKzQsmXLEs1VnA4dOgRLS0vUq1dP6SiEZ80vdTqdwamq0uyvv/4y+KytXLmy0pEK5OnTp9ixYwd+//13XL9+XX9KvV69eggJCVG8ESGLGyIiIlIV0+/HXYodO3YMjx8/Nok77uZEnp22NIkjBXn566+/YG9vr2+cZcoePXqEqKgok31PmRpRaZPOvzP1z6nn1DAPNcyhtDDt31ql3PO7pJZ2GRkZeOedd9CiRQtMnz4dADBv3jzY2NjAysoKffv2RVpamsIp87ds2TKkpqYCePZL6f3334eDgwNcXV1hb2+PcePGZbs7tam5fPmySbyn0tPTERERgapVq6JRo0ZYuXKlwXhiYiLMzMwUSmecmzdvon79+mjfvj02b96Mq1ev4vLly9i8eTPatWuHBg0aKHclSDEylc+p/KhhHmqYQ6mh2Gqff4GbN2/K9evXlY6Rr3feeUdcXFxk3LhxUqtWLQkPDxcPDw9ZvXq1rFq1SipVqlTqV/CLGF4t9fnnn4u1tbV8/PHH8scff8jixYvFzs5OFi9erHDKojGVbrLTp08XFxcXmTdvnkyZMkXs7OxkyJAh+vGEhIRSf4WOWpp05sdUPqfyo4Z5qGEOpQWLGxIfHx/Ztm2biIhcunRJtFqtrFu3Tj++YcMG8fPzUyqe0f5+tVTDhg1l/vz5BuPLly+XOnXqKBHNaA4ODnl+2dramkRxU7VqVf17SuRZc7Jq1apJv379JCsrSxISEkr9PNTSpJPo34hrboqBqd/k8NatW3j55ZcBAFWrVoWFhYX+MQA0aNDAJJqVAf/XdO3atWto3bq1wVirVq0wduxYJWIZLTU1FcOGDcNLL72U43hMTAzefffdEk5VcDdv3jS4WqJKlSr47bff0KpVK/Tp0wcffvihgumMo5Ymnc+Z+ufUc2qYhxrmUOopXV2ZMrXc5NDFxUVOnjypf9y0aVODG36eO3dObG1tlYhWIBqNRlatWiU//PCDeHh4yKFDhwzGT58+Xern0bRp0zx7XZjKaSlvb2/5+eefs22/efOmVK9eXdq0aVPq56GWJp1q+ZxSwzzUMAdTweKmCNRyk8OgoCD5+uuvcx3fsGGD+Pv7l2CiwvlnU6zZs2cbjC9fvlzq1aunUDrjzJ49O8+GarGxsdKvX78STFQ4AwcOlAEDBuQ4duPGDalatWqpL27U0qRTLZ9TapiHGuZgKtjnpgicnJywfv16tGrVKsfxvXv3omfPnvjrr79KOFnBXLx4EWXKlIG3t3eO42vXroW5uTm6d+9ewsmK1/bt21GmTBmEhIQoHUX1YmJicP78+Vx/1vHx8di9ezf69u1bwskKztSbdKrlc0oN81DDHEwF19wUgVpucli9evU8x3v37l1CSV6sTp06KR3hX8PLywteXl65jru5uZlEYQMAtra2Jn15rlo+p9QwDzXMwVTwyE0RhIaG4smTJ1izZg1cXFwMxhITE9GnTx/odDps3bpVoYRFk5iYiNTUVHh6eiodpUAuXbqUrela06ZNUa1aNaWjGUVU0jhOLfPIjak0XFPL55Qa5qGGOZgKFjdFEBcXhw4dOuD8+fPw8/ODi4sLNBoNEhIScPr0adSqVQs//vgjKlWqpHTUPD148ADDhg3D77//jpYtW2L58uUYO3Ysli5dCo1Gg1deeQXbtm0r9Yfhk5OTERYWhm3btsHOzg7Ozs4QEfz1119ISUlBaGgoVq1aVarncfPmTXTq1AmnTp3Sv6dEBLdv38bp06fx8ssvY+vWrahYsaLSUfOklnnkxdfXFxcvXkRmZqbSUfKkls8pNcxDDXMwFSxuikgNNzl866238PPPP2P48OH4/vvvYWdnhytXruDzzz9HVlYWhg8fjs6dO2P27NlKR81TWFgYoqOjsXz5cjRu3Nhg7PDhwxgyZAjq1q2Lb775RqGE+evSpQsePnyI1atXZ7sBYHx8PN58802UK1cOW7ZsUSagkdQyj7zcunUL6enpeZ5+Ky3U8DkFqGMeapiDKWBxQ/D09MQ333yDoKAg3Lp1C5UqVcIPP/yA0NBQAMCOHTswbtw4nD9/XuGkebO3t8euXbuyFTbPHTp0CO3atcP9+/dLNlgB2NjY4I8//jDoM/R3J06cQLNmzfDw4cMSTlYwapkHEZkmloiFFBsbW6D9S/M9aG7fvo2qVasCANzd3WFlZYUaNWrox2vXro24uDil4hVIXus4TGGNh1oax6llHsCzhmv79u3D+vXrsWHDBuzbt89kijK1fE6pYR5qmIMpYXFTSA0bNsTgwYNx5MiRXPdJTk7G8uXL4efnh++//74E0xWMo6OjwaWHXbp0gb29vf7xw4cPYWlpqUCyggkNDcXgwYNx7NixbGPHjh1DeHg4OnfurEAy4/Xs2RN9+/bFxo0bkZycrN+enJyMjRs3on///iZx9Zoa5pGRkYHRo0fD2dkZQUFB6Nu3r/7Ghs7OzhgzZgzS09OVjpkntXxOqWEeapiDKeGl4IV07tw5vP/++2jXrh3KlCmDBg0awN3dHTqdDklJSTh79izOnDmDBg0aYN68eWjfvr3SkXNVp04dHD16FPXr1wfwrK/N3x09ehS+vr5KRCuQxYsXo1evXmjUqBHs7e3h7OwMjUaDxMREJCcnIyQkBJ988onSMfP08ccfIyMjA2+88QYyMjJgYWEBAEhLS4O5uTkGDhyIefPmKZwyf2qYx/jx47Fp0yasXLkSISEh+oL//v372LVrFyZOnAgAWLhwoXIh86GWzyk1zEMNczAlXHNTRE+fPsWOHTvw+++/4/r16/o+BvXq1UNISIjB/XVKq3v37kGr1Rocrfm7nTt3wsrKCi1btizRXIV17ty5HBfr1axZU+FkxjP1xnHPmfI81NRwTQ2fU4A65qGGOZgCFjdERDmwsbFBZGQk6tSpk+N4dHQ0XnnlFZNZf0P0b8LihvRMvfkdwMZxpsIU5sGGa0Smi8UNqaL5HcDGcabEFObBhmtEpovFDami+R3AxnGmxFTmwYZrRKaJxQ2povkdwMZxRET0DC8FJwCm3/wOUF/juOdXGT1fN+Tv7w8bGxuloxWIqc4jNja2QDeMvXnzpkmf7iRSHaF/vTfffFPq1KkjR48ezTZ29OhRqVu3rvTp00eBZAUzcuRI8fDwkO+++07u37+v337//n357rvvxNPTU0aNGqVgwvylp6fLqFGjxMrKSjQajVhaWoqFhYVoNBqxsrKS0aNHS1pamtIx82Xq83B2dpZBgwbJ4cOHc93n/v37smzZMqldu7Z88sknJZiOiPLD4oYkKSlJ2rVrJxqNRhwcHKRGjRpSs2ZNcXBwEK1WK+3bt5ekpCSlY+YrNTVVwsPDxcLCQrRareh0OtHpdKLVasXCwkKGDRsmqampSsfM06hRo6RixYqybt06g595UlKSrFu3Tjw8PGT06NGK5TOWqc/j7t27Mn78eHFwcBBnZ2fp0KGDDBo0SEaOHClvvPGG1KtXTywsLKRp06ayY8cOpeMS0T9wzQ3pqaH5HcDGcaWBWubBhmtEpolrbkjP19fXJG6zkB9bW1sEBQUpHaNQnv/yzI2joyOePHlSgokKRy3z0Ol0ePXVV/Hqq68qHYWICoBHbgiA+pvfAWwcV5LUMg8iMk0sbuhf0fwOYOO4kqSWeRCRaWJxQ/+K5ncAG8eVNLXMg4hMD4sbYvM7IiJSFS4oJlU1vwPYOE5papkHEZkuHhcm9OzZE3379sXGjRuRnJys356cnIyNGzeif//+6N27t4IJjZORkYHRo0fD2dkZQUFB6Nu3L/r06YOgoCA4OztjzJgxSE9PVzpmrho2bIjBgwfjyJEjue6TnJyM5cuXw8/PD99//30JpjOeWuZBRKaLR24IH3/8MTIyMvDGG28gIyMDFhYWAIC0tDSYm5tj4MCBmDdvnsIp8zd+/Hhs2rQJK1euREhICOzt7QEA9+/fx65duzBx4kQAwMKFC5ULmYdz587h/fffR7t27VCmTBk0aNAA7u7u0Ol0SEpKwtmzZ3HmzBk0aNAA8+bNQ/v27ZWOnCO1zIOITBfX3JCeKTe/A9g4rrRRyzyIyPSwuCHVsLGxQWRkJOrUqZPjeHR0NF555RUujCYiUjmuuaF8HTt2DPv371c6Rr6CgoIwbtw4JCYmZhtLTExERERErkd1iIhIPXjkhvJlCs3vADaOIyKiZ1jcUL5MpfkdwMZxRETE4oaIiIhUhpeCk56pNr8D2DiOiIj+D4/Rk8k3vwPYOI6IiP4Pj9yQyTe/A9g4joiI/g/X3JBqmt8BbBxHREQ8ckOAvgDIjaOjI548eVKCiQpPp9Ph1Vdfxauvvqp0FCIiUgiP3BBCQ0Px5MkTrFmzBi4uLgZjiYmJ6NOnD3Q6HbZu3apQQiIiIuOxuCE2vyMiIlVhcUMA2PyOiIjUg8UNERERqQr/Of4vFxsbW6D9b968+YKSEBERFQ8WN/9ybH5HRERqw0vB/+XY/I6IiNSGa24IAJvfERGRerC4ISIiIlXhmhsiIiJSFRY3REREpCosboiIiEhVWNwQERGRqrC4ISLKQeXKlbFw4UKlYxBRIbC4IaIiu337NoYOHQpPT09YWlrC1dUVISEhOHjwoNLRsvn666+h0Wj0XzY2NvD398/WoPLo0aMYMmSIQimJqCjYxI+Iiuy1115Deno6vvnmG/j4+CAxMRF79+7FvXv3XthrpqWlwcLColDPtbW1xYULFwAADx48wMqVK9G9e3ecOXMGNWrUAAA4OTkVW1YiKlk8ckNERXL//n0cOHAAc+fORVBQELy8vNCoUSNMmjQJHTt2NNhvyJAhcHFxgU6ng5+fH7Zv364f37RpE2rXrg1LS0tUrlwZH3/8scHrVK5cGbNmzUK/fv1gZ2eHwYMHAwAiIyPRvHlzWFlZwcPDA6NGjcKjR4/yzKzRaODq6gpXV1dUq1YNs2bNglarxcmTJw1e7++npTQaDVasWIFu3bqhbNmyqFatGrZu3aofT0pKwhtvvAEnJydYWVmhWrVqWLlyZaF+pkRUNCxuiKhIbGxsYGNjgy1btiA1NTXHfbKystC+fXtERkZi9erVOHv2LD744AOYmZkBAKKiotC9e3f07NkTp06dwowZMzB16lR8/fXXBt9n3rx58PPzQ1RUFKZOnYpTp04hJCQEr776Kk6ePIn169fjwIEDGDlypNH5MzMz8c033wAA6tevn+e+7777Lrp3746TJ0+iQ4cOeOONN/RHp6ZOnYqzZ89i586dOHfuHJYuXYoKFSoYnYOIipEQERXRxo0bxcHBQXQ6nTRt2lQmTZokf/75p358165dotVq5cKFCzk+v3fv3tK2bVuDbRMnTpRatWrpH3t5eUnXrl0N9unTp48MGTLEYNvvv/8uWq1Wnjx5kuNrrVy5UgCItbW1WFtbi1arFUtLS1m5cqXBfl5eXrJgwQL9YwDyzjvv6B8/fPhQNBqN7Ny5U0REQkNDpX///jm+JhGVLB65IaIie+2113Dr1i1s3boVISEh+O2331C/fn39kZfo6GhUqlQJ1atXz/H5586dQ2BgoMG2wMBAXLp0CZmZmfptDRo0MNgnKioKX3/9tf7okY2NDUJCQpCVlYVr167lmrdcuXKIjo5GdHQ0Tpw4gffffx9Dhw7Ftm3b8pxnnTp19P9vbW2NcuXK4fbt2wCAYcOGYd26dahbty4iIiIQGRmZ5/cioheHxQ0RFQudToe2bdti2rRpiIyMRL9+/TB9+nQAgJWVVZ7PFRFoNJps2/7J2tra4HFWVhaGDh2qL1Sio6Px559/4tKlS6hSpUqur6fValG1alVUrVoVderUwbhx4xAUFIS5c+fmmbNMmTIGjzUaDbKysgAA7du3R0xMDMaMGYNbt26hdevWmDBhQp7fj4heDBY3RPRC1KpVS7+wt06dOrhx4wYuXryY674HDhww2BYZGYnq1avr1+XkpH79+jhz5oy+UPn7V0GvpDIzM8OTJ08K9Jx/cnJyQr9+/bB69WosXLgQy5YtK9L3I6LC4aXgRFQkd+/exX/+8x8MGDAAderUQbly5XDs2DF8+OGH6NKlCwCgRYsWaN68OV577TXMnz8fVatWxfnz56HRaNCuXTuMHz8eDRs2xHvvvYcePXrg4MGD+PTTT7FkyZI8X/u///0vmjRpghEjRmDw4MGwtrbGuXPnsGfPHixevDjX54kIEhISAABPnjzBnj17sGvXLkybNq3QP4dp06bB398ftWvXRmpqKrZv3w5fX99Cfz8iKjwWN0RUJDY2NmjcuDEWLFiAK1euID09HR4eHhg8eDAmT56s32/Tpk2YMGECevXqhUePHqFq1ar44IMPADw7ArNhwwZMmzYN7733Htzc3DBz5kz069cvz9euU6cO9u3bhylTpqBZs2YQEVSpUgU9evTI83kpKSlwc3MDAFhaWsLLywszZ87Ef//730L/HCwsLDBp0iRcv34dVlZWaNasGdatW1fo70dEhaeRnE5sExEREZkorrkhIiIiVWFxQ0RERKrC4oaIiIhUhcUNERERqQqLGyIiIlIVFjdERESkKixuiIiISFVY3BAREZGqsLghIiIiVWFxQ0RERKrC4oaIiIhUhcUNERERqcr/AyyDTN/TIB/EAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ax = stat.plot(kind='bar', y='Bad Rate', color='blue', alpha=0.7, legend=False)\n",
    "ax.set_xlabel(\"Score Bins\")\n",
    "ax.set_ylabel(\"Bad Rate\")\n",
    "ax.set_title(\"Bad Rate by Score Bins\")\n",
    "\n",
    "# display the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1adc9374",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\parth\\AppData\\Local\\Temp\\ipykernel_30004\\313019796.py:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  summary_stats = summary_stats.append({\n",
      "C:\\Users\\parth\\AppData\\Local\\Temp\\ipykernel_30004\\313019796.py:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  summary_stats = summary_stats.append({\n",
      "C:\\Users\\parth\\AppData\\Local\\Temp\\ipykernel_30004\\313019796.py:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  summary_stats = summary_stats.append({\n",
      "C:\\Users\\parth\\AppData\\Local\\Temp\\ipykernel_30004\\313019796.py:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  summary_stats = summary_stats.append({\n",
      "C:\\Users\\parth\\AppData\\Local\\Temp\\ipykernel_30004\\313019796.py:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  summary_stats = summary_stats.append({\n",
      "C:\\Users\\parth\\AppData\\Local\\Temp\\ipykernel_30004\\313019796.py:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  summary_stats = summary_stats.append({\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Minimum value</th>\n",
       "      <th>Maximum value</th>\n",
       "      <th>1 percentile</th>\n",
       "      <th>5 percentile</th>\n",
       "      <th>99 percentile</th>\n",
       "      <th>95 percentile</th>\n",
       "      <th>Median value</th>\n",
       "      <th>Mean value</th>\n",
       "      <th>Missing Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P_2</td>\n",
       "      <td>-3.558910e-01</td>\n",
       "      <td>1.009994</td>\n",
       "      <td>0.010377</td>\n",
       "      <td>0.218859</td>\n",
       "      <td>1.005579</td>\n",
       "      <td>0.974548</td>\n",
       "      <td>0.680059</td>\n",
       "      <td>0.648390</td>\n",
       "      <td>1.195541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D_45</td>\n",
       "      <td>1.908604e-06</td>\n",
       "      <td>1.580646</td>\n",
       "      <td>0.002662</td>\n",
       "      <td>0.008265</td>\n",
       "      <td>0.992696</td>\n",
       "      <td>0.755164</td>\n",
       "      <td>0.153748</td>\n",
       "      <td>0.237492</td>\n",
       "      <td>0.068695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S_3</td>\n",
       "      <td>-3.057465e-01</td>\n",
       "      <td>3.379019</td>\n",
       "      <td>0.006208</td>\n",
       "      <td>0.063091</td>\n",
       "      <td>1.015163</td>\n",
       "      <td>0.618759</td>\n",
       "      <td>0.166040</td>\n",
       "      <td>0.231109</td>\n",
       "      <td>18.248870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B_9</td>\n",
       "      <td>1.537601e-07</td>\n",
       "      <td>16.337705</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>0.001175</td>\n",
       "      <td>0.983008</td>\n",
       "      <td>0.650929</td>\n",
       "      <td>0.028409</td>\n",
       "      <td>0.192829</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D_42</td>\n",
       "      <td>-2.559827e-04</td>\n",
       "      <td>4.189226</td>\n",
       "      <td>0.002694</td>\n",
       "      <td>0.007013</td>\n",
       "      <td>1.077812</td>\n",
       "      <td>0.585220</td>\n",
       "      <td>0.117499</td>\n",
       "      <td>0.185985</td>\n",
       "      <td>81.238927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>D_50</td>\n",
       "      <td>-1.603947e+00</td>\n",
       "      <td>46.147524</td>\n",
       "      <td>0.002726</td>\n",
       "      <td>0.025922</td>\n",
       "      <td>1.014137</td>\n",
       "      <td>0.448881</td>\n",
       "      <td>0.108473</td>\n",
       "      <td>0.173337</td>\n",
       "      <td>57.612534</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Feature  Minimum value  Maximum value  1 percentile  5 percentile  \\\n",
       "0     P_2  -3.558910e-01       1.009994      0.010377      0.218859   \n",
       "1    D_45   1.908604e-06       1.580646      0.002662      0.008265   \n",
       "2     S_3  -3.057465e-01       3.379019      0.006208      0.063091   \n",
       "3     B_9   1.537601e-07      16.337705      0.000239      0.001175   \n",
       "4    D_42  -2.559827e-04       4.189226      0.002694      0.007013   \n",
       "5    D_50  -1.603947e+00      46.147524      0.002726      0.025922   \n",
       "\n",
       "   99 percentile  95 percentile  Median value  Mean value  Missing Value  \n",
       "0       1.005579       0.974548      0.680059    0.648390       1.195541  \n",
       "1       0.992696       0.755164      0.153748    0.237492       0.068695  \n",
       "2       1.015163       0.618759      0.166040    0.231109      18.248870  \n",
       "3       0.983008       0.650929      0.028409    0.192829       0.000000  \n",
       "4       1.077812       0.585220      0.117499    0.185985      81.238927  \n",
       "5       1.014137       0.448881      0.108473    0.173337      57.612534  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get summary stats\n",
    "\n",
    "def get_summary_statistics(df, column_names):\n",
    "    summary_stats = pd.DataFrame(columns=[\"Feature\", \"Minimum value\", \"Maximum value\", \"1 percentile\", \"5 percentile\", \"99 percentile\", \"95 percentile\", \"Median value\", \"Mean value\", \"Missing Value\"])\n",
    "    for column_name in column_names:\n",
    "        summary_stats = summary_stats.append({\n",
    "            \"Feature\": column_name,\n",
    "            \"Minimum value\": df[column_name].min(),\n",
    "            \"Maximum value\": df[column_name].max(),\n",
    "            \"1 percentile\": df[column_name].quantile(0.01),\n",
    "            \"5 percentile\": df[column_name].quantile(0.05),\n",
    "            \"99 percentile\": df[column_name].quantile(0.99),\n",
    "            \"95 percentile\": df[column_name].quantile(0.95),\n",
    "            \"Median value\": df[column_name].median(),\n",
    "            \"Mean value\": df[column_name].mean(),\n",
    "            \"Missing Value\": df[column_name].isnull().sum() / len(df[column_name]) * 100\n",
    "        }, ignore_index=True)\n",
    "    return summary_stats\n",
    "\n",
    "column_name_input = ['P_2','D_45','S_3','B_9','D_42','D_50']\n",
    "get_summary_statistics(final_data,column_name_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a3f00ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9        0.075384\n",
      "10       0.011603\n",
      "17       0.054218\n",
      "34       0.057162\n",
      "35       0.004674\n",
      "           ...   \n",
      "82942    0.164878\n",
      "82943    0.009772\n",
      "82944    0.065992\n",
      "82947    0.087645\n",
      "82971    0.095395\n",
      "Name: D_45, Length: 16038, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(X_test1_selected['D_45'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2cb8e861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12       0.038232\n",
      "39       0.322905\n",
      "52       0.038526\n",
      "54       0.123623\n",
      "61       0.059196\n",
      "           ...   \n",
      "82957    0.080529\n",
      "82961    0.004656\n",
      "82965    0.359686\n",
      "82966    0.308071\n",
      "82974    0.561051\n",
      "Name: D_45, Length: 11111, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(X_test_selected['D_45'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fe2f12c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_normalized = sc.transform(X_train_selected)\n",
    "X_test_normalized = sc.transform(X_test_selected)\n",
    "X_test1_normalized = sc.transform(X_test1_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "eff8deb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to pandas DF\n",
    "X_train_normalized = pd.DataFrame(X_train_normalized,columns=X_train_selected.columns)\n",
    "X_test_normalized = pd.DataFrame(X_test_normalized,columns=X_test_selected.columns)\n",
    "X_test1_normalized = pd.DataFrame(X_test1_normalized,columns=X_test1_selected.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ee2e31bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>1%</th>\n",
       "      <th>50%</th>\n",
       "      <th>99%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>B_11</th>\n",
       "      <td>55826.0</td>\n",
       "      <td>-2.660112e-17</td>\n",
       "      <td>1.000009</td>\n",
       "      <td>-0.532956</td>\n",
       "      <td>-0.531428</td>\n",
       "      <td>-0.434838</td>\n",
       "      <td>4.353052</td>\n",
       "      <td>6.623334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R_26</th>\n",
       "      <td>6435.0</td>\n",
       "      <td>9.937661e-18</td>\n",
       "      <td>1.000078</td>\n",
       "      <td>-0.331138</td>\n",
       "      <td>-0.330432</td>\n",
       "      <td>-0.186771</td>\n",
       "      <td>3.306861</td>\n",
       "      <td>22.601578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_49</th>\n",
       "      <td>5454.0</td>\n",
       "      <td>7.816752e-18</td>\n",
       "      <td>1.000092</td>\n",
       "      <td>-0.872858</td>\n",
       "      <td>-0.858330</td>\n",
       "      <td>-0.270438</td>\n",
       "      <td>3.497066</td>\n",
       "      <td>32.812245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_9</th>\n",
       "      <td>55826.0</td>\n",
       "      <td>-4.123811e-17</td>\n",
       "      <td>1.000009</td>\n",
       "      <td>-0.645931</td>\n",
       "      <td>-0.645131</td>\n",
       "      <td>-0.551619</td>\n",
       "      <td>2.739362</td>\n",
       "      <td>54.323911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P_2</th>\n",
       "      <td>55179.0</td>\n",
       "      <td>-2.778867e-16</td>\n",
       "      <td>1.000009</td>\n",
       "      <td>-3.924600</td>\n",
       "      <td>-2.584323</td>\n",
       "      <td>0.133533</td>\n",
       "      <td>1.458064</td>\n",
       "      <td>1.475765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_110</th>\n",
       "      <td>312.0</td>\n",
       "      <td>-3.985416e-17</td>\n",
       "      <td>1.001606</td>\n",
       "      <td>-2.514402</td>\n",
       "      <td>-2.279392</td>\n",
       "      <td>0.405253</td>\n",
       "      <td>0.890362</td>\n",
       "      <td>0.890873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_48</th>\n",
       "      <td>48432.0</td>\n",
       "      <td>1.599132e-16</td>\n",
       "      <td>1.000010</td>\n",
       "      <td>-1.208966</td>\n",
       "      <td>-1.175378</td>\n",
       "      <td>-0.271703</td>\n",
       "      <td>1.890956</td>\n",
       "      <td>26.238041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_43</th>\n",
       "      <td>38334.0</td>\n",
       "      <td>5.560673e-17</td>\n",
       "      <td>1.000013</td>\n",
       "      <td>-0.692987</td>\n",
       "      <td>-0.680945</td>\n",
       "      <td>-0.302693</td>\n",
       "      <td>3.916392</td>\n",
       "      <td>31.597239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_75</th>\n",
       "      <td>55826.0</td>\n",
       "      <td>-3.881982e-17</td>\n",
       "      <td>1.000009</td>\n",
       "      <td>-0.765706</td>\n",
       "      <td>-0.764397</td>\n",
       "      <td>-0.431731</td>\n",
       "      <td>3.731336</td>\n",
       "      <td>10.601733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_56</th>\n",
       "      <td>25154.0</td>\n",
       "      <td>1.299394e-17</td>\n",
       "      <td>1.000020</td>\n",
       "      <td>-1.069606</td>\n",
       "      <td>-0.970446</td>\n",
       "      <td>-0.261640</td>\n",
       "      <td>3.895420</td>\n",
       "      <td>20.792436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R_1</th>\n",
       "      <td>55826.0</td>\n",
       "      <td>4.276544e-17</td>\n",
       "      <td>1.000009</td>\n",
       "      <td>-0.347628</td>\n",
       "      <td>-0.347104</td>\n",
       "      <td>-0.321235</td>\n",
       "      <td>4.208504</td>\n",
       "      <td>12.099909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_112</th>\n",
       "      <td>55780.0</td>\n",
       "      <td>-2.356587e-16</td>\n",
       "      <td>1.000009</td>\n",
       "      <td>-2.333970</td>\n",
       "      <td>-2.328540</td>\n",
       "      <td>0.430439</td>\n",
       "      <td>0.446446</td>\n",
       "      <td>0.446769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_66_1.0</th>\n",
       "      <td>55826.0</td>\n",
       "      <td>6.873018e-18</td>\n",
       "      <td>1.000009</td>\n",
       "      <td>-0.348794</td>\n",
       "      <td>-0.348794</td>\n",
       "      <td>-0.348794</td>\n",
       "      <td>2.867023</td>\n",
       "      <td>2.867023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_79</th>\n",
       "      <td>54517.0</td>\n",
       "      <td>1.094807e-17</td>\n",
       "      <td>1.000009</td>\n",
       "      <td>-0.320268</td>\n",
       "      <td>-0.319784</td>\n",
       "      <td>-0.294235</td>\n",
       "      <td>4.385435</td>\n",
       "      <td>18.406694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_4</th>\n",
       "      <td>55826.0</td>\n",
       "      <td>3.932893e-17</td>\n",
       "      <td>1.000009</td>\n",
       "      <td>-0.777693</td>\n",
       "      <td>-0.774438</td>\n",
       "      <td>-0.400618</td>\n",
       "      <td>3.751313</td>\n",
       "      <td>12.730119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_45</th>\n",
       "      <td>55787.0</td>\n",
       "      <td>1.105546e-16</td>\n",
       "      <td>1.000009</td>\n",
       "      <td>-1.002778</td>\n",
       "      <td>-0.991155</td>\n",
       "      <td>-0.323003</td>\n",
       "      <td>3.113676</td>\n",
       "      <td>5.519249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_50</th>\n",
       "      <td>23801.0</td>\n",
       "      <td>-6.478206e-17</td>\n",
       "      <td>1.000021</td>\n",
       "      <td>-1.151532</td>\n",
       "      <td>-0.317943</td>\n",
       "      <td>-0.120913</td>\n",
       "      <td>1.577803</td>\n",
       "      <td>86.248547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_46</th>\n",
       "      <td>42221.0</td>\n",
       "      <td>-3.273266e-16</td>\n",
       "      <td>1.000012</td>\n",
       "      <td>-27.350874</td>\n",
       "      <td>-2.721115</td>\n",
       "      <td>-0.098944</td>\n",
       "      <td>3.167833</td>\n",
       "      <td>20.014238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_6</th>\n",
       "      <td>55821.0</td>\n",
       "      <td>-1.782053e-17</td>\n",
       "      <td>1.000009</td>\n",
       "      <td>-0.222483</td>\n",
       "      <td>-0.213397</td>\n",
       "      <td>-0.096216</td>\n",
       "      <td>1.228443</td>\n",
       "      <td>104.838132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_42</th>\n",
       "      <td>9846.0</td>\n",
       "      <td>-2.237134e-17</td>\n",
       "      <td>1.000051</td>\n",
       "      <td>-0.766238</td>\n",
       "      <td>-0.755050</td>\n",
       "      <td>-0.270014</td>\n",
       "      <td>3.396420</td>\n",
       "      <td>16.306661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_2</th>\n",
       "      <td>55787.0</td>\n",
       "      <td>-3.260597e-16</td>\n",
       "      <td>1.000009</td>\n",
       "      <td>-1.558516</td>\n",
       "      <td>-1.550723</td>\n",
       "      <td>0.483095</td>\n",
       "      <td>0.973248</td>\n",
       "      <td>0.974005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R_3</th>\n",
       "      <td>55826.0</td>\n",
       "      <td>4.378367e-17</td>\n",
       "      <td>1.000009</td>\n",
       "      <td>-0.576618</td>\n",
       "      <td>-0.575791</td>\n",
       "      <td>-0.533293</td>\n",
       "      <td>3.884514</td>\n",
       "      <td>34.021618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R_27</th>\n",
       "      <td>54292.0</td>\n",
       "      <td>-3.782267e-17</td>\n",
       "      <td>1.000009</td>\n",
       "      <td>-2.830666</td>\n",
       "      <td>-2.769068</td>\n",
       "      <td>0.364303</td>\n",
       "      <td>0.381781</td>\n",
       "      <td>0.382134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_7</th>\n",
       "      <td>55826.0</td>\n",
       "      <td>1.705527e-17</td>\n",
       "      <td>1.000009</td>\n",
       "      <td>-2.807888</td>\n",
       "      <td>-0.798532</td>\n",
       "      <td>-0.469974</td>\n",
       "      <td>3.586686</td>\n",
       "      <td>4.567894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_3</th>\n",
       "      <td>55787.0</td>\n",
       "      <td>-8.750118e-17</td>\n",
       "      <td>1.000009</td>\n",
       "      <td>-0.553947</td>\n",
       "      <td>-0.552850</td>\n",
       "      <td>-0.512024</td>\n",
       "      <td>3.762450</td>\n",
       "      <td>5.684749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_111</th>\n",
       "      <td>312.0</td>\n",
       "      <td>8.255505e-17</td>\n",
       "      <td>1.001606</td>\n",
       "      <td>-3.464645</td>\n",
       "      <td>-3.453955</td>\n",
       "      <td>0.453359</td>\n",
       "      <td>0.475955</td>\n",
       "      <td>0.476744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_5</th>\n",
       "      <td>55826.0</td>\n",
       "      <td>5.600237e-18</td>\n",
       "      <td>1.000009</td>\n",
       "      <td>-0.223402</td>\n",
       "      <td>-0.222376</td>\n",
       "      <td>-0.181712</td>\n",
       "      <td>2.530079</td>\n",
       "      <td>148.626963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_132</th>\n",
       "      <td>5414.0</td>\n",
       "      <td>-1.076182e-16</td>\n",
       "      <td>1.000092</td>\n",
       "      <td>-0.852900</td>\n",
       "      <td>-0.797577</td>\n",
       "      <td>-0.179098</td>\n",
       "      <td>3.004249</td>\n",
       "      <td>17.460501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P_3</th>\n",
       "      <td>51120.0</td>\n",
       "      <td>5.527833e-16</td>\n",
       "      <td>1.000010</td>\n",
       "      <td>-10.005332</td>\n",
       "      <td>-3.384121</td>\n",
       "      <td>0.088624</td>\n",
       "      <td>2.436157</td>\n",
       "      <td>8.984482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_51</th>\n",
       "      <td>55826.0</td>\n",
       "      <td>-3.767432e-17</td>\n",
       "      <td>1.000009</td>\n",
       "      <td>-0.590729</td>\n",
       "      <td>-0.590144</td>\n",
       "      <td>-0.560423</td>\n",
       "      <td>3.649157</td>\n",
       "      <td>9.279878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_1</th>\n",
       "      <td>55826.0</td>\n",
       "      <td>3.716521e-17</td>\n",
       "      <td>1.000009</td>\n",
       "      <td>-3.970610</td>\n",
       "      <td>-0.584520</td>\n",
       "      <td>-0.433869</td>\n",
       "      <td>4.238029</td>\n",
       "      <td>5.642361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_8</th>\n",
       "      <td>55598.0</td>\n",
       "      <td>-2.619901e-17</td>\n",
       "      <td>1.000009</td>\n",
       "      <td>-0.946044</td>\n",
       "      <td>-0.945669</td>\n",
       "      <td>-0.927202</td>\n",
       "      <td>1.077930</td>\n",
       "      <td>1.081766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_41</th>\n",
       "      <td>55787.0</td>\n",
       "      <td>3.935643e-17</td>\n",
       "      <td>1.000009</td>\n",
       "      <td>-0.290168</td>\n",
       "      <td>-0.289568</td>\n",
       "      <td>-0.260035</td>\n",
       "      <td>4.574984</td>\n",
       "      <td>29.532571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_134</th>\n",
       "      <td>2021.0</td>\n",
       "      <td>-4.570537e-17</td>\n",
       "      <td>1.000247</td>\n",
       "      <td>-1.149556</td>\n",
       "      <td>-1.102211</td>\n",
       "      <td>-0.372349</td>\n",
       "      <td>2.260139</td>\n",
       "      <td>2.262494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_114_1.0</th>\n",
       "      <td>55826.0</td>\n",
       "      <td>-1.794621e-16</td>\n",
       "      <td>1.000009</td>\n",
       "      <td>-1.187667</td>\n",
       "      <td>-1.187667</td>\n",
       "      <td>0.841987</td>\n",
       "      <td>0.841987</td>\n",
       "      <td>0.841987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S_3</th>\n",
       "      <td>45736.0</td>\n",
       "      <td>7.620282e-17</td>\n",
       "      <td>1.000011</td>\n",
       "      <td>-2.736030</td>\n",
       "      <td>-1.149286</td>\n",
       "      <td>-0.330365</td>\n",
       "      <td>4.032315</td>\n",
       "      <td>16.068996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_52</th>\n",
       "      <td>55392.0</td>\n",
       "      <td>8.876653e-17</td>\n",
       "      <td>1.000009</td>\n",
       "      <td>-1.051972</td>\n",
       "      <td>-0.988311</td>\n",
       "      <td>-0.210879</td>\n",
       "      <td>4.712651</td>\n",
       "      <td>4.734955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_44</th>\n",
       "      <td>52848.0</td>\n",
       "      <td>-8.564482e-17</td>\n",
       "      <td>1.000009</td>\n",
       "      <td>-0.542053</td>\n",
       "      <td>-0.541329</td>\n",
       "      <td>-0.506563</td>\n",
       "      <td>4.067435</td>\n",
       "      <td>14.968934</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             count          mean       std        min        1%       50%  \\\n",
       "B_11       55826.0 -2.660112e-17  1.000009  -0.532956 -0.531428 -0.434838   \n",
       "R_26        6435.0  9.937661e-18  1.000078  -0.331138 -0.330432 -0.186771   \n",
       "D_49        5454.0  7.816752e-18  1.000092  -0.872858 -0.858330 -0.270438   \n",
       "B_9        55826.0 -4.123811e-17  1.000009  -0.645931 -0.645131 -0.551619   \n",
       "P_2        55179.0 -2.778867e-16  1.000009  -3.924600 -2.584323  0.133533   \n",
       "D_110        312.0 -3.985416e-17  1.001606  -2.514402 -2.279392  0.405253   \n",
       "D_48       48432.0  1.599132e-16  1.000010  -1.208966 -1.175378 -0.271703   \n",
       "D_43       38334.0  5.560673e-17  1.000013  -0.692987 -0.680945 -0.302693   \n",
       "D_75       55826.0 -3.881982e-17  1.000009  -0.765706 -0.764397 -0.431731   \n",
       "D_56       25154.0  1.299394e-17  1.000020  -1.069606 -0.970446 -0.261640   \n",
       "R_1        55826.0  4.276544e-17  1.000009  -0.347628 -0.347104 -0.321235   \n",
       "D_112      55780.0 -2.356587e-16  1.000009  -2.333970 -2.328540  0.430439   \n",
       "D_66_1.0   55826.0  6.873018e-18  1.000009  -0.348794 -0.348794 -0.348794   \n",
       "D_79       54517.0  1.094807e-17  1.000009  -0.320268 -0.319784 -0.294235   \n",
       "B_4        55826.0  3.932893e-17  1.000009  -0.777693 -0.774438 -0.400618   \n",
       "D_45       55787.0  1.105546e-16  1.000009  -1.002778 -0.991155 -0.323003   \n",
       "D_50       23801.0 -6.478206e-17  1.000021  -1.151532 -0.317943 -0.120913   \n",
       "D_46       42221.0 -3.273266e-16  1.000012 -27.350874 -2.721115 -0.098944   \n",
       "B_6        55821.0 -1.782053e-17  1.000009  -0.222483 -0.213397 -0.096216   \n",
       "D_42        9846.0 -2.237134e-17  1.000051  -0.766238 -0.755050 -0.270014   \n",
       "B_2        55787.0 -3.260597e-16  1.000009  -1.558516 -1.550723  0.483095   \n",
       "R_3        55826.0  4.378367e-17  1.000009  -0.576618 -0.575791 -0.533293   \n",
       "R_27       54292.0 -3.782267e-17  1.000009  -2.830666 -2.769068  0.364303   \n",
       "B_7        55826.0  1.705527e-17  1.000009  -2.807888 -0.798532 -0.469974   \n",
       "B_3        55787.0 -8.750118e-17  1.000009  -0.553947 -0.552850 -0.512024   \n",
       "D_111        312.0  8.255505e-17  1.001606  -3.464645 -3.453955  0.453359   \n",
       "B_5        55826.0  5.600237e-18  1.000009  -0.223402 -0.222376 -0.181712   \n",
       "D_132       5414.0 -1.076182e-16  1.000092  -0.852900 -0.797577 -0.179098   \n",
       "P_3        51120.0  5.527833e-16  1.000010 -10.005332 -3.384121  0.088624   \n",
       "D_51       55826.0 -3.767432e-17  1.000009  -0.590729 -0.590144 -0.560423   \n",
       "B_1        55826.0  3.716521e-17  1.000009  -3.970610 -0.584520 -0.433869   \n",
       "B_8        55598.0 -2.619901e-17  1.000009  -0.946044 -0.945669 -0.927202   \n",
       "D_41       55787.0  3.935643e-17  1.000009  -0.290168 -0.289568 -0.260035   \n",
       "D_134       2021.0 -4.570537e-17  1.000247  -1.149556 -1.102211 -0.372349   \n",
       "D_114_1.0  55826.0 -1.794621e-16  1.000009  -1.187667 -1.187667  0.841987   \n",
       "S_3        45736.0  7.620282e-17  1.000011  -2.736030 -1.149286 -0.330365   \n",
       "D_52       55392.0  8.876653e-17  1.000009  -1.051972 -0.988311 -0.210879   \n",
       "D_44       52848.0 -8.564482e-17  1.000009  -0.542053 -0.541329 -0.506563   \n",
       "\n",
       "                99%         max  \n",
       "B_11       4.353052    6.623334  \n",
       "R_26       3.306861   22.601578  \n",
       "D_49       3.497066   32.812245  \n",
       "B_9        2.739362   54.323911  \n",
       "P_2        1.458064    1.475765  \n",
       "D_110      0.890362    0.890873  \n",
       "D_48       1.890956   26.238041  \n",
       "D_43       3.916392   31.597239  \n",
       "D_75       3.731336   10.601733  \n",
       "D_56       3.895420   20.792436  \n",
       "R_1        4.208504   12.099909  \n",
       "D_112      0.446446    0.446769  \n",
       "D_66_1.0   2.867023    2.867023  \n",
       "D_79       4.385435   18.406694  \n",
       "B_4        3.751313   12.730119  \n",
       "D_45       3.113676    5.519249  \n",
       "D_50       1.577803   86.248547  \n",
       "D_46       3.167833   20.014238  \n",
       "B_6        1.228443  104.838132  \n",
       "D_42       3.396420   16.306661  \n",
       "B_2        0.973248    0.974005  \n",
       "R_3        3.884514   34.021618  \n",
       "R_27       0.381781    0.382134  \n",
       "B_7        3.586686    4.567894  \n",
       "B_3        3.762450    5.684749  \n",
       "D_111      0.475955    0.476744  \n",
       "B_5        2.530079  148.626963  \n",
       "D_132      3.004249   17.460501  \n",
       "P_3        2.436157    8.984482  \n",
       "D_51       3.649157    9.279878  \n",
       "B_1        4.238029    5.642361  \n",
       "B_8        1.077930    1.081766  \n",
       "D_41       4.574984   29.532571  \n",
       "D_134      2.260139    2.262494  \n",
       "D_114_1.0  0.841987    0.841987  \n",
       "S_3        4.032315   16.068996  \n",
       "D_52       4.712651    4.734955  \n",
       "D_44       4.067435   14.968934  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Outlier Treatment\n",
    "X_train_normalized.describe(percentiles=[0.01,0.99]).transpose()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0997f7c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>1%</th>\n",
       "      <th>50%</th>\n",
       "      <th>99%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>B_11</th>\n",
       "      <td>11111.0</td>\n",
       "      <td>-0.064915</td>\n",
       "      <td>0.916431</td>\n",
       "      <td>-0.532940</td>\n",
       "      <td>-0.531502</td>\n",
       "      <td>-0.457077</td>\n",
       "      <td>3.900623</td>\n",
       "      <td>6.369532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R_26</th>\n",
       "      <td>1019.0</td>\n",
       "      <td>0.009134</td>\n",
       "      <td>1.093030</td>\n",
       "      <td>-0.331090</td>\n",
       "      <td>-0.330263</td>\n",
       "      <td>-0.188001</td>\n",
       "      <td>3.294335</td>\n",
       "      <td>25.117755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_49</th>\n",
       "      <td>989.0</td>\n",
       "      <td>0.055363</td>\n",
       "      <td>1.901818</td>\n",
       "      <td>-0.866104</td>\n",
       "      <td>-0.849219</td>\n",
       "      <td>-0.308957</td>\n",
       "      <td>4.378319</td>\n",
       "      <td>49.344205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_9</th>\n",
       "      <td>11111.0</td>\n",
       "      <td>-0.076743</td>\n",
       "      <td>0.938358</td>\n",
       "      <td>-0.645929</td>\n",
       "      <td>-0.645121</td>\n",
       "      <td>-0.584148</td>\n",
       "      <td>2.458276</td>\n",
       "      <td>22.672212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P_2</th>\n",
       "      <td>10962.0</td>\n",
       "      <td>0.082564</td>\n",
       "      <td>0.956063</td>\n",
       "      <td>-3.887774</td>\n",
       "      <td>-2.447798</td>\n",
       "      <td>0.231603</td>\n",
       "      <td>1.458504</td>\n",
       "      <td>1.475715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_110</th>\n",
       "      <td>36.0</td>\n",
       "      <td>-0.049389</td>\n",
       "      <td>1.022287</td>\n",
       "      <td>-2.173892</td>\n",
       "      <td>-2.151003</td>\n",
       "      <td>0.265817</td>\n",
       "      <td>0.886587</td>\n",
       "      <td>0.886762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_48</th>\n",
       "      <td>9536.0</td>\n",
       "      <td>-0.076172</td>\n",
       "      <td>0.961371</td>\n",
       "      <td>-1.209173</td>\n",
       "      <td>-1.176668</td>\n",
       "      <td>-0.372417</td>\n",
       "      <td>1.847588</td>\n",
       "      <td>3.080063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_43</th>\n",
       "      <td>7760.0</td>\n",
       "      <td>-0.075144</td>\n",
       "      <td>0.779066</td>\n",
       "      <td>-0.692489</td>\n",
       "      <td>-0.683456</td>\n",
       "      <td>-0.323928</td>\n",
       "      <td>2.847186</td>\n",
       "      <td>14.811971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_75</th>\n",
       "      <td>11111.0</td>\n",
       "      <td>-0.044932</td>\n",
       "      <td>0.946634</td>\n",
       "      <td>-0.765696</td>\n",
       "      <td>-0.764603</td>\n",
       "      <td>-0.436325</td>\n",
       "      <td>3.428330</td>\n",
       "      <td>8.208111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_56</th>\n",
       "      <td>4667.0</td>\n",
       "      <td>0.019244</td>\n",
       "      <td>0.995334</td>\n",
       "      <td>-1.058151</td>\n",
       "      <td>-0.970933</td>\n",
       "      <td>-0.243087</td>\n",
       "      <td>3.707319</td>\n",
       "      <td>19.143162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R_1</th>\n",
       "      <td>11111.0</td>\n",
       "      <td>-0.078588</td>\n",
       "      <td>0.850462</td>\n",
       "      <td>-0.347627</td>\n",
       "      <td>-0.347056</td>\n",
       "      <td>-0.322319</td>\n",
       "      <td>4.187871</td>\n",
       "      <td>8.714207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_112</th>\n",
       "      <td>11100.0</td>\n",
       "      <td>0.014351</td>\n",
       "      <td>0.986130</td>\n",
       "      <td>-2.333981</td>\n",
       "      <td>-2.328105</td>\n",
       "      <td>0.430468</td>\n",
       "      <td>0.446426</td>\n",
       "      <td>0.446767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_66_1.0</th>\n",
       "      <td>11111.0</td>\n",
       "      <td>0.007490</td>\n",
       "      <td>1.009404</td>\n",
       "      <td>-0.348794</td>\n",
       "      <td>-0.348794</td>\n",
       "      <td>-0.348794</td>\n",
       "      <td>2.867023</td>\n",
       "      <td>2.867023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_79</th>\n",
       "      <td>10861.0</td>\n",
       "      <td>-0.028191</td>\n",
       "      <td>0.956606</td>\n",
       "      <td>-0.320269</td>\n",
       "      <td>-0.319775</td>\n",
       "      <td>-0.294544</td>\n",
       "      <td>4.377383</td>\n",
       "      <td>13.725576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_4</th>\n",
       "      <td>11111.0</td>\n",
       "      <td>-0.012806</td>\n",
       "      <td>0.986876</td>\n",
       "      <td>-0.777599</td>\n",
       "      <td>-0.774977</td>\n",
       "      <td>-0.410886</td>\n",
       "      <td>3.657643</td>\n",
       "      <td>8.734596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_45</th>\n",
       "      <td>11100.0</td>\n",
       "      <td>0.071599</td>\n",
       "      <td>0.995923</td>\n",
       "      <td>-1.002792</td>\n",
       "      <td>-0.988676</td>\n",
       "      <td>-0.132860</td>\n",
       "      <td>3.152429</td>\n",
       "      <td>5.184677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_50</th>\n",
       "      <td>4865.0</td>\n",
       "      <td>0.013401</td>\n",
       "      <td>0.773044</td>\n",
       "      <td>-3.331946</td>\n",
       "      <td>-0.317585</td>\n",
       "      <td>-0.112123</td>\n",
       "      <td>1.813053</td>\n",
       "      <td>31.759715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_46</th>\n",
       "      <td>8600.0</td>\n",
       "      <td>-0.042831</td>\n",
       "      <td>0.946370</td>\n",
       "      <td>-12.722481</td>\n",
       "      <td>-2.799875</td>\n",
       "      <td>-0.115762</td>\n",
       "      <td>2.752970</td>\n",
       "      <td>14.340894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_6</th>\n",
       "      <td>11111.0</td>\n",
       "      <td>0.010238</td>\n",
       "      <td>1.506373</td>\n",
       "      <td>-0.221036</td>\n",
       "      <td>-0.213264</td>\n",
       "      <td>-0.085019</td>\n",
       "      <td>1.268028</td>\n",
       "      <td>116.140088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_42</th>\n",
       "      <td>1641.0</td>\n",
       "      <td>-0.040589</td>\n",
       "      <td>0.828735</td>\n",
       "      <td>-0.765733</td>\n",
       "      <td>-0.756883</td>\n",
       "      <td>-0.292800</td>\n",
       "      <td>3.250130</td>\n",
       "      <td>5.526977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_2</th>\n",
       "      <td>11100.0</td>\n",
       "      <td>0.063944</td>\n",
       "      <td>0.975794</td>\n",
       "      <td>-1.558303</td>\n",
       "      <td>-1.548533</td>\n",
       "      <td>0.485324</td>\n",
       "      <td>0.973208</td>\n",
       "      <td>0.973996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R_3</th>\n",
       "      <td>11111.0</td>\n",
       "      <td>-0.069036</td>\n",
       "      <td>0.841056</td>\n",
       "      <td>-0.576616</td>\n",
       "      <td>-0.575831</td>\n",
       "      <td>-0.534974</td>\n",
       "      <td>3.011624</td>\n",
       "      <td>13.211785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R_27</th>\n",
       "      <td>10955.0</td>\n",
       "      <td>0.078738</td>\n",
       "      <td>0.898507</td>\n",
       "      <td>-2.815399</td>\n",
       "      <td>-2.765514</td>\n",
       "      <td>0.364765</td>\n",
       "      <td>0.381807</td>\n",
       "      <td>0.382134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_7</th>\n",
       "      <td>11111.0</td>\n",
       "      <td>-0.052537</td>\n",
       "      <td>0.960014</td>\n",
       "      <td>-1.026770</td>\n",
       "      <td>-0.799903</td>\n",
       "      <td>-0.511445</td>\n",
       "      <td>3.368122</td>\n",
       "      <td>4.566281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_3</th>\n",
       "      <td>11100.0</td>\n",
       "      <td>-0.060212</td>\n",
       "      <td>0.934264</td>\n",
       "      <td>-0.553936</td>\n",
       "      <td>-0.552816</td>\n",
       "      <td>-0.513643</td>\n",
       "      <td>3.622369</td>\n",
       "      <td>5.101709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_111</th>\n",
       "      <td>36.0</td>\n",
       "      <td>0.081437</td>\n",
       "      <td>1.025545</td>\n",
       "      <td>-3.463131</td>\n",
       "      <td>-3.452533</td>\n",
       "      <td>0.457617</td>\n",
       "      <td>0.475656</td>\n",
       "      <td>0.475856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_5</th>\n",
       "      <td>11111.0</td>\n",
       "      <td>0.001082</td>\n",
       "      <td>0.742506</td>\n",
       "      <td>-0.223393</td>\n",
       "      <td>-0.222380</td>\n",
       "      <td>-0.179268</td>\n",
       "      <td>2.417454</td>\n",
       "      <td>26.727962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_132</th>\n",
       "      <td>989.0</td>\n",
       "      <td>0.062241</td>\n",
       "      <td>1.166851</td>\n",
       "      <td>-0.842345</td>\n",
       "      <td>-0.799547</td>\n",
       "      <td>-0.169625</td>\n",
       "      <td>5.357128</td>\n",
       "      <td>14.525843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P_3</th>\n",
       "      <td>10439.0</td>\n",
       "      <td>0.041666</td>\n",
       "      <td>0.948481</td>\n",
       "      <td>-8.129801</td>\n",
       "      <td>-3.100998</td>\n",
       "      <td>0.115514</td>\n",
       "      <td>2.325122</td>\n",
       "      <td>10.951202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_51</th>\n",
       "      <td>11111.0</td>\n",
       "      <td>0.023343</td>\n",
       "      <td>1.035705</td>\n",
       "      <td>-0.590730</td>\n",
       "      <td>-0.590107</td>\n",
       "      <td>-0.560055</td>\n",
       "      <td>3.654970</td>\n",
       "      <td>7.877477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_1</th>\n",
       "      <td>11111.0</td>\n",
       "      <td>-0.068660</td>\n",
       "      <td>0.923003</td>\n",
       "      <td>-0.915875</td>\n",
       "      <td>-0.584991</td>\n",
       "      <td>-0.458188</td>\n",
       "      <td>3.815486</td>\n",
       "      <td>5.640755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_8</th>\n",
       "      <td>11069.0</td>\n",
       "      <td>-0.108568</td>\n",
       "      <td>0.986877</td>\n",
       "      <td>-0.946045</td>\n",
       "      <td>-0.945769</td>\n",
       "      <td>-0.928947</td>\n",
       "      <td>1.077853</td>\n",
       "      <td>1.079304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_41</th>\n",
       "      <td>11100.0</td>\n",
       "      <td>-0.036427</td>\n",
       "      <td>0.881830</td>\n",
       "      <td>-0.290169</td>\n",
       "      <td>-0.289645</td>\n",
       "      <td>-0.260231</td>\n",
       "      <td>3.906145</td>\n",
       "      <td>20.378172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_134</th>\n",
       "      <td>366.0</td>\n",
       "      <td>-0.023186</td>\n",
       "      <td>1.000317</td>\n",
       "      <td>-1.141057</td>\n",
       "      <td>-1.096063</td>\n",
       "      <td>-0.353624</td>\n",
       "      <td>2.259794</td>\n",
       "      <td>2.262306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_114_1.0</th>\n",
       "      <td>11111.0</td>\n",
       "      <td>-0.014008</td>\n",
       "      <td>1.002365</td>\n",
       "      <td>-1.187667</td>\n",
       "      <td>-1.187667</td>\n",
       "      <td>0.841987</td>\n",
       "      <td>0.841987</td>\n",
       "      <td>0.841987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S_3</th>\n",
       "      <td>9030.0</td>\n",
       "      <td>-0.084893</td>\n",
       "      <td>0.902209</td>\n",
       "      <td>-1.998163</td>\n",
       "      <td>-1.114435</td>\n",
       "      <td>-0.357845</td>\n",
       "      <td>3.557706</td>\n",
       "      <td>10.310700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_52</th>\n",
       "      <td>10976.0</td>\n",
       "      <td>0.025816</td>\n",
       "      <td>0.972684</td>\n",
       "      <td>-1.041839</td>\n",
       "      <td>-0.979267</td>\n",
       "      <td>-0.179625</td>\n",
       "      <td>4.711304</td>\n",
       "      <td>4.734181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_44</th>\n",
       "      <td>10489.0</td>\n",
       "      <td>-0.074327</td>\n",
       "      <td>0.898094</td>\n",
       "      <td>-0.542051</td>\n",
       "      <td>-0.541395</td>\n",
       "      <td>-0.508313</td>\n",
       "      <td>3.512812</td>\n",
       "      <td>7.491310</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             count      mean       std        min        1%       50%  \\\n",
       "B_11       11111.0 -0.064915  0.916431  -0.532940 -0.531502 -0.457077   \n",
       "R_26        1019.0  0.009134  1.093030  -0.331090 -0.330263 -0.188001   \n",
       "D_49         989.0  0.055363  1.901818  -0.866104 -0.849219 -0.308957   \n",
       "B_9        11111.0 -0.076743  0.938358  -0.645929 -0.645121 -0.584148   \n",
       "P_2        10962.0  0.082564  0.956063  -3.887774 -2.447798  0.231603   \n",
       "D_110         36.0 -0.049389  1.022287  -2.173892 -2.151003  0.265817   \n",
       "D_48        9536.0 -0.076172  0.961371  -1.209173 -1.176668 -0.372417   \n",
       "D_43        7760.0 -0.075144  0.779066  -0.692489 -0.683456 -0.323928   \n",
       "D_75       11111.0 -0.044932  0.946634  -0.765696 -0.764603 -0.436325   \n",
       "D_56        4667.0  0.019244  0.995334  -1.058151 -0.970933 -0.243087   \n",
       "R_1        11111.0 -0.078588  0.850462  -0.347627 -0.347056 -0.322319   \n",
       "D_112      11100.0  0.014351  0.986130  -2.333981 -2.328105  0.430468   \n",
       "D_66_1.0   11111.0  0.007490  1.009404  -0.348794 -0.348794 -0.348794   \n",
       "D_79       10861.0 -0.028191  0.956606  -0.320269 -0.319775 -0.294544   \n",
       "B_4        11111.0 -0.012806  0.986876  -0.777599 -0.774977 -0.410886   \n",
       "D_45       11100.0  0.071599  0.995923  -1.002792 -0.988676 -0.132860   \n",
       "D_50        4865.0  0.013401  0.773044  -3.331946 -0.317585 -0.112123   \n",
       "D_46        8600.0 -0.042831  0.946370 -12.722481 -2.799875 -0.115762   \n",
       "B_6        11111.0  0.010238  1.506373  -0.221036 -0.213264 -0.085019   \n",
       "D_42        1641.0 -0.040589  0.828735  -0.765733 -0.756883 -0.292800   \n",
       "B_2        11100.0  0.063944  0.975794  -1.558303 -1.548533  0.485324   \n",
       "R_3        11111.0 -0.069036  0.841056  -0.576616 -0.575831 -0.534974   \n",
       "R_27       10955.0  0.078738  0.898507  -2.815399 -2.765514  0.364765   \n",
       "B_7        11111.0 -0.052537  0.960014  -1.026770 -0.799903 -0.511445   \n",
       "B_3        11100.0 -0.060212  0.934264  -0.553936 -0.552816 -0.513643   \n",
       "D_111         36.0  0.081437  1.025545  -3.463131 -3.452533  0.457617   \n",
       "B_5        11111.0  0.001082  0.742506  -0.223393 -0.222380 -0.179268   \n",
       "D_132        989.0  0.062241  1.166851  -0.842345 -0.799547 -0.169625   \n",
       "P_3        10439.0  0.041666  0.948481  -8.129801 -3.100998  0.115514   \n",
       "D_51       11111.0  0.023343  1.035705  -0.590730 -0.590107 -0.560055   \n",
       "B_1        11111.0 -0.068660  0.923003  -0.915875 -0.584991 -0.458188   \n",
       "B_8        11069.0 -0.108568  0.986877  -0.946045 -0.945769 -0.928947   \n",
       "D_41       11100.0 -0.036427  0.881830  -0.290169 -0.289645 -0.260231   \n",
       "D_134        366.0 -0.023186  1.000317  -1.141057 -1.096063 -0.353624   \n",
       "D_114_1.0  11111.0 -0.014008  1.002365  -1.187667 -1.187667  0.841987   \n",
       "S_3         9030.0 -0.084893  0.902209  -1.998163 -1.114435 -0.357845   \n",
       "D_52       10976.0  0.025816  0.972684  -1.041839 -0.979267 -0.179625   \n",
       "D_44       10489.0 -0.074327  0.898094  -0.542051 -0.541395 -0.508313   \n",
       "\n",
       "                99%         max  \n",
       "B_11       3.900623    6.369532  \n",
       "R_26       3.294335   25.117755  \n",
       "D_49       4.378319   49.344205  \n",
       "B_9        2.458276   22.672212  \n",
       "P_2        1.458504    1.475715  \n",
       "D_110      0.886587    0.886762  \n",
       "D_48       1.847588    3.080063  \n",
       "D_43       2.847186   14.811971  \n",
       "D_75       3.428330    8.208111  \n",
       "D_56       3.707319   19.143162  \n",
       "R_1        4.187871    8.714207  \n",
       "D_112      0.446426    0.446767  \n",
       "D_66_1.0   2.867023    2.867023  \n",
       "D_79       4.377383   13.725576  \n",
       "B_4        3.657643    8.734596  \n",
       "D_45       3.152429    5.184677  \n",
       "D_50       1.813053   31.759715  \n",
       "D_46       2.752970   14.340894  \n",
       "B_6        1.268028  116.140088  \n",
       "D_42       3.250130    5.526977  \n",
       "B_2        0.973208    0.973996  \n",
       "R_3        3.011624   13.211785  \n",
       "R_27       0.381807    0.382134  \n",
       "B_7        3.368122    4.566281  \n",
       "B_3        3.622369    5.101709  \n",
       "D_111      0.475656    0.475856  \n",
       "B_5        2.417454   26.727962  \n",
       "D_132      5.357128   14.525843  \n",
       "P_3        2.325122   10.951202  \n",
       "D_51       3.654970    7.877477  \n",
       "B_1        3.815486    5.640755  \n",
       "B_8        1.077853    1.079304  \n",
       "D_41       3.906145   20.378172  \n",
       "D_134      2.259794    2.262306  \n",
       "D_114_1.0  0.841987    0.841987  \n",
       "S_3        3.557706   10.310700  \n",
       "D_52       4.711304    4.734181  \n",
       "D_44       3.512812    7.491310  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_normalized.describe(percentiles=[0.01,0.99]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b58714de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>1%</th>\n",
       "      <th>50%</th>\n",
       "      <th>99%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>B_11</th>\n",
       "      <td>16038.0</td>\n",
       "      <td>0.121503</td>\n",
       "      <td>1.176472</td>\n",
       "      <td>-0.532954</td>\n",
       "      <td>-0.531206</td>\n",
       "      <td>-0.411362</td>\n",
       "      <td>5.425988</td>\n",
       "      <td>7.940977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R_26</th>\n",
       "      <td>1620.0</td>\n",
       "      <td>0.020764</td>\n",
       "      <td>0.971976</td>\n",
       "      <td>-0.331143</td>\n",
       "      <td>-0.330359</td>\n",
       "      <td>-0.187221</td>\n",
       "      <td>4.126059</td>\n",
       "      <td>14.565508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_49</th>\n",
       "      <td>1877.0</td>\n",
       "      <td>0.011307</td>\n",
       "      <td>1.110656</td>\n",
       "      <td>-0.872777</td>\n",
       "      <td>-0.859872</td>\n",
       "      <td>-0.266965</td>\n",
       "      <td>3.265411</td>\n",
       "      <td>25.355732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_9</th>\n",
       "      <td>16038.0</td>\n",
       "      <td>0.067975</td>\n",
       "      <td>0.969425</td>\n",
       "      <td>-0.645928</td>\n",
       "      <td>-0.645106</td>\n",
       "      <td>-0.507611</td>\n",
       "      <td>2.577903</td>\n",
       "      <td>23.263273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P_2</th>\n",
       "      <td>15842.0</td>\n",
       "      <td>-0.148636</td>\n",
       "      <td>1.077307</td>\n",
       "      <td>-4.165423</td>\n",
       "      <td>-2.916614</td>\n",
       "      <td>-0.046049</td>\n",
       "      <td>1.453646</td>\n",
       "      <td>1.475670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_110</th>\n",
       "      <td>113.0</td>\n",
       "      <td>0.073871</td>\n",
       "      <td>0.978221</td>\n",
       "      <td>-2.256773</td>\n",
       "      <td>-2.230965</td>\n",
       "      <td>0.620538</td>\n",
       "      <td>0.890521</td>\n",
       "      <td>0.890598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_48</th>\n",
       "      <td>13971.0</td>\n",
       "      <td>0.109906</td>\n",
       "      <td>1.037369</td>\n",
       "      <td>-1.208526</td>\n",
       "      <td>-1.173922</td>\n",
       "      <td>-0.116241</td>\n",
       "      <td>1.991288</td>\n",
       "      <td>9.891990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_43</th>\n",
       "      <td>10138.0</td>\n",
       "      <td>0.055121</td>\n",
       "      <td>1.006692</td>\n",
       "      <td>-0.692870</td>\n",
       "      <td>-0.682216</td>\n",
       "      <td>-0.264805</td>\n",
       "      <td>4.201075</td>\n",
       "      <td>19.717757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_75</th>\n",
       "      <td>16038.0</td>\n",
       "      <td>0.041762</td>\n",
       "      <td>1.043294</td>\n",
       "      <td>-0.765690</td>\n",
       "      <td>-0.764264</td>\n",
       "      <td>-0.428254</td>\n",
       "      <td>3.751527</td>\n",
       "      <td>13.285983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_56</th>\n",
       "      <td>6593.0</td>\n",
       "      <td>0.023935</td>\n",
       "      <td>1.039534</td>\n",
       "      <td>-1.061037</td>\n",
       "      <td>-0.961045</td>\n",
       "      <td>-0.250147</td>\n",
       "      <td>3.916720</td>\n",
       "      <td>21.749474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R_1</th>\n",
       "      <td>16038.0</td>\n",
       "      <td>0.133297</td>\n",
       "      <td>1.238533</td>\n",
       "      <td>-0.347625</td>\n",
       "      <td>-0.347039</td>\n",
       "      <td>-0.320732</td>\n",
       "      <td>6.432988</td>\n",
       "      <td>10.980271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_112</th>\n",
       "      <td>16031.0</td>\n",
       "      <td>-0.053486</td>\n",
       "      <td>1.048564</td>\n",
       "      <td>-2.333971</td>\n",
       "      <td>-2.330546</td>\n",
       "      <td>0.430255</td>\n",
       "      <td>0.446494</td>\n",
       "      <td>0.446769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_66_1.0</th>\n",
       "      <td>16038.0</td>\n",
       "      <td>-0.008725</td>\n",
       "      <td>0.988946</td>\n",
       "      <td>-0.348794</td>\n",
       "      <td>-0.348794</td>\n",
       "      <td>-0.348794</td>\n",
       "      <td>2.867023</td>\n",
       "      <td>2.867023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_79</th>\n",
       "      <td>15657.0</td>\n",
       "      <td>0.074929</td>\n",
       "      <td>1.184475</td>\n",
       "      <td>-0.320260</td>\n",
       "      <td>-0.319655</td>\n",
       "      <td>-0.293424</td>\n",
       "      <td>4.394238</td>\n",
       "      <td>37.099150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_4</th>\n",
       "      <td>16038.0</td>\n",
       "      <td>0.047593</td>\n",
       "      <td>1.058690</td>\n",
       "      <td>-0.777685</td>\n",
       "      <td>-0.774119</td>\n",
       "      <td>-0.384207</td>\n",
       "      <td>4.104157</td>\n",
       "      <td>11.973915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_45</th>\n",
       "      <td>16031.0</td>\n",
       "      <td>-0.167819</td>\n",
       "      <td>0.955861</td>\n",
       "      <td>-1.002663</td>\n",
       "      <td>-0.994719</td>\n",
       "      <td>-0.611060</td>\n",
       "      <td>2.989022</td>\n",
       "      <td>5.448075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_50</th>\n",
       "      <td>6505.0</td>\n",
       "      <td>0.001820</td>\n",
       "      <td>1.038130</td>\n",
       "      <td>-1.863389</td>\n",
       "      <td>-0.317887</td>\n",
       "      <td>-0.120504</td>\n",
       "      <td>1.497231</td>\n",
       "      <td>58.508599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_46</th>\n",
       "      <td>10780.0</td>\n",
       "      <td>0.041721</td>\n",
       "      <td>1.064905</td>\n",
       "      <td>-12.705927</td>\n",
       "      <td>-2.954098</td>\n",
       "      <td>-0.076946</td>\n",
       "      <td>3.538839</td>\n",
       "      <td>17.490316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_6</th>\n",
       "      <td>16030.0</td>\n",
       "      <td>0.026686</td>\n",
       "      <td>1.079439</td>\n",
       "      <td>-0.223211</td>\n",
       "      <td>-0.213966</td>\n",
       "      <td>-0.080046</td>\n",
       "      <td>1.782985</td>\n",
       "      <td>68.150881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_42</th>\n",
       "      <td>4080.0</td>\n",
       "      <td>-0.011413</td>\n",
       "      <td>1.157470</td>\n",
       "      <td>-0.761604</td>\n",
       "      <td>-0.751342</td>\n",
       "      <td>-0.315323</td>\n",
       "      <td>4.158621</td>\n",
       "      <td>15.094677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_2</th>\n",
       "      <td>16031.0</td>\n",
       "      <td>-0.084172</td>\n",
       "      <td>1.023398</td>\n",
       "      <td>-1.558513</td>\n",
       "      <td>-1.554872</td>\n",
       "      <td>0.480373</td>\n",
       "      <td>0.973054</td>\n",
       "      <td>0.973995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R_3</th>\n",
       "      <td>16038.0</td>\n",
       "      <td>0.067878</td>\n",
       "      <td>1.035812</td>\n",
       "      <td>-0.576618</td>\n",
       "      <td>-0.575708</td>\n",
       "      <td>-0.126843</td>\n",
       "      <td>3.893281</td>\n",
       "      <td>20.309942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R_27</th>\n",
       "      <td>12684.0</td>\n",
       "      <td>-0.029017</td>\n",
       "      <td>1.033737</td>\n",
       "      <td>-2.818347</td>\n",
       "      <td>-2.773150</td>\n",
       "      <td>0.364169</td>\n",
       "      <td>0.381811</td>\n",
       "      <td>0.382129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_7</th>\n",
       "      <td>16038.0</td>\n",
       "      <td>0.096440</td>\n",
       "      <td>1.092048</td>\n",
       "      <td>-1.203733</td>\n",
       "      <td>-0.798048</td>\n",
       "      <td>-0.398341</td>\n",
       "      <td>4.211261</td>\n",
       "      <td>4.567586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_3</th>\n",
       "      <td>16031.0</td>\n",
       "      <td>0.059625</td>\n",
       "      <td>1.067393</td>\n",
       "      <td>-0.553951</td>\n",
       "      <td>-0.553001</td>\n",
       "      <td>-0.512042</td>\n",
       "      <td>3.862297</td>\n",
       "      <td>5.799789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_111</th>\n",
       "      <td>113.0</td>\n",
       "      <td>0.060535</td>\n",
       "      <td>0.871493</td>\n",
       "      <td>-3.447493</td>\n",
       "      <td>-3.215104</td>\n",
       "      <td>0.454538</td>\n",
       "      <td>0.476116</td>\n",
       "      <td>0.476291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_5</th>\n",
       "      <td>16038.0</td>\n",
       "      <td>-0.016938</td>\n",
       "      <td>0.904832</td>\n",
       "      <td>-0.223401</td>\n",
       "      <td>-0.222445</td>\n",
       "      <td>-0.186503</td>\n",
       "      <td>2.277212</td>\n",
       "      <td>53.571805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_132</th>\n",
       "      <td>1877.0</td>\n",
       "      <td>-0.015274</td>\n",
       "      <td>0.963093</td>\n",
       "      <td>-0.822095</td>\n",
       "      <td>-0.789697</td>\n",
       "      <td>-0.251879</td>\n",
       "      <td>2.916396</td>\n",
       "      <td>18.474034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P_3</th>\n",
       "      <td>12986.0</td>\n",
       "      <td>-0.148732</td>\n",
       "      <td>1.148080</td>\n",
       "      <td>-8.858212</td>\n",
       "      <td>-4.352974</td>\n",
       "      <td>0.036773</td>\n",
       "      <td>2.396786</td>\n",
       "      <td>6.248817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_51</th>\n",
       "      <td>16038.0</td>\n",
       "      <td>-0.044691</td>\n",
       "      <td>0.971931</td>\n",
       "      <td>-0.590727</td>\n",
       "      <td>-0.590162</td>\n",
       "      <td>-0.561414</td>\n",
       "      <td>3.649008</td>\n",
       "      <td>7.861229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_1</th>\n",
       "      <td>16038.0</td>\n",
       "      <td>0.128043</td>\n",
       "      <td>1.171732</td>\n",
       "      <td>-1.164754</td>\n",
       "      <td>-0.584650</td>\n",
       "      <td>-0.407506</td>\n",
       "      <td>5.260022</td>\n",
       "      <td>5.642473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_8</th>\n",
       "      <td>15461.0</td>\n",
       "      <td>0.170878</td>\n",
       "      <td>0.996814</td>\n",
       "      <td>-0.946044</td>\n",
       "      <td>-0.945581</td>\n",
       "      <td>1.060293</td>\n",
       "      <td>1.077998</td>\n",
       "      <td>1.078895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_41</th>\n",
       "      <td>16031.0</td>\n",
       "      <td>0.163677</td>\n",
       "      <td>1.438561</td>\n",
       "      <td>-0.290165</td>\n",
       "      <td>-0.289488</td>\n",
       "      <td>-0.258907</td>\n",
       "      <td>6.895374</td>\n",
       "      <td>25.612721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_134</th>\n",
       "      <td>578.0</td>\n",
       "      <td>0.128973</td>\n",
       "      <td>1.092603</td>\n",
       "      <td>-1.148801</td>\n",
       "      <td>-1.096121</td>\n",
       "      <td>-0.293752</td>\n",
       "      <td>2.258788</td>\n",
       "      <td>2.261820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_114_1.0</th>\n",
       "      <td>16038.0</td>\n",
       "      <td>-0.088556</td>\n",
       "      <td>1.011352</td>\n",
       "      <td>-1.187667</td>\n",
       "      <td>-1.187667</td>\n",
       "      <td>0.841987</td>\n",
       "      <td>0.841987</td>\n",
       "      <td>0.841987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S_3</th>\n",
       "      <td>13067.0</td>\n",
       "      <td>0.078328</td>\n",
       "      <td>1.059799</td>\n",
       "      <td>-2.256088</td>\n",
       "      <td>-1.144704</td>\n",
       "      <td>-0.304332</td>\n",
       "      <td>4.163186</td>\n",
       "      <td>11.996583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_52</th>\n",
       "      <td>15912.0</td>\n",
       "      <td>-0.040585</td>\n",
       "      <td>0.992847</td>\n",
       "      <td>-1.045989</td>\n",
       "      <td>-0.999869</td>\n",
       "      <td>-0.279552</td>\n",
       "      <td>4.713348</td>\n",
       "      <td>4.734942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_44</th>\n",
       "      <td>15145.0</td>\n",
       "      <td>0.093629</td>\n",
       "      <td>1.110112</td>\n",
       "      <td>-0.542054</td>\n",
       "      <td>-0.541368</td>\n",
       "      <td>-0.504535</td>\n",
       "      <td>4.626402</td>\n",
       "      <td>11.501430</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             count      mean       std        min        1%       50%  \\\n",
       "B_11       16038.0  0.121503  1.176472  -0.532954 -0.531206 -0.411362   \n",
       "R_26        1620.0  0.020764  0.971976  -0.331143 -0.330359 -0.187221   \n",
       "D_49        1877.0  0.011307  1.110656  -0.872777 -0.859872 -0.266965   \n",
       "B_9        16038.0  0.067975  0.969425  -0.645928 -0.645106 -0.507611   \n",
       "P_2        15842.0 -0.148636  1.077307  -4.165423 -2.916614 -0.046049   \n",
       "D_110        113.0  0.073871  0.978221  -2.256773 -2.230965  0.620538   \n",
       "D_48       13971.0  0.109906  1.037369  -1.208526 -1.173922 -0.116241   \n",
       "D_43       10138.0  0.055121  1.006692  -0.692870 -0.682216 -0.264805   \n",
       "D_75       16038.0  0.041762  1.043294  -0.765690 -0.764264 -0.428254   \n",
       "D_56        6593.0  0.023935  1.039534  -1.061037 -0.961045 -0.250147   \n",
       "R_1        16038.0  0.133297  1.238533  -0.347625 -0.347039 -0.320732   \n",
       "D_112      16031.0 -0.053486  1.048564  -2.333971 -2.330546  0.430255   \n",
       "D_66_1.0   16038.0 -0.008725  0.988946  -0.348794 -0.348794 -0.348794   \n",
       "D_79       15657.0  0.074929  1.184475  -0.320260 -0.319655 -0.293424   \n",
       "B_4        16038.0  0.047593  1.058690  -0.777685 -0.774119 -0.384207   \n",
       "D_45       16031.0 -0.167819  0.955861  -1.002663 -0.994719 -0.611060   \n",
       "D_50        6505.0  0.001820  1.038130  -1.863389 -0.317887 -0.120504   \n",
       "D_46       10780.0  0.041721  1.064905 -12.705927 -2.954098 -0.076946   \n",
       "B_6        16030.0  0.026686  1.079439  -0.223211 -0.213966 -0.080046   \n",
       "D_42        4080.0 -0.011413  1.157470  -0.761604 -0.751342 -0.315323   \n",
       "B_2        16031.0 -0.084172  1.023398  -1.558513 -1.554872  0.480373   \n",
       "R_3        16038.0  0.067878  1.035812  -0.576618 -0.575708 -0.126843   \n",
       "R_27       12684.0 -0.029017  1.033737  -2.818347 -2.773150  0.364169   \n",
       "B_7        16038.0  0.096440  1.092048  -1.203733 -0.798048 -0.398341   \n",
       "B_3        16031.0  0.059625  1.067393  -0.553951 -0.553001 -0.512042   \n",
       "D_111        113.0  0.060535  0.871493  -3.447493 -3.215104  0.454538   \n",
       "B_5        16038.0 -0.016938  0.904832  -0.223401 -0.222445 -0.186503   \n",
       "D_132       1877.0 -0.015274  0.963093  -0.822095 -0.789697 -0.251879   \n",
       "P_3        12986.0 -0.148732  1.148080  -8.858212 -4.352974  0.036773   \n",
       "D_51       16038.0 -0.044691  0.971931  -0.590727 -0.590162 -0.561414   \n",
       "B_1        16038.0  0.128043  1.171732  -1.164754 -0.584650 -0.407506   \n",
       "B_8        15461.0  0.170878  0.996814  -0.946044 -0.945581  1.060293   \n",
       "D_41       16031.0  0.163677  1.438561  -0.290165 -0.289488 -0.258907   \n",
       "D_134        578.0  0.128973  1.092603  -1.148801 -1.096121 -0.293752   \n",
       "D_114_1.0  16038.0 -0.088556  1.011352  -1.187667 -1.187667  0.841987   \n",
       "S_3        13067.0  0.078328  1.059799  -2.256088 -1.144704 -0.304332   \n",
       "D_52       15912.0 -0.040585  0.992847  -1.045989 -0.999869 -0.279552   \n",
       "D_44       15145.0  0.093629  1.110112  -0.542054 -0.541368 -0.504535   \n",
       "\n",
       "                99%        max  \n",
       "B_11       5.425988   7.940977  \n",
       "R_26       4.126059  14.565508  \n",
       "D_49       3.265411  25.355732  \n",
       "B_9        2.577903  23.263273  \n",
       "P_2        1.453646   1.475670  \n",
       "D_110      0.890521   0.890598  \n",
       "D_48       1.991288   9.891990  \n",
       "D_43       4.201075  19.717757  \n",
       "D_75       3.751527  13.285983  \n",
       "D_56       3.916720  21.749474  \n",
       "R_1        6.432988  10.980271  \n",
       "D_112      0.446494   0.446769  \n",
       "D_66_1.0   2.867023   2.867023  \n",
       "D_79       4.394238  37.099150  \n",
       "B_4        4.104157  11.973915  \n",
       "D_45       2.989022   5.448075  \n",
       "D_50       1.497231  58.508599  \n",
       "D_46       3.538839  17.490316  \n",
       "B_6        1.782985  68.150881  \n",
       "D_42       4.158621  15.094677  \n",
       "B_2        0.973054   0.973995  \n",
       "R_3        3.893281  20.309942  \n",
       "R_27       0.381811   0.382129  \n",
       "B_7        4.211261   4.567586  \n",
       "B_3        3.862297   5.799789  \n",
       "D_111      0.476116   0.476291  \n",
       "B_5        2.277212  53.571805  \n",
       "D_132      2.916396  18.474034  \n",
       "P_3        2.396786   6.248817  \n",
       "D_51       3.649008   7.861229  \n",
       "B_1        5.260022   5.642473  \n",
       "B_8        1.077998   1.078895  \n",
       "D_41       6.895374  25.612721  \n",
       "D_134      2.258788   2.261820  \n",
       "D_114_1.0  0.841987   0.841987  \n",
       "S_3        4.163186  11.996583  \n",
       "D_52       4.713348   4.734942  \n",
       "D_44       4.626402  11.501430  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test1_normalized.describe(percentiles=[0.01,0.99]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2da95faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the 1st and 99th percentile values for each column\n",
    "p1 = X_train_normalized.quantile(0.01)\n",
    "p99 = X_train_normalized.quantile(0.99)\n",
    "\n",
    "# Replace values outside the percentile range with the percentile value for each column\n",
    "X_train_normalized = X_train_normalized.apply(lambda x: nm.where(x < p1[x.name], p1[x.name], x))\n",
    "X_train_normalized = X_train_normalized.apply(lambda x: nm.where(x > p99[x.name], p99[x.name], x))\n",
    "\n",
    "# Apply the same operation to the test data\n",
    "X_test_normalized = X_test_normalized.apply(lambda x: nm.where(x < p1[x.name], p1[x.name], x))\n",
    "X_test_normalized = X_test_normalized.apply(lambda x: nm.where(x > p99[x.name], p99[x.name], x))\n",
    "\n",
    "#Apply to test2 data\n",
    "X_test1_normalized = X_test1_normalized.apply(lambda x: nm.where(x < p1[x.name], p1[x.name], x))\n",
    "X_test1_normalized = X_test1_normalized.apply(lambda x: nm.where(x > p99[x.name], p99[x.name], x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "de2bdb03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>1%</th>\n",
       "      <th>50%</th>\n",
       "      <th>99%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>B_11</th>\n",
       "      <td>55826.0</td>\n",
       "      <td>-8.676915e-03</td>\n",
       "      <td>0.955572</td>\n",
       "      <td>-0.531428</td>\n",
       "      <td>-0.531428</td>\n",
       "      <td>-0.434838</td>\n",
       "      <td>4.352780</td>\n",
       "      <td>4.353052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R_26</th>\n",
       "      <td>6435.0</td>\n",
       "      <td>-4.277126e-02</td>\n",
       "      <td>0.550416</td>\n",
       "      <td>-0.330432</td>\n",
       "      <td>-0.330430</td>\n",
       "      <td>-0.186771</td>\n",
       "      <td>3.305639</td>\n",
       "      <td>3.306861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_49</th>\n",
       "      <td>5454.0</td>\n",
       "      <td>-1.906092e-02</td>\n",
       "      <td>0.816735</td>\n",
       "      <td>-0.858330</td>\n",
       "      <td>-0.858192</td>\n",
       "      <td>-0.270438</td>\n",
       "      <td>3.482042</td>\n",
       "      <td>3.497066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_9</th>\n",
       "      <td>55826.0</td>\n",
       "      <td>-1.797829e-02</td>\n",
       "      <td>0.853091</td>\n",
       "      <td>-0.645131</td>\n",
       "      <td>-0.645131</td>\n",
       "      <td>-0.551619</td>\n",
       "      <td>2.738655</td>\n",
       "      <td>2.739362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P_2</th>\n",
       "      <td>55179.0</td>\n",
       "      <td>3.316911e-03</td>\n",
       "      <td>0.990092</td>\n",
       "      <td>-2.584323</td>\n",
       "      <td>-2.584113</td>\n",
       "      <td>0.133533</td>\n",
       "      <td>1.458052</td>\n",
       "      <td>1.458064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_110</th>\n",
       "      <td>312.0</td>\n",
       "      <td>1.669480e-03</td>\n",
       "      <td>0.997620</td>\n",
       "      <td>-2.279392</td>\n",
       "      <td>-2.278458</td>\n",
       "      <td>0.405253</td>\n",
       "      <td>0.890299</td>\n",
       "      <td>0.890362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_48</th>\n",
       "      <td>48432.0</td>\n",
       "      <td>-3.601424e-03</td>\n",
       "      <td>0.984628</td>\n",
       "      <td>-1.175378</td>\n",
       "      <td>-1.175373</td>\n",
       "      <td>-0.271703</td>\n",
       "      <td>1.890854</td>\n",
       "      <td>1.890956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_43</th>\n",
       "      <td>38334.0</td>\n",
       "      <td>-2.263545e-02</td>\n",
       "      <td>0.805052</td>\n",
       "      <td>-0.680945</td>\n",
       "      <td>-0.680935</td>\n",
       "      <td>-0.302693</td>\n",
       "      <td>3.916065</td>\n",
       "      <td>3.916392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_75</th>\n",
       "      <td>55826.0</td>\n",
       "      <td>-1.142219e-02</td>\n",
       "      <td>0.942196</td>\n",
       "      <td>-0.764397</td>\n",
       "      <td>-0.764397</td>\n",
       "      <td>-0.431731</td>\n",
       "      <td>3.731327</td>\n",
       "      <td>3.731336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_56</th>\n",
       "      <td>25154.0</td>\n",
       "      <td>-2.083145e-02</td>\n",
       "      <td>0.849168</td>\n",
       "      <td>-0.970446</td>\n",
       "      <td>-0.970443</td>\n",
       "      <td>-0.261640</td>\n",
       "      <td>3.894035</td>\n",
       "      <td>3.895420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R_1</th>\n",
       "      <td>55826.0</td>\n",
       "      <td>-1.675110e-02</td>\n",
       "      <td>0.903011</td>\n",
       "      <td>-0.347104</td>\n",
       "      <td>-0.347104</td>\n",
       "      <td>-0.321235</td>\n",
       "      <td>4.208480</td>\n",
       "      <td>4.208504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_112</th>\n",
       "      <td>55780.0</td>\n",
       "      <td>2.221996e-05</td>\n",
       "      <td>0.999953</td>\n",
       "      <td>-2.328540</td>\n",
       "      <td>-2.328539</td>\n",
       "      <td>0.430439</td>\n",
       "      <td>0.446446</td>\n",
       "      <td>0.446446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_66_1.0</th>\n",
       "      <td>55826.0</td>\n",
       "      <td>6.873018e-18</td>\n",
       "      <td>1.000009</td>\n",
       "      <td>-0.348794</td>\n",
       "      <td>-0.348794</td>\n",
       "      <td>-0.348794</td>\n",
       "      <td>2.867023</td>\n",
       "      <td>2.867023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_79</th>\n",
       "      <td>54517.0</td>\n",
       "      <td>-1.522024e-02</td>\n",
       "      <td>0.895802</td>\n",
       "      <td>-0.319784</td>\n",
       "      <td>-0.319784</td>\n",
       "      <td>-0.294235</td>\n",
       "      <td>4.385427</td>\n",
       "      <td>4.385435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_4</th>\n",
       "      <td>55826.0</td>\n",
       "      <td>-1.129397e-02</td>\n",
       "      <td>0.941709</td>\n",
       "      <td>-0.774438</td>\n",
       "      <td>-0.774438</td>\n",
       "      <td>-0.400618</td>\n",
       "      <td>3.751307</td>\n",
       "      <td>3.751313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_45</th>\n",
       "      <td>55787.0</td>\n",
       "      <td>-5.788520e-03</td>\n",
       "      <td>0.978556</td>\n",
       "      <td>-0.991155</td>\n",
       "      <td>-0.991154</td>\n",
       "      <td>-0.323003</td>\n",
       "      <td>3.113627</td>\n",
       "      <td>3.113676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_50</th>\n",
       "      <td>23801.0</td>\n",
       "      <td>-3.170027e-02</td>\n",
       "      <td>0.296269</td>\n",
       "      <td>-0.317943</td>\n",
       "      <td>-0.317943</td>\n",
       "      <td>-0.120913</td>\n",
       "      <td>1.577803</td>\n",
       "      <td>1.577803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_46</th>\n",
       "      <td>42221.0</td>\n",
       "      <td>1.071898e-03</td>\n",
       "      <td>0.811682</td>\n",
       "      <td>-2.721115</td>\n",
       "      <td>-2.720124</td>\n",
       "      <td>-0.098944</td>\n",
       "      <td>3.167416</td>\n",
       "      <td>3.167833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_6</th>\n",
       "      <td>55821.0</td>\n",
       "      <td>-3.593165e-02</td>\n",
       "      <td>0.215100</td>\n",
       "      <td>-0.213397</td>\n",
       "      <td>-0.213397</td>\n",
       "      <td>-0.096216</td>\n",
       "      <td>1.228349</td>\n",
       "      <td>1.228443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_42</th>\n",
       "      <td>9846.0</td>\n",
       "      <td>-2.681616e-02</td>\n",
       "      <td>0.801922</td>\n",
       "      <td>-0.755050</td>\n",
       "      <td>-0.755044</td>\n",
       "      <td>-0.270014</td>\n",
       "      <td>3.380430</td>\n",
       "      <td>3.396420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_2</th>\n",
       "      <td>55787.0</td>\n",
       "      <td>3.507253e-05</td>\n",
       "      <td>0.999945</td>\n",
       "      <td>-1.550723</td>\n",
       "      <td>-1.550722</td>\n",
       "      <td>0.483095</td>\n",
       "      <td>0.973248</td>\n",
       "      <td>0.973248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R_3</th>\n",
       "      <td>55826.0</td>\n",
       "      <td>-2.114357e-02</td>\n",
       "      <td>0.838838</td>\n",
       "      <td>-0.575791</td>\n",
       "      <td>-0.575790</td>\n",
       "      <td>-0.533293</td>\n",
       "      <td>3.884462</td>\n",
       "      <td>3.884514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R_27</th>\n",
       "      <td>54292.0</td>\n",
       "      <td>1.259513e-04</td>\n",
       "      <td>0.999653</td>\n",
       "      <td>-2.769068</td>\n",
       "      <td>-2.769067</td>\n",
       "      <td>0.364303</td>\n",
       "      <td>0.381781</td>\n",
       "      <td>0.381781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_7</th>\n",
       "      <td>55826.0</td>\n",
       "      <td>-6.065299e-03</td>\n",
       "      <td>0.974679</td>\n",
       "      <td>-0.798532</td>\n",
       "      <td>-0.798530</td>\n",
       "      <td>-0.469974</td>\n",
       "      <td>3.586189</td>\n",
       "      <td>3.586686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_3</th>\n",
       "      <td>55787.0</td>\n",
       "      <td>-4.627796e-03</td>\n",
       "      <td>0.980664</td>\n",
       "      <td>-0.552850</td>\n",
       "      <td>-0.552850</td>\n",
       "      <td>-0.512024</td>\n",
       "      <td>3.762443</td>\n",
       "      <td>3.762450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_111</th>\n",
       "      <td>312.0</td>\n",
       "      <td>5.371687e-05</td>\n",
       "      <td>1.001405</td>\n",
       "      <td>-3.453955</td>\n",
       "      <td>-3.453786</td>\n",
       "      <td>0.453359</td>\n",
       "      <td>0.475948</td>\n",
       "      <td>0.475955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_5</th>\n",
       "      <td>55826.0</td>\n",
       "      <td>-3.084357e-02</td>\n",
       "      <td>0.418167</td>\n",
       "      <td>-0.222376</td>\n",
       "      <td>-0.222376</td>\n",
       "      <td>-0.181712</td>\n",
       "      <td>2.529386</td>\n",
       "      <td>2.530079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_132</th>\n",
       "      <td>5414.0</td>\n",
       "      <td>-3.916802e-02</td>\n",
       "      <td>0.665556</td>\n",
       "      <td>-0.797577</td>\n",
       "      <td>-0.797534</td>\n",
       "      <td>-0.179098</td>\n",
       "      <td>3.001108</td>\n",
       "      <td>3.004249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P_3</th>\n",
       "      <td>51120.0</td>\n",
       "      <td>2.688987e-03</td>\n",
       "      <td>0.928998</td>\n",
       "      <td>-3.384121</td>\n",
       "      <td>-3.383912</td>\n",
       "      <td>0.088624</td>\n",
       "      <td>2.436130</td>\n",
       "      <td>2.436157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_51</th>\n",
       "      <td>55826.0</td>\n",
       "      <td>-7.786719e-03</td>\n",
       "      <td>0.962941</td>\n",
       "      <td>-0.590144</td>\n",
       "      <td>-0.590144</td>\n",
       "      <td>-0.560423</td>\n",
       "      <td>3.649151</td>\n",
       "      <td>3.649157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_1</th>\n",
       "      <td>55826.0</td>\n",
       "      <td>-7.171448e-03</td>\n",
       "      <td>0.963381</td>\n",
       "      <td>-0.584520</td>\n",
       "      <td>-0.584518</td>\n",
       "      <td>-0.433869</td>\n",
       "      <td>4.237263</td>\n",
       "      <td>4.238029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B_8</th>\n",
       "      <td>55598.0</td>\n",
       "      <td>-6.763956e-07</td>\n",
       "      <td>1.000005</td>\n",
       "      <td>-0.945669</td>\n",
       "      <td>-0.945669</td>\n",
       "      <td>-0.927202</td>\n",
       "      <td>1.077930</td>\n",
       "      <td>1.077930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_41</th>\n",
       "      <td>55787.0</td>\n",
       "      <td>-2.511768e-02</td>\n",
       "      <td>0.788834</td>\n",
       "      <td>-0.289568</td>\n",
       "      <td>-0.289567</td>\n",
       "      <td>-0.260035</td>\n",
       "      <td>4.574841</td>\n",
       "      <td>4.574984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_134</th>\n",
       "      <td>2021.0</td>\n",
       "      <td>9.569936e-05</td>\n",
       "      <td>1.000097</td>\n",
       "      <td>-1.102211</td>\n",
       "      <td>-1.101925</td>\n",
       "      <td>-0.372349</td>\n",
       "      <td>2.260133</td>\n",
       "      <td>2.260139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_114_1.0</th>\n",
       "      <td>55826.0</td>\n",
       "      <td>-1.794621e-16</td>\n",
       "      <td>1.000009</td>\n",
       "      <td>-1.187667</td>\n",
       "      <td>-1.187667</td>\n",
       "      <td>0.841987</td>\n",
       "      <td>0.841987</td>\n",
       "      <td>0.841987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S_3</th>\n",
       "      <td>45736.0</td>\n",
       "      <td>-1.099704e-02</td>\n",
       "      <td>0.922788</td>\n",
       "      <td>-1.149286</td>\n",
       "      <td>-1.148943</td>\n",
       "      <td>-0.330365</td>\n",
       "      <td>4.032117</td>\n",
       "      <td>4.032315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_52</th>\n",
       "      <td>55392.0</td>\n",
       "      <td>7.508181e-05</td>\n",
       "      <td>0.999292</td>\n",
       "      <td>-0.988311</td>\n",
       "      <td>-0.988299</td>\n",
       "      <td>-0.210879</td>\n",
       "      <td>4.712645</td>\n",
       "      <td>4.712651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_44</th>\n",
       "      <td>52848.0</td>\n",
       "      <td>-1.226317e-02</td>\n",
       "      <td>0.932133</td>\n",
       "      <td>-0.541329</td>\n",
       "      <td>-0.541329</td>\n",
       "      <td>-0.506563</td>\n",
       "      <td>4.067407</td>\n",
       "      <td>4.067435</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             count          mean       std       min        1%       50%  \\\n",
       "B_11       55826.0 -8.676915e-03  0.955572 -0.531428 -0.531428 -0.434838   \n",
       "R_26        6435.0 -4.277126e-02  0.550416 -0.330432 -0.330430 -0.186771   \n",
       "D_49        5454.0 -1.906092e-02  0.816735 -0.858330 -0.858192 -0.270438   \n",
       "B_9        55826.0 -1.797829e-02  0.853091 -0.645131 -0.645131 -0.551619   \n",
       "P_2        55179.0  3.316911e-03  0.990092 -2.584323 -2.584113  0.133533   \n",
       "D_110        312.0  1.669480e-03  0.997620 -2.279392 -2.278458  0.405253   \n",
       "D_48       48432.0 -3.601424e-03  0.984628 -1.175378 -1.175373 -0.271703   \n",
       "D_43       38334.0 -2.263545e-02  0.805052 -0.680945 -0.680935 -0.302693   \n",
       "D_75       55826.0 -1.142219e-02  0.942196 -0.764397 -0.764397 -0.431731   \n",
       "D_56       25154.0 -2.083145e-02  0.849168 -0.970446 -0.970443 -0.261640   \n",
       "R_1        55826.0 -1.675110e-02  0.903011 -0.347104 -0.347104 -0.321235   \n",
       "D_112      55780.0  2.221996e-05  0.999953 -2.328540 -2.328539  0.430439   \n",
       "D_66_1.0   55826.0  6.873018e-18  1.000009 -0.348794 -0.348794 -0.348794   \n",
       "D_79       54517.0 -1.522024e-02  0.895802 -0.319784 -0.319784 -0.294235   \n",
       "B_4        55826.0 -1.129397e-02  0.941709 -0.774438 -0.774438 -0.400618   \n",
       "D_45       55787.0 -5.788520e-03  0.978556 -0.991155 -0.991154 -0.323003   \n",
       "D_50       23801.0 -3.170027e-02  0.296269 -0.317943 -0.317943 -0.120913   \n",
       "D_46       42221.0  1.071898e-03  0.811682 -2.721115 -2.720124 -0.098944   \n",
       "B_6        55821.0 -3.593165e-02  0.215100 -0.213397 -0.213397 -0.096216   \n",
       "D_42        9846.0 -2.681616e-02  0.801922 -0.755050 -0.755044 -0.270014   \n",
       "B_2        55787.0  3.507253e-05  0.999945 -1.550723 -1.550722  0.483095   \n",
       "R_3        55826.0 -2.114357e-02  0.838838 -0.575791 -0.575790 -0.533293   \n",
       "R_27       54292.0  1.259513e-04  0.999653 -2.769068 -2.769067  0.364303   \n",
       "B_7        55826.0 -6.065299e-03  0.974679 -0.798532 -0.798530 -0.469974   \n",
       "B_3        55787.0 -4.627796e-03  0.980664 -0.552850 -0.552850 -0.512024   \n",
       "D_111        312.0  5.371687e-05  1.001405 -3.453955 -3.453786  0.453359   \n",
       "B_5        55826.0 -3.084357e-02  0.418167 -0.222376 -0.222376 -0.181712   \n",
       "D_132       5414.0 -3.916802e-02  0.665556 -0.797577 -0.797534 -0.179098   \n",
       "P_3        51120.0  2.688987e-03  0.928998 -3.384121 -3.383912  0.088624   \n",
       "D_51       55826.0 -7.786719e-03  0.962941 -0.590144 -0.590144 -0.560423   \n",
       "B_1        55826.0 -7.171448e-03  0.963381 -0.584520 -0.584518 -0.433869   \n",
       "B_8        55598.0 -6.763956e-07  1.000005 -0.945669 -0.945669 -0.927202   \n",
       "D_41       55787.0 -2.511768e-02  0.788834 -0.289568 -0.289567 -0.260035   \n",
       "D_134       2021.0  9.569936e-05  1.000097 -1.102211 -1.101925 -0.372349   \n",
       "D_114_1.0  55826.0 -1.794621e-16  1.000009 -1.187667 -1.187667  0.841987   \n",
       "S_3        45736.0 -1.099704e-02  0.922788 -1.149286 -1.148943 -0.330365   \n",
       "D_52       55392.0  7.508181e-05  0.999292 -0.988311 -0.988299 -0.210879   \n",
       "D_44       52848.0 -1.226317e-02  0.932133 -0.541329 -0.541329 -0.506563   \n",
       "\n",
       "                99%       max  \n",
       "B_11       4.352780  4.353052  \n",
       "R_26       3.305639  3.306861  \n",
       "D_49       3.482042  3.497066  \n",
       "B_9        2.738655  2.739362  \n",
       "P_2        1.458052  1.458064  \n",
       "D_110      0.890299  0.890362  \n",
       "D_48       1.890854  1.890956  \n",
       "D_43       3.916065  3.916392  \n",
       "D_75       3.731327  3.731336  \n",
       "D_56       3.894035  3.895420  \n",
       "R_1        4.208480  4.208504  \n",
       "D_112      0.446446  0.446446  \n",
       "D_66_1.0   2.867023  2.867023  \n",
       "D_79       4.385427  4.385435  \n",
       "B_4        3.751307  3.751313  \n",
       "D_45       3.113627  3.113676  \n",
       "D_50       1.577803  1.577803  \n",
       "D_46       3.167416  3.167833  \n",
       "B_6        1.228349  1.228443  \n",
       "D_42       3.380430  3.396420  \n",
       "B_2        0.973248  0.973248  \n",
       "R_3        3.884462  3.884514  \n",
       "R_27       0.381781  0.381781  \n",
       "B_7        3.586189  3.586686  \n",
       "B_3        3.762443  3.762450  \n",
       "D_111      0.475948  0.475955  \n",
       "B_5        2.529386  2.530079  \n",
       "D_132      3.001108  3.004249  \n",
       "P_3        2.436130  2.436157  \n",
       "D_51       3.649151  3.649157  \n",
       "B_1        4.237263  4.238029  \n",
       "B_8        1.077930  1.077930  \n",
       "D_41       4.574841  4.574984  \n",
       "D_134      2.260133  2.260139  \n",
       "D_114_1.0  0.841987  0.841987  \n",
       "S_3        4.032117  4.032315  \n",
       "D_52       4.712645  4.712651  \n",
       "D_44       4.067407  4.067435  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_normalized.describe(percentiles=[0.01,0.99]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "108ec652",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Missing Value Imputation\n",
    "X_train_normalized.fillna(0,inplace=True)\n",
    "X_test_normalized.fillna(0,inplace=True)\n",
    "X_test1_normalized.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2fa6d50d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "B_11         0\n",
       "R_26         0\n",
       "D_49         0\n",
       "B_9          0\n",
       "P_2          0\n",
       "D_110        0\n",
       "D_48         0\n",
       "D_43         0\n",
       "D_75         0\n",
       "D_56         0\n",
       "R_1          0\n",
       "D_112        0\n",
       "D_66_1.0     0\n",
       "D_79         0\n",
       "B_4          0\n",
       "D_45         0\n",
       "D_50         0\n",
       "D_46         0\n",
       "B_6          0\n",
       "D_42         0\n",
       "B_2          0\n",
       "R_3          0\n",
       "R_27         0\n",
       "B_7          0\n",
       "B_3          0\n",
       "D_111        0\n",
       "B_5          0\n",
       "D_132        0\n",
       "P_3          0\n",
       "D_51         0\n",
       "B_1          0\n",
       "B_8          0\n",
       "D_41         0\n",
       "D_134        0\n",
       "D_114_1.0    0\n",
       "S_3          0\n",
       "D_52         0\n",
       "D_44         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_normalized.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "00f3bfc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\parth\\AppData\\Local\\Temp\\ipykernel_30004\\1480584889.py:27: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  nn_model = KerasClassifier(build_fn=create_model, epochs=20, batch_size=32, verbose=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "349/349 [==============================] - 2s 4ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 1ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 1ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 1ms/step\n",
      "349/349 [==============================] - 1s 1ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 3ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 3ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 1ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 3ms/step\n",
      "349/349 [==============================] - 0s 1ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 1ms/step\n",
      "349/349 [==============================] - 0s 1ms/step\n",
      "349/349 [==============================] - 1s 1ms/step\n",
      "349/349 [==============================] - 0s 1ms/step\n",
      "349/349 [==============================] - 0s 1ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 0s 1ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 1ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 1ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 1ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 1ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 1ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 1ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 0s 1ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 0s 1ms/step\n",
      "349/349 [==============================] - 1s 1ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 0s 1ms/step\n",
      "349/349 [==============================] - 0s 1ms/step\n",
      "349/349 [==============================] - 0s 1ms/step\n",
      "349/349 [==============================] - 1s 1ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 0s 1ms/step\n",
      "349/349 [==============================] - 0s 1ms/step\n",
      "349/349 [==============================] - 0s 1ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 1ms/step\n",
      "349/349 [==============================] - 0s 1ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 0s 1ms/step\n",
      "349/349 [==============================] - 1s 1ms/step\n",
      "349/349 [==============================] - 1s 1ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 1ms/step\n",
      "349/349 [==============================] - 0s 1ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 0s 1ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 1ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "349/349 [==============================] - 1s 2ms/step\n",
      "Best AUC: 0.9267175746730523\n",
      "Best parameters: {'activation': 'relu', 'batch_size': 100, 'dropout_rate': 0, 'hidden_layers': 4, 'nodes_per_layer': 6}\n"
     ]
    }
   ],
   "source": [
    "#Neural Network-Grid search\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Define the neural network model\n",
    "def create_model(hidden_layers=2, nodes_per_layer=4, activation='relu', dropout_rate=0.5):\n",
    "    model = Sequential()\n",
    "    for i in range(hidden_layers):\n",
    "        if i == 0:\n",
    "            model.add(Dense(nodes_per_layer, activation=activation, input_shape=(X_train_normalized.shape[1],)))\n",
    "        else:\n",
    "            model.add(Dense(nodes_per_layer, activation=activation))\n",
    "        if dropout_rate > 0:\n",
    "            model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create a KerasClassifier with default parameters\n",
    "nn_model = KerasClassifier(build_fn=create_model, epochs=20, batch_size=32, verbose=0)\n",
    "\n",
    "# Define the hyperparameters for the grid search\n",
    "params = {\n",
    "    'hidden_layers': [2, 4],\n",
    "    'nodes_per_layer': [4, 6],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'dropout_rate': [0,0.5],\n",
    "    'batch_size': [100, 10000]\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(nn_model, param_grid=params, cv=5, scoring='roc_auc')\n",
    "grid_search.fit(X_train_normalized, y_train)\n",
    "\n",
    "# Print the results\n",
    "print(\"Best AUC:\", grid_search.best_score_)\n",
    "print(\"Best parameters:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "85411d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters {'activation': 'relu', 'batch_size': 100, 'dropout_rate': 0, 'hidden_layers': 2, 'nodes_per_layer': 4}\n",
      "AUC Score 0.9261707261454877\n",
      "Parameters {'activation': 'relu', 'batch_size': 100, 'dropout_rate': 0, 'hidden_layers': 2, 'nodes_per_layer': 6}\n",
      "AUC Score 0.9261510718686099\n",
      "Parameters {'activation': 'relu', 'batch_size': 100, 'dropout_rate': 0, 'hidden_layers': 4, 'nodes_per_layer': 4}\n",
      "AUC Score 0.9258760996700259\n",
      "Parameters {'activation': 'relu', 'batch_size': 100, 'dropout_rate': 0, 'hidden_layers': 4, 'nodes_per_layer': 6}\n",
      "AUC Score 0.9267175746730523\n",
      "Parameters {'activation': 'relu', 'batch_size': 100, 'dropout_rate': 0.5, 'hidden_layers': 2, 'nodes_per_layer': 4}\n",
      "AUC Score 0.9248112705321235\n",
      "Parameters {'activation': 'relu', 'batch_size': 100, 'dropout_rate': 0.5, 'hidden_layers': 2, 'nodes_per_layer': 6}\n",
      "AUC Score 0.9231302372220809\n",
      "Parameters {'activation': 'relu', 'batch_size': 100, 'dropout_rate': 0.5, 'hidden_layers': 4, 'nodes_per_layer': 4}\n",
      "AUC Score 0.8975249621042527\n",
      "Parameters {'activation': 'relu', 'batch_size': 100, 'dropout_rate': 0.5, 'hidden_layers': 4, 'nodes_per_layer': 6}\n",
      "AUC Score 0.9141982985966773\n",
      "Parameters {'activation': 'relu', 'batch_size': 10000, 'dropout_rate': 0, 'hidden_layers': 2, 'nodes_per_layer': 4}\n",
      "AUC Score 0.8450128438704901\n",
      "Parameters {'activation': 'relu', 'batch_size': 10000, 'dropout_rate': 0, 'hidden_layers': 2, 'nodes_per_layer': 6}\n",
      "AUC Score 0.8862772078435551\n",
      "Parameters {'activation': 'relu', 'batch_size': 10000, 'dropout_rate': 0, 'hidden_layers': 4, 'nodes_per_layer': 4}\n",
      "AUC Score 0.8627472977759659\n",
      "Parameters {'activation': 'relu', 'batch_size': 10000, 'dropout_rate': 0, 'hidden_layers': 4, 'nodes_per_layer': 6}\n",
      "AUC Score 0.8845782000979302\n",
      "Parameters {'activation': 'relu', 'batch_size': 10000, 'dropout_rate': 0.5, 'hidden_layers': 2, 'nodes_per_layer': 4}\n",
      "AUC Score 0.8283436932042602\n",
      "Parameters {'activation': 'relu', 'batch_size': 10000, 'dropout_rate': 0.5, 'hidden_layers': 2, 'nodes_per_layer': 6}\n",
      "AUC Score 0.8633227538640635\n",
      "Parameters {'activation': 'relu', 'batch_size': 10000, 'dropout_rate': 0.5, 'hidden_layers': 4, 'nodes_per_layer': 4}\n",
      "AUC Score 0.8140936997933214\n",
      "Parameters {'activation': 'relu', 'batch_size': 10000, 'dropout_rate': 0.5, 'hidden_layers': 4, 'nodes_per_layer': 6}\n",
      "AUC Score 0.7708239921982679\n",
      "Parameters {'activation': 'tanh', 'batch_size': 100, 'dropout_rate': 0, 'hidden_layers': 2, 'nodes_per_layer': 4}\n",
      "AUC Score 0.925891644064474\n",
      "Parameters {'activation': 'tanh', 'batch_size': 100, 'dropout_rate': 0, 'hidden_layers': 2, 'nodes_per_layer': 6}\n",
      "AUC Score 0.9257287663591942\n",
      "Parameters {'activation': 'tanh', 'batch_size': 100, 'dropout_rate': 0, 'hidden_layers': 4, 'nodes_per_layer': 4}\n",
      "AUC Score 0.9260149292936296\n",
      "Parameters {'activation': 'tanh', 'batch_size': 100, 'dropout_rate': 0, 'hidden_layers': 4, 'nodes_per_layer': 6}\n",
      "AUC Score 0.926435679751628\n",
      "Parameters {'activation': 'tanh', 'batch_size': 100, 'dropout_rate': 0.5, 'hidden_layers': 2, 'nodes_per_layer': 4}\n",
      "AUC Score 0.9253691628269269\n",
      "Parameters {'activation': 'tanh', 'batch_size': 100, 'dropout_rate': 0.5, 'hidden_layers': 2, 'nodes_per_layer': 6}\n",
      "AUC Score 0.9251770252221562\n",
      "Parameters {'activation': 'tanh', 'batch_size': 100, 'dropout_rate': 0.5, 'hidden_layers': 4, 'nodes_per_layer': 4}\n",
      "AUC Score 0.9248445095984053\n",
      "Parameters {'activation': 'tanh', 'batch_size': 100, 'dropout_rate': 0.5, 'hidden_layers': 4, 'nodes_per_layer': 6}\n",
      "AUC Score 0.9250170159383513\n",
      "Parameters {'activation': 'tanh', 'batch_size': 10000, 'dropout_rate': 0, 'hidden_layers': 2, 'nodes_per_layer': 4}\n",
      "AUC Score 0.8883764308362331\n",
      "Parameters {'activation': 'tanh', 'batch_size': 10000, 'dropout_rate': 0, 'hidden_layers': 2, 'nodes_per_layer': 6}\n",
      "AUC Score 0.8976484026596723\n",
      "Parameters {'activation': 'tanh', 'batch_size': 10000, 'dropout_rate': 0, 'hidden_layers': 4, 'nodes_per_layer': 4}\n",
      "AUC Score 0.8941907169215686\n",
      "Parameters {'activation': 'tanh', 'batch_size': 10000, 'dropout_rate': 0, 'hidden_layers': 4, 'nodes_per_layer': 6}\n",
      "AUC Score 0.9037372426530654\n",
      "Parameters {'activation': 'tanh', 'batch_size': 10000, 'dropout_rate': 0.5, 'hidden_layers': 2, 'nodes_per_layer': 4}\n",
      "AUC Score 0.8860117276027865\n",
      "Parameters {'activation': 'tanh', 'batch_size': 10000, 'dropout_rate': 0.5, 'hidden_layers': 2, 'nodes_per_layer': 6}\n",
      "AUC Score 0.899360957285707\n",
      "Parameters {'activation': 'tanh', 'batch_size': 10000, 'dropout_rate': 0.5, 'hidden_layers': 4, 'nodes_per_layer': 4}\n",
      "AUC Score 0.8810121575551664\n",
      "Parameters {'activation': 'tanh', 'batch_size': 10000, 'dropout_rate': 0.5, 'hidden_layers': 4, 'nodes_per_layer': 6}\n",
      "AUC Score 0.8909436226788134\n"
     ]
    }
   ],
   "source": [
    "results_nn = grid_search.cv_results_\n",
    "for i in range(len(results_nn['params'])):\n",
    "    print(\"Parameters\",results_nn['params'][i])\n",
    "    print(\"AUC Score\",results_nn['mean_test_score'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "901088dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>auc_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'activation': 'relu', 'batch_size': 100, 'dro...</td>\n",
       "      <td>0.926171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'activation': 'relu', 'batch_size': 100, 'dro...</td>\n",
       "      <td>0.926151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'activation': 'relu', 'batch_size': 100, 'dro...</td>\n",
       "      <td>0.925876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'activation': 'relu', 'batch_size': 100, 'dro...</td>\n",
       "      <td>0.926718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'activation': 'relu', 'batch_size': 100, 'dro...</td>\n",
       "      <td>0.924811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'activation': 'relu', 'batch_size': 100, 'dro...</td>\n",
       "      <td>0.923130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>{'activation': 'relu', 'batch_size': 100, 'dro...</td>\n",
       "      <td>0.897525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>{'activation': 'relu', 'batch_size': 100, 'dro...</td>\n",
       "      <td>0.914198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>{'activation': 'relu', 'batch_size': 10000, 'd...</td>\n",
       "      <td>0.845013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>{'activation': 'relu', 'batch_size': 10000, 'd...</td>\n",
       "      <td>0.886277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>{'activation': 'relu', 'batch_size': 10000, 'd...</td>\n",
       "      <td>0.862747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>{'activation': 'relu', 'batch_size': 10000, 'd...</td>\n",
       "      <td>0.884578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>{'activation': 'relu', 'batch_size': 10000, 'd...</td>\n",
       "      <td>0.828344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>{'activation': 'relu', 'batch_size': 10000, 'd...</td>\n",
       "      <td>0.863323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>{'activation': 'relu', 'batch_size': 10000, 'd...</td>\n",
       "      <td>0.814094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>{'activation': 'relu', 'batch_size': 10000, 'd...</td>\n",
       "      <td>0.770824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 100, 'dro...</td>\n",
       "      <td>0.925892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 100, 'dro...</td>\n",
       "      <td>0.925729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 100, 'dro...</td>\n",
       "      <td>0.926015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 100, 'dro...</td>\n",
       "      <td>0.926436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 100, 'dro...</td>\n",
       "      <td>0.925369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 100, 'dro...</td>\n",
       "      <td>0.925177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 100, 'dro...</td>\n",
       "      <td>0.924845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 100, 'dro...</td>\n",
       "      <td>0.925017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 10000, 'd...</td>\n",
       "      <td>0.888376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 10000, 'd...</td>\n",
       "      <td>0.897648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 10000, 'd...</td>\n",
       "      <td>0.894191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 10000, 'd...</td>\n",
       "      <td>0.903737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 10000, 'd...</td>\n",
       "      <td>0.886012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 10000, 'd...</td>\n",
       "      <td>0.899361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 10000, 'd...</td>\n",
       "      <td>0.881012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 10000, 'd...</td>\n",
       "      <td>0.890944</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               params  auc_score\n",
       "0   {'activation': 'relu', 'batch_size': 100, 'dro...   0.926171\n",
       "1   {'activation': 'relu', 'batch_size': 100, 'dro...   0.926151\n",
       "2   {'activation': 'relu', 'batch_size': 100, 'dro...   0.925876\n",
       "3   {'activation': 'relu', 'batch_size': 100, 'dro...   0.926718\n",
       "4   {'activation': 'relu', 'batch_size': 100, 'dro...   0.924811\n",
       "5   {'activation': 'relu', 'batch_size': 100, 'dro...   0.923130\n",
       "6   {'activation': 'relu', 'batch_size': 100, 'dro...   0.897525\n",
       "7   {'activation': 'relu', 'batch_size': 100, 'dro...   0.914198\n",
       "8   {'activation': 'relu', 'batch_size': 10000, 'd...   0.845013\n",
       "9   {'activation': 'relu', 'batch_size': 10000, 'd...   0.886277\n",
       "10  {'activation': 'relu', 'batch_size': 10000, 'd...   0.862747\n",
       "11  {'activation': 'relu', 'batch_size': 10000, 'd...   0.884578\n",
       "12  {'activation': 'relu', 'batch_size': 10000, 'd...   0.828344\n",
       "13  {'activation': 'relu', 'batch_size': 10000, 'd...   0.863323\n",
       "14  {'activation': 'relu', 'batch_size': 10000, 'd...   0.814094\n",
       "15  {'activation': 'relu', 'batch_size': 10000, 'd...   0.770824\n",
       "16  {'activation': 'tanh', 'batch_size': 100, 'dro...   0.925892\n",
       "17  {'activation': 'tanh', 'batch_size': 100, 'dro...   0.925729\n",
       "18  {'activation': 'tanh', 'batch_size': 100, 'dro...   0.926015\n",
       "19  {'activation': 'tanh', 'batch_size': 100, 'dro...   0.926436\n",
       "20  {'activation': 'tanh', 'batch_size': 100, 'dro...   0.925369\n",
       "21  {'activation': 'tanh', 'batch_size': 100, 'dro...   0.925177\n",
       "22  {'activation': 'tanh', 'batch_size': 100, 'dro...   0.924845\n",
       "23  {'activation': 'tanh', 'batch_size': 100, 'dro...   0.925017\n",
       "24  {'activation': 'tanh', 'batch_size': 10000, 'd...   0.888376\n",
       "25  {'activation': 'tanh', 'batch_size': 10000, 'd...   0.897648\n",
       "26  {'activation': 'tanh', 'batch_size': 10000, 'd...   0.894191\n",
       "27  {'activation': 'tanh', 'batch_size': 10000, 'd...   0.903737\n",
       "28  {'activation': 'tanh', 'batch_size': 10000, 'd...   0.886012\n",
       "29  {'activation': 'tanh', 'batch_size': 10000, 'd...   0.899361\n",
       "30  {'activation': 'tanh', 'batch_size': 10000, 'd...   0.881012\n",
       "31  {'activation': 'tanh', 'batch_size': 10000, 'd...   0.890944"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_nn = pd.DataFrame({\n",
    "    'params': results_nn['params'],\n",
    "    'auc_score': results_nn['mean_test_score']\n",
    "})\n",
    "\n",
    "# Write the dataframe to a CSV file\n",
    "results_nn.to_csv('grid_search_results_nn.csv', index=False)\n",
    "\n",
    "results_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cc260c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1745/1745 [==============================] - 3s 2ms/step\n",
      "348/348 [==============================] - 1s 2ms/step\n",
      "502/502 [==============================] - 1s 1ms/step\n",
      "1745/1745 [==============================] - 4s 2ms/step\n",
      "348/348 [==============================] - 1s 2ms/step\n",
      "502/502 [==============================] - 2s 3ms/step\n",
      "1745/1745 [==============================] - 3s 1ms/step\n",
      "348/348 [==============================] - 0s 1ms/step\n",
      "502/502 [==============================] - 1s 1ms/step\n",
      "1745/1745 [==============================] - 2s 1ms/step\n",
      "348/348 [==============================] - 1s 2ms/step\n",
      "502/502 [==============================] - 1s 2ms/step\n",
      "1745/1745 [==============================] - 2s 1ms/step\n",
      "348/348 [==============================] - 0s 1ms/step\n",
      "502/502 [==============================] - 1s 1ms/step\n",
      "1745/1745 [==============================] - 2s 1ms/step\n",
      "348/348 [==============================] - 1s 2ms/step\n",
      "502/502 [==============================] - 1s 2ms/step\n",
      "1745/1745 [==============================] - 2s 1ms/step\n",
      "348/348 [==============================] - 0s 1ms/step\n",
      "502/502 [==============================] - 1s 1ms/step\n",
      "1745/1745 [==============================] - 3s 2ms/step\n",
      "348/348 [==============================] - 1s 2ms/step\n",
      "502/502 [==============================] - 1s 3ms/step\n",
      "1745/1745 [==============================] - 2s 1ms/step\n",
      "348/348 [==============================] - 0s 1ms/step\n",
      "502/502 [==============================] - 1s 1ms/step\n",
      "1745/1745 [==============================] - 3s 2ms/step\n",
      "348/348 [==============================] - 1s 2ms/step\n",
      "502/502 [==============================] - 1s 2ms/step\n",
      "1745/1745 [==============================] - 2s 1ms/step\n",
      "348/348 [==============================] - 0s 1ms/step\n",
      "502/502 [==============================] - 1s 1ms/step\n",
      "1745/1745 [==============================] - 3s 2ms/step\n",
      "348/348 [==============================] - 1s 1ms/step\n",
      "502/502 [==============================] - 1s 2ms/step\n",
      "1745/1745 [==============================] - 2s 1ms/step\n",
      "348/348 [==============================] - 0s 1ms/step\n",
      "502/502 [==============================] - 1s 1ms/step\n",
      "1745/1745 [==============================] - 3s 2ms/step\n",
      "348/348 [==============================] - 1s 2ms/step\n",
      "502/502 [==============================] - 1s 2ms/step\n",
      "1745/1745 [==============================] - 2s 1ms/step\n",
      "348/348 [==============================] - 1s 2ms/step\n",
      "502/502 [==============================] - 1s 2ms/step\n",
      "1745/1745 [==============================] - 3s 2ms/step\n",
      "348/348 [==============================] - 1s 1ms/step\n",
      "502/502 [==============================] - 1s 1ms/step\n",
      "1745/1745 [==============================] - 3s 2ms/step\n",
      "348/348 [==============================] - 1s 2ms/step\n",
      "502/502 [==============================] - 1s 2ms/step\n",
      "1745/1745 [==============================] - 2s 1ms/step\n",
      "348/348 [==============================] - 0s 1ms/step\n",
      "502/502 [==============================] - 1s 1ms/step\n",
      "1745/1745 [==============================] - 3s 1ms/step\n",
      "348/348 [==============================] - 0s 1ms/step\n",
      "502/502 [==============================] - 1s 1ms/step\n",
      "1745/1745 [==============================] - 3s 1ms/step\n",
      "348/348 [==============================] - 1s 2ms/step\n",
      "502/502 [==============================] - 1s 2ms/step\n",
      "1745/1745 [==============================] - 3s 1ms/step\n",
      "348/348 [==============================] - 1s 2ms/step\n",
      "502/502 [==============================] - 1s 2ms/step\n",
      "1745/1745 [==============================] - 3s 2ms/step\n",
      "348/348 [==============================] - 1s 2ms/step\n",
      "502/502 [==============================] - 1s 2ms/step\n",
      "1745/1745 [==============================] - 4s 2ms/step\n",
      "348/348 [==============================] - 1s 2ms/step\n",
      "502/502 [==============================] - 1s 2ms/step\n",
      "1745/1745 [==============================] - 2s 1ms/step\n",
      "348/348 [==============================] - 0s 1ms/step\n",
      "502/502 [==============================] - 1s 1ms/step\n",
      "1745/1745 [==============================] - 3s 2ms/step\n",
      "348/348 [==============================] - 1s 1ms/step\n",
      "502/502 [==============================] - 1s 1ms/step\n",
      "1745/1745 [==============================] - 2s 1ms/step\n",
      "348/348 [==============================] - 1s 2ms/step\n",
      "502/502 [==============================] - 1s 2ms/step\n",
      "1745/1745 [==============================] - 2s 1ms/step\n",
      "348/348 [==============================] - 0s 1ms/step\n",
      "502/502 [==============================] - 1s 2ms/step\n",
      "1745/1745 [==============================] - 3s 2ms/step\n",
      "348/348 [==============================] - 1s 2ms/step\n",
      "502/502 [==============================] - 1s 1ms/step\n",
      "1745/1745 [==============================] - 3s 2ms/step\n",
      "348/348 [==============================] - 1s 2ms/step\n",
      "502/502 [==============================] - 1s 2ms/step\n",
      "1745/1745 [==============================] - 2s 1ms/step\n",
      "348/348 [==============================] - 0s 1ms/step\n",
      "502/502 [==============================] - 1s 1ms/step\n",
      "1745/1745 [==============================] - 2s 1ms/step\n",
      "348/348 [==============================] - 0s 1ms/step\n",
      "502/502 [==============================] - 1s 1ms/step\n",
      "1745/1745 [==============================] - 3s 2ms/step\n",
      "348/348 [==============================] - 1s 2ms/step\n",
      "502/502 [==============================] - 1s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# Define the hyperparameters for the grid search\n",
    "\n",
    "# Create empty lists to store results\n",
    "results_train = []\n",
    "results_test_1 = []\n",
    "results_test_2 = []\n",
    "h= []\n",
    "nodes=[]\n",
    "act=[]\n",
    "drop=[]\n",
    "batch=[]\n",
    "scores_nn = pd.DataFrame()\n",
    "\n",
    "# Nested for loop to iterate through hyperparameters\n",
    "for hidden_layers in [2,4]:\n",
    "    for nodes_per_layer in [4,6]:\n",
    "        for activation in ['relu','tanh']:\n",
    "            for dropout_rate in [0,0.5]:\n",
    "                for batch_size in [100,10000]:\n",
    "                    # Create the model with the current hyperparameters\n",
    "                    model_nn_test = Sequential()\n",
    "                    for i in range(hidden_layers):\n",
    "                        if i == 0:\n",
    "                            model_nn_test.add(Dense(nodes_per_layer, activation=activation, input_shape=(X_train_normalized.shape[1],)))\n",
    "                        else:\n",
    "                            model_nn_test.add(Dense(nodes_per_layer, activation=activation))\n",
    "                        if dropout_rate > 0:\n",
    "                            model_nn_test.add(Dropout(dropout_rate))\n",
    "                    model_nn_test.add(Dense(1, activation='sigmoid'))\n",
    "                    model_nn_test.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "                    \n",
    "                    # Train the model on the training data\n",
    "                    model_nn_test.fit(X_train_normalized, y_train, epochs=20, batch_size=batch_size, verbose=0)\n",
    "                    \n",
    "                    #creating lists\n",
    "                    h.append(hidden_layers)\n",
    "                    nodes.append(nodes_per_layer)\n",
    "                    act.append(activation)\n",
    "                    drop.append(dropout_rate)\n",
    "                    batch.append(batch_size)\n",
    "                    # Calculate AUC scores for the training data and the two test datasets\n",
    "                    y_train_pred = model_nn_test.predict(X_train_normalized)\n",
    "                    auc_train = roc_auc_score(y_train, y_train_pred)\n",
    "                    \n",
    "                    y_test_1_pred = model_nn_test.predict(X_test_normalized)\n",
    "                    auc_test_1 = roc_auc_score(y_test, y_test_1_pred)\n",
    "                    \n",
    "                    y_test_2_pred = model_nn_test.predict(X_test1_normalized)\n",
    "                    auc_test_2 = roc_auc_score(y_test1, y_test_2_pred)\n",
    "                    \n",
    "                    # Store the results in the lists\n",
    "                    results_train.append(auc_train)\n",
    "                    results_test_1.append(auc_test_1)\n",
    "                    results_test_2.append(auc_test_2)\n",
    "\n",
    "scores_nn['hidden_layers'] = h\n",
    "scores_nn['nodes_per_layer'] = nodes\n",
    "scores_nn['activation'] = act\n",
    "scores_nn['dropout_rate'] = drop\n",
    "scores_nn['batch_size'] = batch\n",
    "scores_nn['AUC Train'] = results_train\n",
    "scores_nn['AUC Test 1'] = results_test_1\n",
    "scores_nn['AUC Test 2'] = results_test_2\n",
    "scores_nn.to_csv(\"Neural_Network_Scores.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a0f05ab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9275643341060942,\n",
       " 0.8893915414602903,\n",
       " 0.9268073558970986,\n",
       " 0.8713005051416868,\n",
       " 0.9281029702331854,\n",
       " 0.892825104058681,\n",
       " 0.9256755643832901,\n",
       " 0.8868904510289668,\n",
       " 0.9292988645062661,\n",
       " 0.8873303190685548,\n",
       " 0.9238525992332345,\n",
       " 0.8215427778747506,\n",
       " 0.9287934066126331,\n",
       " 0.8986815776728513,\n",
       " 0.9255332436262074,\n",
       " 0.9036628083644711,\n",
       " 0.9280443570595236,\n",
       " 0.8868841436670434,\n",
       " 0.8789422381272864,\n",
       " 0.8172132620638342,\n",
       " 0.9277850473199343,\n",
       " 0.9029703438997665,\n",
       " 0.9249862653419764,\n",
       " 0.869506239327128,\n",
       " 0.9290133741822247,\n",
       " 0.8814341635926874,\n",
       " 0.9264772703435216,\n",
       " 0.8759135830020354,\n",
       " 0.9288091523713287,\n",
       " 0.9103670149060157,\n",
       " 0.9254106442788197,\n",
       " 0.897897289928719]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "54289b40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9114166672714357,\n",
       " 0.8764916178573313,\n",
       " 0.9118511153776441,\n",
       " 0.8595283635301197,\n",
       " 0.9109318665490818,\n",
       " 0.8755771175850956,\n",
       " 0.9117517764752399,\n",
       " 0.8756561631288579,\n",
       " 0.9136903297979883,\n",
       " 0.8801282280439524,\n",
       " 0.9091286694804999,\n",
       " 0.7885156075638948,\n",
       " 0.9122343821102351,\n",
       " 0.8847704346281929,\n",
       " 0.9119950727904408,\n",
       " 0.8883830556458229,\n",
       " 0.911477345757707,\n",
       " 0.8713847919151259,\n",
       " 0.8766919979747855,\n",
       " 0.8057394053212681,\n",
       " 0.9116119628498038,\n",
       " 0.8871977756687022,\n",
       " 0.9113027019195905,\n",
       " 0.8613809500642714,\n",
       " 0.9118357049684324,\n",
       " 0.8475363773012915,\n",
       " 0.911250960574708,\n",
       " 0.8613734240504701,\n",
       " 0.9125379761312472,\n",
       " 0.8945159057373939,\n",
       " 0.9111183817780152,\n",
       " 0.8858756387144241]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e778d99f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9376379069624325,\n",
       " 0.8972614350706318,\n",
       " 0.9394633150238008,\n",
       " 0.8795076260077619,\n",
       " 0.9386709160364586,\n",
       " 0.9012855672448633,\n",
       " 0.9386998199412728,\n",
       " 0.901071142144383,\n",
       " 0.9401495719758957,\n",
       " 0.8929665573389809,\n",
       " 0.9328832098836919,\n",
       " 0.8601092406551051,\n",
       " 0.9393168344678392,\n",
       " 0.9118389977281589,\n",
       " 0.9382354722210264,\n",
       " 0.9196943893797636,\n",
       " 0.938863030846596,\n",
       " 0.8942855360199305,\n",
       " 0.8744168819570212,\n",
       " 0.8126865253955713,\n",
       " 0.9387050967275664,\n",
       " 0.9134919770869624,\n",
       " 0.9373681277105819,\n",
       " 0.8700397829897479,\n",
       " 0.9391564277433833,\n",
       " 0.8998183459213266,\n",
       " 0.9390435026219848,\n",
       " 0.881223026811607,\n",
       " 0.9387364353434713,\n",
       " 0.925996037465069,\n",
       " 0.9382107272195924,\n",
       " 0.9015634179902905]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "922a71c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best AUC: 0.9267175746730523\n",
      "Best parameters: {'activation': 'relu', 'batch_size': 100, 'dropout_rate': 0, 'hidden_layers': 4, 'nodes_per_layer': 6}\n"
     ]
    }
   ],
   "source": [
    "# Print the results\n",
    "print(\"Best AUC:\", grid_search.best_score_)\n",
    "print(\"Best parameters:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "507f611d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x211c821ddb0>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Best Neural Network\n",
    "# Define the neural network model with the best parameters\n",
    "model_best_nn = Sequential()\n",
    "model_best_nn.add(Dense(6, activation='relu', input_shape=(X_train_normalized.shape[1],)))\n",
    "model_best_nn.add(Dropout(0))\n",
    "model_best_nn.add(Dense(6, activation='relu'))\n",
    "model_best_nn.add(Dropout(0))\n",
    "model_best_nn.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model_best_nn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model on the training data\n",
    "model_best_nn.fit(X_train_normalized, y_train, batch_size=100, epochs=20, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f5c0c513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "348/348 [==============================] - 0s 1ms/step\n",
      "348/348 [==============================] - 1s 1ms/step - loss: 0.3130 - accuracy: 0.8552\n",
      "Test loss: 0.313\n",
      "Test accuracy: 0.855\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "y_pred_nn = model_best_nn.predict(X_test_normalized)\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "loss, accuracy = model_best_nn.evaluate(X_test_normalized, y_test)\n",
    "print(f\"Test loss: {loss:.3f}\")\n",
    "print(f\"Test accuracy: {accuracy:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d4975a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default rate for train data is :  0.25767563500877727\n",
      "Default rate for train data is :  0.23697236972369723\n",
      "Default rate for train data is :  0.2883152512782142\n"
     ]
    }
   ],
   "source": [
    "###Strategy#####\n",
    "Default_rate_train = sum(y_train['target'])/(len(y_train['target']))\n",
    "Default_rate_test= sum(y_test['target'])/(len(y_test['target']))\n",
    "Default_rate_test1=sum(y_test1['target'])/(len(y_test1['target']))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Default rate for train data is : \",Default_rate_train)\n",
    "print(\"Default rate for train data is : \",Default_rate_test)\n",
    "print(\"Default rate for train data is : \",Default_rate_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2c4701a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['target'], dtype='object')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6d5ece18",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_test1 = pd.DataFrame(y_test1[['target']])\n",
    "\n",
    "Data_test1.to_csv(\"Probability_of_default.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0f295c7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16038"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "75cd71a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data_test1=Data_test1.sort_values(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "36d1b812",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_xgb = pd.DataFrame(xgb_best_model.predict_proba(X_test1_selected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9e83ca82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16038"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Data_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4c24e0fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16038, 1)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data_test1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c26c7622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16038, 2)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_xgb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c7e3eafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_test1 = Data_test1.reset_index(drop=True)\n",
    "df_xgb = df_xgb.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c9558b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.concat([Data_test1, df_xgb],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3735ce00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16038, 3)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_new)\n",
    "df_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4508442f",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_testing_rate=df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "212fb519",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "\n",
    "\n",
    "fpr,tpr,threshold= roc_curve(nm.array(y_test1),xgb_best_model.predict(X_test1_selected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "79e4a021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "youden_index = tpr-fpr\n",
    "type(youden_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "07412c09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nm.argmax(youden_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c17d6927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.10443315 1.        ] [0.         0.81725779 1.        ] [2 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(fpr,tpr,threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b689c116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold[nm.argmax(youden_index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "cb82757b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9        1\n",
       "10       1\n",
       "17       1\n",
       "34       1\n",
       "35       0\n",
       "        ..\n",
       "82942    0\n",
       "82943    0\n",
       "82944    0\n",
       "82947    1\n",
       "82971    1\n",
       "Name: target, Length: 16038, dtype: int64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test1['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b617743e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"C:\\\\Users\\\\parth\\\\The University of Texas at Dallas\\\\Saxena, Sakshi - Parth's\\\\Final-Code\""
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "58f23b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Accepted:  8859\n",
      "Default Rate:  0.09967264928321481\n",
      "Revenue:  18.858450363965247\n"
     ]
    }
   ],
   "source": [
    "#Strategy\n",
    "\n",
    "#def revenue(TH,x,y,TR):\n",
    "def revenue(TH,x,y):\n",
    "    df = pd.DataFrame(y)\n",
    "    df['Prob of 1'] = xgb_best_model.predict_proba(x)[:, 1]\n",
    "    df['Accepted_customers'] = (df['Prob of 1'] < TH).astype(int)\n",
    "    df = df[df['Accepted_customers'] == 1]\n",
    "    balance_feature = x['B_9']\n",
    "    spend_feature = x['S_3']\n",
    "    df['B9_Balance'] = balance_feature\n",
    "    df['S3_Spend'] = spend_feature\n",
    "    df = df.dropna()\n",
    "    length = len(df['target'])\n",
    "    RR = nm.mean(df['target'])\n",
    "    df = df.drop(df[df['target'] == 1].index)\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    return(length,RR,(sum(df['B9_Balance'])*0.02) + (sum(df['S3_Spend'])*0.001))\n",
    "\n",
    "\n",
    "a,b,c = revenue(0.56,X_test1_selected,y_test1)\n",
    "print(\"#Accepted: \",a)\n",
    "print(\"Default Rate: \",b)\n",
    "print(\"Revenue: \",c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "c979e0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Accepted:  41500\n",
      "Default Rate:  0.042433734939759035\n",
      "Revenue:  73.73352704459457\n"
     ]
    }
   ],
   "source": [
    "input_x = pd.concat([X_train_selected,X_test_selected,X_test1_selected])\n",
    "input_y= pd.concat([y_train,y_test,y_test1])\n",
    "a,b,c = revenue(0.3,input_x,input_y)\n",
    "print(\"#Accepted: \",a)\n",
    "print(\"Default Rate: \",b)\n",
    "print(\"Revenue: \",c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec59f3dd-6d95-4d14-8fdb-dfa73007cbc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8ebe5b-53e9-4cb6-9bf3-f0e043d5c770",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
